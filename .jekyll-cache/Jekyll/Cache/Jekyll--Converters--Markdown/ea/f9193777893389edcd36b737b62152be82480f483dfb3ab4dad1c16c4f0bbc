I"Ϻ<h1 id="面向神经机器翻译的领域适应方法研究">面向神经机器翻译的领域适应方法研究</h1>

<h1 id="abstract">Abstract</h1>

<p>​		数据驱动的机器翻译技术的性能在很大程度上取决于训练和测试数据之间的领域匹配程度。由于不同领域训练数据差异大导致跨领域翻译性能下降，研究者通常通过使翻译系统适应到目标领域的方法来提高特定领域的翻译性能。<strong>神经机器翻译（NMT）领域适应研究的主要策略大致可以分为两类：一类是将领域外的知识迁移和适应到领域内的翻译中；另外一类是使用混合领域语料构建翻译模型来翻译多领域文本</strong>。本文分别针对这两类情景，开展了面向神经机器翻译的领域适应方法研究。本文主要工作包括：</p>

<p>​		（１）<strong>基于句子权重的神经机器翻译的领域适应方法</strong>。在机器翻译中，与目标领域相关的那些领域外的实例对于模型的训练通常是有益的，而那些与目标领域不相关的领域外数据可能会降低翻译质量。在本文中，我们提出一种基于句子权重的领域适应方法，根据句子与目标领域的相关程度来评估句子的权重，并将权重融入NMT影响参数更新。我们将该方法应用在标准的领域适应和低资源领域伪语料训练的两个场景中，在中英IWSLT领域适应任务和低资源的电商领域翻译任务中，该方法均取得了特定领域翻译性能的显著提升。</p>

<p>​		（２）<strong>基于自注意力机制的神经机器翻译的多领域适应方法</strong>。在通用平行语料库上．训练的模型通常会受到数据领域多样性的影响，我们期望用单个模型来同时提升多个领域的翻译质量。在本文中，我们为多领域翻译提了一种<u>基于领域感知的自注意力机制</u>，强制ＮＭＴ模型同时编码和解码语义信息和领域信息，联合学习领域表示以训练多领域的祌经机器翻译系统。在中英和英法的多领域翻译任务上：，实验结果显示，各个领域的翻译质量均得到了提升，表明了基于领域感知的自注意力机制的多领域ＮＭＴ方法在多领域适应上的有效性。</p>

<p>​		（３）<strong>基于无监督单词级别的神经机器翻译的多领域适应方法</strong>。现实的多领域的翻译场景要求模型具有可扩展性，且能适用于未知领域信息的句子输入的训练与翻译。在本文中，我们提出了基于无监瞥单词级别适应的多领域神经机器翻译模型，与基于领域感知的自注意力机制一起，通过基于领域注意力网络的单词级别的无监督学习、加入辅助的损失的引导学习等方法来学习词的领域表示。在中英多领域翻译任务上的实验和分析表明，我们的模型显著优于基线系统，并且确实学习到了数据的领域结构。进一步的分析实验表明，即使没有与领域结构相关的先验知识，我们的模型也可以学习到领域信息，并将句子按领域聚集起来。</p>

<h1 id="关键词">关键词：</h1>

<p>​		神经机器翻译：领域适应；多领域神经机器翻译；领域感知；无监督</p>

<h1 id="第一章-绪论">第一章 绪论</h1>

<p>​		随着互联网和社交网络的发展，机器翻译（Machine Translation, ＭＴ）在社会发展和信息传播中的作用越来越突出。机器翻译研究如何利用计算机实现自然语言之间的自动翻译，是人工智能和自然语言处理领域的重要研究方向之一。为了满足人们对机器翻译的强烈需求，国内外许多知名研究机构和公司，对机器翻译进行了深入的研究，例如谷歌、微软、百度、搜狗等。神经机器翻译[1-7]（Neural Machine Translation, NMT）作为机器翻译最新的研究方向，相对于统计机器翻译（Statistical Machine Translation, SMT）而言在翻译质量上获得显著提升。无论是统计机器翻译，还是神经机器翻译，训练数据通常来源复杂，领域多样、文体不一，与待翻译目标文本的领域不能保证完全一致，从而导致领域适应问题。为了解决这个问题，我们针对神经机器翻译这个方向，探讨如何提升翻译模型的领域适应性。</p>

<h2 id="11-研究背景及意义">1.1 研究背景及意义</h2>

<p>​		世界上有超过几千种语言，仅仅是联合国的工作语言就有六种之多。人工翻译需要训练有素的双语专家，翻译工作非常耗时耗力，对于特定领域的翻译，翻译人员还需要具备该领域的基本知识。如果能够通过机器翻译准确地进行语言间的翻译，将大大提高人类沟通交流的效率。</p>

<p>​		机器翻译是指借助计算机自动地将？种Ｄ然语ａ文本（源ｉ／ｕ＇ｎ翻译成另－种Ｄ然语言文本（Ｈ标ｍ）１５７１。ｗｅａｖｅｒｆ１９４９年最早提出了使用机器做翻译的思想。２０世纪５０￥代到８０￥代，足第－代堪Ｔ规则的机器翻译（Ｒｕｌｅ－ｂａｓｅｄＭａｃｈｉｎｅＴｒａｎｓｌａｔｉｏｎ，ＲＢＭＴ）系统的发展时期，这？时期的机器翻译都是通过研宄源语ｄ与Ｈ标语３的语言学信息来做的，即基于词典和语法生成翻译。随着统计学的发展，统计模型被研宄者应用于机器翻译，基于对双语文本语料库的分析来生成翻译结果。这种方法被称为统计机器翻译。ＳＭＴ的表现比ＲＢＭＴ更好，并且在１９８０年代到２０００年代之间主宰了机器翻译领域。１９９７年，Ｎｅｃｏ和Ｆｏｒｃａｄａ提出了使用编码器－解码器结构做机器翻译的想法１Ｗ。２００３年，蒙特利尔大学Ｂｅｎｇｉｏ领导的研宄团队开发了一个基于神经网络的语言模型Ｐ１，改善了传统ＳＭＴ模型的数据稀疏性问题。他们的研究工作为未来神经网络在机器翻译上的应用奠定了基础。神经机器翻从２０１４年开始兴起，逐渐应用卷积神经网络（ＣｏｎｖｏｌｕｔｉｏｎＮｅｕｒａｌＮｅｔｗｏｒｋ，ＣＮＮ），递归神经网络（ＲｅｃｕｒｒｅｎｔＮｅｕｒａｌＮｅｔｗｏｒｋ，ＲＮＮ），注意力机制（ＡｔｔｅｎｔｉｏｎＭｅｃｈａｎｉｓｍ）等技术。ＮＭＴ架构简单且无需人工构建特征，相比于ＳＭＴ的译文更具流畅性和可读性，许多ＭＴ提供商从２０１６年开始正在转向这项技术，ＮＭＴ基本全面取代传统的统计机器翻译。Ｇｏｏｇｌｅ，百度，搜狗等已上线神经机器翻译系统。</p>

<p>​		神经机器翻译是一种基于深度学习的机器翻译方法，在大规模平行语料可得到的情况下，具有先进的翻译性能。虽然高质量和特定领域的翻译至关重要，但是在现实世界中，特定领域的语料库通常是稀缺的或不存在的，通用翻译系统在这种情况下表现不佳，因此很难开发特定领域的翻译系统。</p>

<p>​		同时，数据驱动的机器翻译技术的性能在很大程度上取决于训练和测试数据之间的领域匹配度，其中“领域”表示特定的因素组合，如流派、主题、国籍或作者及出版物的风格等。训练数据通常是来源广泛、多种多样的，但是跨领域的翻译是不可靠的，因此通常可以通过适应技术来使翻译系统更适用于特定领域的翻译。</p>

<p>​		<strong>利用领域外平行语料库和领域内单语语料库来改进特定领域的翻译被称为机器翻译的领域适应</strong>。在统计机器翻译中，领域适应问题己有着广泛的研究，但在神经机器翻译领域中，领域适应还是一个尚在研究的问题。因此，神经机器翻译模型的适应性研究是具有挑战性和广泛应用前景的。</p>

<h2 id="12-研究现状">1.2 研究现状</h2>

<p>​		在统计机器翻译中，针对特定领域和语言缺乏数据的问题，研究者提出了许多领域适应方法。SMT领域适应方法可以大致分为两类：数据层面和模型层面。</p>

<p>​		第一类方法主要是利用领域内的数据来挑选或者生成领域相关的数据。当有充足的领域外的平行语料时，主要思想是利用领域内外的数据训练的模型来给领域外的数据评分，并且根据分数设定阈值从领域外数据中挑选句子作为训练语料。语言模型[12][60-61]、联合模型[62-63]和最近的卷积神经网络可以用来给句子评分。当没有充足的平行语料时，有许多生成伪平行语料的方法研究，如信息检索[64]、自我增强[65]或平行单词向量[66]。大多数SMT的数据层面的领域适应方法可以被使用在ＮＭＴ里，这些方法可以在一定程度上提升ＮＭＴ的翻译。</p>

<p>​		第二类方法的主要思想是对各个领域模型的插值法，又可细分为<strong>模型级别的插值法</strong>和<strong>实例级别的插值法</strong>。<u>模型级别的插值法</u>是将SMT模型（如语言模型、翻译模型和重排序模型）针对各自对应的语料库训练，然后将这些模型组合到一起达到最佳性能[67-68]。<u>实例级别的插值方法</u>已应用于很多自然语言处理（Natural Language Processing, NLP）领域适应任务，尤其是SMT[69]。他们首先通过规则或统计方法对实例/领域评分，然后通过为每个实例/领域赋予权重来训练ＳＭＴ模型。另一种方法是通过数据重新采样来对语料库进行加权[70]。对于ＮＭＴ，己经提出了几种与SMT模型/数据插值法类似的方法。<strong>与模型级别的插值法最相关的NMT技术是模型融合[71]。与实例级别的插值法最相关是在NMT目标函数中分配权重的方法[13-14]。</strong>然而，SMT和NMT的模型结构完全不同。SMT是几个独立模型的组合，相比之下，NMT本身就是一个完整的模型。因此，大多数SMT的领域适应方法不能直接应用于ＮＭＴ。</p>

<p>​		NMT的领域适应性是一个新的研究领域，引起了研究界的广泛关注。最近关于神经机器翻译领域适应方法可以被分为两大类。</p>

<p>​		<strong>第二类方法是将领域外知识转移到领域内的翻译</strong>。这类方法主要有微调[8-10]、数据选择[11-14]等。微调的方法是采用在源领域上训练的模型并用目标域数据进行微调。Luong, Manning和zoph等人[8-9]探索了如何通过仅微调网络的部分来提升低资源语言对的适应学习。Chu等人[15]评估了多种领域适应方法,并建议在微调时混合源领
域和目标领域的数据。 Freitag和Al-Onaiκzan[16]探索了仅使用小部分目标领域数据微调的方法。从数据选择的角度来看, Chell[12]等人提出根据训练句子和开发集的相似性用分类器给句子分配权重的损失加权方法。Wang[17]等人探讨了基于句子向量的NMT领域适应的数据选择策略。而且,Wang等人[14]进一步提出了几种具有动态权重学习策略的句子和领域加权方法。这些方法旨在将知识转移到新的领域并仅在目标领域上表现良好。
​		第二类方法是多领域翻译的情景,其中所有领域的性能都很重要。一个通用的多领域翻译方法是将领域的指示标记附加到源端序列[18]。Chu等人[15]进一步完善了这想法,将源端标记融合在微调中。 Sajjad[19]等人提岀了数据融合、模型叠加、数据选择和多模型融合的方法来训练多领域神经翻译模型。Zeng等人[20]通过构建领域特有的和领域共享的表示,并且在目标函数中调整目标端单词的权重,提出基于单词级领域上下文的多领域翻译模型的方法。 Pryzant等人[21]提出了三种模型:加入了领域分类器与翻译过程联合学习的判别的混合领域翻译模型、对抗的判别混合领域翻译模型、目标端加领域标记的混合领域翻译模型,并探索了半监督情景中领域之间的知识适应。 Farajian等人[22]提出了无监督的适应模型,利用了测试集句子和训练句子之间的相似性来动态设置学习算法的超参数并直接更新通用模型。</p>

<h2 id="13-本文研究内容">1.3 本文研究内容</h2>

<p>​		训练一个特定领域的翻译模型需要大规模的本领域的平行语料,这通常是不可得到的。于是我们用大规模领琙外数据或单语的领域内数据来提升特定领域的翻译性能。针对如何使神经机器翻译系统提升领域适应性这一问题,我们进行了一系列实验,提出了三个有效的模型:基于句子权重的神经机器翻译的领域适应方法、基于自注意力机制的神经机器翻译的多领域适应方法、基于无监督单词级别的神经机器翻译的多领域适应方法。具体研究内容包括：
​	（1）基于句子权重的神经机器翻译的领域适应方法
​		基于句子权重的神经机器翻译的领域适应的方法适用于两个场景:1)领域外语料对特定领域的适应;2)伪平行语料的低资源语言神经机器翻译
鉴于领域外的语料中,有一部分接近领域内的语料能帮助领域内的翻译,但与领域内语料相差较远的语料可能会降低领域内的翻译效果,我们采用基于句子权重的神经机器翻译的领域适应方法。在基于句子权重的神经机器翻译的领域适应方法中,我们将领域外(out-of- domain)语料的句子按照与领域内(in- domain)语料的相似程度来赋予不同的杈重,与in- domain语料越接近,权重越高,越远,权重越低。我们通过 Jensen- Shannon(JS)散度、欧式距离等相似度的计算来表示out- of-domain句子与in- domain语料的相似程度,从而赋予句子权重。将带有权重的句子融入神绎机器翻译的训练,来提高神经机器翻译的领域适应性。
​		神经机器翻译的性能高度依赖于平行语料的规模,对于低资源语言,利用现有机器翻译系统翻译单语数据来构造伪平行语料的方法,使得伪平行语料与测试数据间存在着不匹配。我们利用神经机器翻译的领域适应的方法,来解决翻译模型在训练时所采用的伪平行语料和在测试时所采用真实的领域数据不一致所导致的领域适应性问题。通过多种方法,提高基于伪平行语料的低资源语言神经机器翻译的翻译性能。
​		（2）基于自注意力杋制的神经机器翻译的多领域适应方法
​		特定领域的神经机器翻译需要大量的本领域的平行数据,但常常是难以得到的。因此常用混合领域的平行语料来构建神经机器翻译系统。我们提出了一种新的多领域神经机器翻译方法,使用一种基于领域感知的自注意力机制,将联合学习得到的多领域的表示融入编码端和解码端,使神经杋器翻译模型同时对词乂和领域信息进行编码和解码。一系列实验结果显示,提出的该模型能够显著地提高领域的适应性。
​		（3）基于无监督单词级别的神经机器翻译的多领域适应方法
​		在多领域混合翻译的场景下,在通用平行语料库上训练的模型受到数据多样性的影响。通常我们得到的通用数据和翻译请求属于多个领域,且很少带有领域信息。针对这个问题,我们提出了基于无监督单词级別的神经机器翻译多领域适应方法,与领域感知的自注意力机制模型一起,通过个注意力网终无监督地学习每个词对应的领域信息,这种学习不需要任何与训练集、测试集领域相关的先验知识。此外,我们还提出了一种用词的领域分布来指导领域向量学习的方法。实验证明,我们提出的方法叮以在训练翻译模型的同时学习到词的领域纳构。</p>

<h2 id="14-论文组织结构">1.4 论文组织结构</h2>

<p>​		本文共分为六个章节,各章节的主要内容如下：
​		第一章绪论。本章首先介绍了论文的研究背景和研究意义,然后简要介绍国内外关于神经机器翻译的领域适应的研究进展,最后引出本文的研究内容并简要闸述本文的组织结构。
​		第二章相关知识介绍。本章主要对本文涉及到的相关技术和概念进行阐述。首先介绍了基于注意力的循环神经网络的神经机器翻译模型和基于自注意力的神经机器翻译 Transformer模型,然后介绍机器翻译的语料获取方法以及译文评测方法,最后介绍了一些本文应用的技术
​		第三章基于句子权重的神经机器翻译的领域适应方法。本章首先引出神经机器翻译的领域适应研究的必要性,随后介绍了机器翻译领域适应的相关工作。接着本章提出了基于句子权重的神经机器翻译的领域适应方法,并将其应用在两种场景中。最后在两种情景下验证所提出的方法的有效性并进行深入分析。
​		第四章基于自注意力机制的神经机器翻译的多领域适应方法。本章首先引出多领域神经机器翻译的研究意义,随后详细介绍了领域适应、多领域神经机器翻译的相关工作及二者的差异。接着针对多领域翻译场景,提出了一种基于领域感知的自注意力机制,并采用联合学习领域向量的方式训练多领域的神经机器翻译系统。最后通过实验来分析提出模型的效果并得出具体结论。
​		第五章基于无监督单词级别的神经机器翻译的多领域适应方法。本章首先引出现实应用场景下无监督的多领域神经机器翻译的重要性,然后介绍了相关工作。随后提出了一种无监督单词级别的神经机器翻译多领域适应模型,并进行实验验证和分析。
​		第六章总结与展望。本章首先对本文的研究内容进行总结,然后针对木文研究过程中存在的不足点,提出进一步的研究方向。</p>

<h1 id="第二章-相关知识介绍">第二章 相关知识介绍</h1>
<p>​		本章主要对本文研究涉及到的一些相关知识进行介绍,分为五个部分:第一部分介绍基于注意力的循环神经网终的神经机器翻译模型;第二部分介纲基于白注意( self-attention)的神经机器翻译 Transformer模型;第三部分介绍用于机器翻译的平行语料库和伪平行语料库的获取方法;第四部分介绍目前主流的机器翻译的译文评测方法;第五部分介绍本文第三章应用的相似度的计算方法。</p>

<h2 id="21-基于注意力的循环神经网络的神经机器翻译模型简介">2.1 基于注意力的循环神经网络的神经机器翻译模型简介</h2>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez8t7q8bfj30t80fu7h6.jpg" alt="image-20200520213651657" /></p>

<p>​		图2-1基于注意力的神经机器翻译流程框架图
​		如图2-1所示,基于注意力的神经机器翻译模型[3-4]建立在递归神经网络的基础上,采用编码器-解码器( Encoder- Decoder)的框架[1-2，32]。给定端输入x=(x,x2…xn)及其对应的H标端输出y=(n12…n),以最大化条件概率p(v|x)作为训练且标函数。为了解决基础的编码器-解码器框架随着句子输入长度的增加而迅速恶化的问题, Bahdanau、 Luong等人[3-4]提出了基于注意力的神经机器翻译慎型。</p>

<h3 id="211-编码器">2.1.1 编码器</h3>
<p>​		编码器使用双向循环神经网络( Bidirectional Recurent Neural Network,Bⅰ-RN)将源端句子x编码为隐藏状态序列h=,h2灬…,hn}。h是由前向隐层状态h和反向隐层状态h拼接得到的,即h=[h;]。每个隐层状态h是由前一个隐层状态h和当前词x计算得来的,如公式(21)所示:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez8wy9hrwj30tu01ut8r.jpg" alt="image-20200520214030224" />
​		其中,<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez8x7i6obj300w01ajr8.jpg" alt="image-20200520214045278" />是一个全零的向量。f是一个非线性激活函数,例如GRU[23]。</p>

<h3 id="212解码器">2.1.2解码器</h3>

<p>​		解码器读取隐层状态并预测目标端翻译y。基于当前循环隐层状态s,和获取源端信息的上下文向量c预测目标端每个单词y1。c1是双向隐层表示h的加权和。
​		给定源端句子x,根据公式(2-2)生成目标端句子y的概率。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez8xzccinj30t602kt8w.jpg" alt="image-20200520214129000" />
​		其中,S是解码状态,c是上下文向量,y是前一时刻生成的词。</p>

<h3 id="213-注意力网络">2.1.3 注意力网络</h3>
<p>​		解码器产生译文的过程中,需要用到上下文向量c在神经网络翻译模型中,c般使用注意力网络来获得,是源端语句隐层向量h=3,h2…hn}的加权和如公式
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez8yh0o24j30t802ajrg.jpg" alt="image-20200520214157807" />
​		s每个隐层状态h对应的权重an是经过一个两层的前馈神经网络φ,然后做softmax操作后得到的标准化输出,如公式(24)所示。叭s1,h)是注意力模型,计算j时刻生成词与源端第;个词的匹配程度。每个隐层状态h对应的权重a;表示目标端y与源端x对齐的概率。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez8yx1nvpj30tu02y0t2.jpg" alt="image-20200520214223161" /></p>
<h3 id="214训练目标函数">2.1.4训练目标函数</h3>
<p>​		神经机器翻译模型训练过程中,给定大小为N的平行语料,(x,y)是双语句对参数θ被训练来最大化条件对数似然函数。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez8zfqs7wj30tw02g3yq.jpg" alt="image-20200520214253160" /></p>

<h2 id="22-基于自注意力的神经机器翻译-transformer模型简介">2.2 基于自注意力的神经机器翻译 Transformer模型简介</h2>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez8zusnmdj30je0hyn3r.jpg" alt="image-20200520214317541" />
		图2-2基于自注意力( self-attention)的裨经机器翻译 Transformer模型流程框架图
		Transformer是-个完全基于注意力机制的编码器-解码器模型。 Transformer相对于基于注意力的循环神经网络的神经机器翻译模型具有两个显著优势:1)完全避免了循环网络,最大程度地并行化训练。2)利用注意力机制的优势,能获取长距离依赖。</p>
<h3 id="221编码器">2.2.1编码器</h3>
<p>​		在编码器中,几个相同的层彼此堆叠。每层包含两个子层,一个多头自注意子层( multi-head attention layer)和一个位置前馈层( position- wise feed- forward layer),它们通过残差连接与层标准化连接[26]。
###2.2.2解码器
​		解码器使用与编码器类似的层架构,除了使用掩码来避免对未预测的单词进行处理。解码器同样是由6层组成。类似地,解码器的每个层包含2.2.1节提到的两个子层,以及个多头注意力子层,接收来自相应编码器的输出。</p>
<h3 id="223多头注意力机制">2.2.3多头注意力机制</h3>
<p>​		注意力机制可以由公式(2-6)表示
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez913nib1j30sm01wjrm.jpg" alt="image-20200520214429328" />
​		其中, Attention的计算采用了点积的方式,如公式(2-7)所示
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez91b2c3vj30s802m74m.jpg" alt="image-20200520214441728" />
​		多头注意力机制则是通过h个不同的线性变换,对Q,K,V进行投影,最后将不同的 Attention的结果拼接起来,如公式(2-8)所示:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez91kxfudj30si01qq39.jpg" alt="image-20200520214456833" />
​		其中，<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez91vbx1kj30ck018aa7.jpg" alt="image-20200520214513492" />
​		多头自注意力机制则是取Q,K,V相同。
​		注意力机制在 Transformer中的应用有三种
​		1)编码器注意力:自注意力机制, query,key, value都是来自于同一个地方
​		2)解码器注意力:自注意力机制, query,key, value都是来自于同一个地方;
​		3)编码器解码器注意力: query来自解码器,key和vaue米自编码器。</p>
<h3 id="224前馈网络层">2.2.4前馈网络层</h3>
<p>​		前馈网络层采用了全连接层加Relu函数实现,公式如(29)所示:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez9676cg0j30su01m3yo.jpg" alt="image-20200520214923682" />
​		可以在每个子层的后面加个10%的 Dropout层,则子层的输出可以表示为
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez96vg3r8j30si01g74i.jpg" alt="image-20200520215002628" /></p>
<h3 id="225自注意力机制">2.2.5自注意力机制</h3>
<p>​		一般的注意力机制的源端和目标端的内容不同。自注意力机制发生在单个句子内,它的 Query属于Key,如公式(2-11)所示。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez9sntvdzj30t2022mxl.jpg" alt="image-20200520221058576" />
其中, Query=Key=Value。
​		自注意力机制有以下优点:
​		(1)可以并行化处理,计算自注意力机制不需要依赖其他结果。
​		(2)较低的计算复杂度,自注意力机制的计算复杂度是n2,而循环神经网终是nd2。其中,n是序列长度,d是词向量维度,·般d&gt;n
​		(3)自注意力机制能很好地捕捉仝局信息,词与词之间的跖离是1,计算词与词之间的关系时不需要依赖其他的词。</p>

<h3 id="226位置编码">2.2.6位置编码</h3>
<p>​		在解码器和编码器之前,先对数据进行位置编码。 Transformer没有使用循环神经网络在时间序列上对数据抽象,它运用位置编码来保留每个词的位置信息。
​		使用不同频率的正弦、余弦函数来生成位置向量,公式如(2-12)和(2-13)所示：
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez9tgfvbtj30t403s3z3.jpg" alt="image-20200520221144724" />
​		其中pos表示序列中单词的位置,i表示位置向量中值的维度,i&lt;dm位置向量的维度与单词向量的维度一致。</p>
<h2 id="23-平行语料库和伪平行语料库获取">2.3 平行语料库和伪平行语料库获取</h2>
<h3 id="231平行语料库获取">2.3.1平行语料库获取</h3>
<p>​		平行语料库指的是由源语言句子和其对应的译文组成的双语平行语料库。神经机器翻译系统会很大程度地受到语料库的质量以及规模的影响。语料库的规模越大、质量越高,训练岀的神经机器翻译系统的性能越好,译文质量越高。目前已公开的平行语料库有LDC双语平行语料库、 Europarl双语平行语料库、OPUS双语平行语料库等,规模达到上千万到数亿级别。</p>
<h3 id="232伪平行语料库获取">2.3.2伪平行语料库获取</h3>
<p>​		目前工业级的机器翻译主要采用的是平行语料的监督学习方法,但是对于小语种、特定领域来说,平行语料是一种稀缺资源。伪平行语料的获取通常是通过机器翻译将源文本转换为给定语言对的目标语言,以便获得新的平行语料库。许多研究发现伪平行数据与真实双语平行语料库相结合,能显著提高NMT模型的质量。此外伪平行数据在诸如领域适应等许多NMT间题中发挥了至关重要的作用[55]。
​		伪平行语料库的获取方式有多种,比如通过第三种语言的枢纽机器翻译( Synthetic Data for Pivot-based MT)[54]、回译( Back Translation)[56]等。通过第三种语言的枢纽机器翻译指的是使用称为枢轴语言(PL)的中间语言将源语言(SL)翻译为目标语言(TL)。与直接从SL转换为TL的典型MT系统不同,基于枢轴的系统从SL顺序转换为PL,然后从PL转换为TL。构建基于枢轴的MT系统的主要动机是缺乏语言对SL-TL的语言资源,而语言对SL-PL和PLTL的这种语料资源是可得到的。
​		回译是一种常用的基于单语的数据增强方法。首先,利用平行语料训练一个反向模型。然后,对于目标语言的单语数据,利用反冋模型将其翻译成源语言数捃,这样就伪造了一批新的平行语料(源语言句子是由翻译得来,目标语言句子是真实的单语数据)。最后,将伪平行语料加入训练。通过反向模型伪造的平行语料,虽然源语言句子质量不够好,但其目标语言句子是一个真实句子。这样,模型学习的方向还是朝着一个真实表达的句子的目标来进行。</p>
<h2 id="24译文评测方法">2.4译文评测方法</h2>
<p>​		目前主流的机器翻译的译文评测方法分为人工评测和白动评测,它能让科研人员直观观察出翻译系统的好与坏。
​		人工评测的两个主要标准是:忠实度和流利度。忠实度( Adequacy)主要反映机器译文与源语句的语义致性:流利度( Fluency)则是评价译文是否流畅,以及是否符合目标语言的表达习惯。人工评测的优点是结果比较准确,但是同时也存在很多缺点,包括评测成本高、周期长、评测结果较大稈度地受到评测人员的主观性影响、评价过程以及分数叮能无法复现。
​		自动评价的方法降低了人工评测的成本、缩短了周期,可以复现评测流程,但是准确率相对较低。机器翻译系统在调试时需要对译文频繁地评测以选取岀最优的系统,因此一般采用自动评测的方法。日前自动评测方法普遍使用的评估指标为BLEU[24],本文后续章节的实验均使用BLEU进行详文评测BLEU评测指标的基本思想是:将机器翻译系统给出的译文与人工的译文进行比较,机器生成的候选译文与翻译人员提供的译文越接近,正确率越高,BLEU的得分也越高。BLEU计算候选译文里的n元词语在参考译文中所占的比例[25],通常n为1-4,将一元语法到四元语法的平均值作为总的评价结東。BLEU评测的计算公式如(2-14)所示:</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez9xwhc9bj30so02iaaa.jpg" alt="image-20200520221601138" /></p>

<p>​		其中,N是最大n元语法阶数,w为杈重系数,p是n元词语在参考译文中所占的比例。BP是长度惩罚因了,由公式(2-15)计算得到
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gez9zlzch7j30ru02g0sv.jpg" alt="image-20200520221738935" />
​		其中,c为候选译文中单词的个数,P是机器译文和参考译文中同时出现的单词数量。</p>
<h2 id="25-相似度及距离的度量方法介绍">2.5 相似度及距离的度量方法介绍</h2>
<p>​		常见的相似度算法有余弦相似度( Cosine similarity)、皮尔森相关系数( Pearson Correlation Coefficient)、 Jaccard相似系数( Jaccard Coefficient)、 Tanimoto系数(广义 Jaccard相似系数)、对数似然相似度、互信息/信息增益,相对熵/KL散度( Kullback-Leibler Divergence)、信息检索-词频逆文档频率(TF-IDF)、词对相似度-点间互信息等。
​		常见的距离算法包括:欧氏距离( Euclidean distance)、马哈拉诺比斯距离( Mahalanobis distance)、曼哈顿距离( Manhattan distance)、海明距离( Hammingdistance)等。
​		以下介绍其中与本文相关的相似度及距离度量方法
​		1)KL散度
​		KL散度又叫相对熵,是一种量化两种概率分布P和Q之间差异的方式。在概率学和统计学上,我们经常会使用一种更简单的、近似的分布来替代观察数据或太复杂的分布。K-L散度能帮助我们度量使用一个分布来近似另一个分布时所损失的信息。
​		设p为观察得到的概率分布,q为另一分布来近似p,则p、q的KL散度由公式(2-16)计算得到
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geza7pgijmj30se02gmxg.jpg" alt="image-20200520222526523" />
​		2) JS散度( Jensen-Shannon Divergence)[27]
​		JS散度度量了两个概率分布的相似度,是基于KL散度的变体,解决了KL散度非对称的问题。一般地,Js散度是对称的,其取值是0到1之间。定义如下:
​		设p为观察得到的概率分布,q为另一分布来近似p,则p、q的JS散度13.公式(2-17)所示
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geza88p3ilj30sg02qwes.jpg" alt="image-20200520222556830" />
​		3)交叉熵( Cross-Entropy)
对一随机事件,其真实概率分布为p,从数据中得到的概率分布为q,则我们定义,交叉熵为:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geza8gmhx9j30rw0283yn.jpg" alt="image-20200520222609774" />
​		机器学习中常用交叉熵作为损失函数。常见的做法是先用 Softmax函数将神经网络的输出转换为概率分布,然后用交叉熵刻画估算的概率分布与真实的概率分布的距离
​		4)欧氏距离( Euclidean Distance)[53]
​		欧氏距离是一个通常用的距离定义,它是在m维空间中两个点x和y之间的真实距离。公式如(2-19)所示:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geza8vphmmj30sc02wmxb.jpg" alt="image-20200520222633819" /></p>

<h1 id="第三章基于句子权重的神经机器翻译的领域适应方法">第三章基于句子权重的神经机器翻译的领域适应方法</h1>
<h2 id="31引言">3.1引言</h2>
<p>​		神经机器翻译(NMT)相对于统计机器翻译(SMT)取得了更令人满意的表现。但是,以数据驱动的NMT的翻译质量严重依赖于训练数据的规模以及训练数据与领域内测试数据的相关程度,NMT在领域适应方面仍面临着巨大挑战。训练语料通常具有领域多样性,与目标领域相关的那些领域外的实例通常对于模型的训练有益,而那些与目标领域不相关的领域外数据可能会降低翻译质量。NMT中通常使用的领域适应方法有微调,即在用领域外数据训练好的翻译模型的基础上,用领域内的数据微调现有的领域外模型。但是,微调没有考虑到不相关的领域外数据对模型质量造成的损失。由于领域内数据的规模通常很小,微调的方法容易造成过拟合
​		本章提出了一种基于句子权重的领域适应方法,根据句子与目标领域的相关程度来评估句子的权重。领域相似度已经成功地使用在句法分析、知识适应等任务中[29-30]。本章引入了一个领域相似性度量标准来衡量一个句子和整个领域数据集的相关程度,每个句子与目标领域的相似性用了多种方法评估,计算所得的相似度作为句子的权重被融入训练目标函数中。用领域相似度衡量领域外句子与领域内数据的相关性,以权重的方式综合考虑到领域外数据对目标领域的有利影响和有害因素。此外,与现有的NMT句子加权方法不同的是,本章提出的方法不需要借助额外的工具包的训练,如SRI语言模型工具3或RNN分类器,而是利用NMT系统训练出的参数作为为句子赋予权重的依据。本章的方法同样适用于低资源领域的翻译14749,在只有合成的伪平行语料的情景{40.454下得到了有效验证。它同样可以被用在反向翻译{434中,与和其他的训练方法相结合使用。
​		实验在中英 IWSLT翻译任务上获得了明显提升。相较于以前的句权重方法,本章提出的方法在4个公开测试集上的平均BLEU值提升了1.1个点,超过 Wangle[14]的方法1.42个BLEU。在只有伪平行语料的翻译任务中,实验同样验证了本章提出的基于句子权重的方法在领域适应上的有效性。
​		在本章后续内容中,首先介绍和本章内容联系紧密的相关研究,然后详细介绍提出的基于句子权重的领域适应方法,其中包括标准情景的领域适应下和仅有伪平行语料情景下的句子加权方法。随后介绍实验设置和分析,最后对本章工作进行总结。</p>
<h2 id="32相关工作">3.2相关工作</h2>
<p>​		在统计机器翻译中,领域适应已有着广泛的研究,但在神经机器翻译领域中,领域适应仍是一个具有挑战性的问题。领域适应问题的研究方法可以被分为两个大类:模型层面和数据层面。
​		在模型层面,统计机器翻译可以采用以赋予杈重的方式结合多个模型来解决领域适应问题。在神经机器翻译中,针对领域适应问题, Luong和 Manning[4]等人提出了微调、模型叠加、多模型融合等方法。 Luong和 Manning提出的微调的方法,是在已经训练至收敛的领域外的神经机器翻译模型上,使用领域内的语料继续训练。这种方法的一个显著缺点是该模型很容易在领域内数据上过拟合。具体来说,神经机器翻译模型在领域外测试集上的翻译性能严重下降。研究人员的实验表明,领域外与领域内混合训练的方法和用领域内语料微调的方法相比较,当领域内数据量不是很小时,混合训练可以获得更好的翻译性能,反之,则使用微调的方法能更明显地提升翻译效果。模型叠加的方法是将多个不同的领域按与领域内数据的相近程度排序,连续训练多个神经机器翻译模型,最开始在和领域内语料最不相近的领域上训练模型,接着用更相近的领域微调,最后以在领域内微调而结束。多领域融合的方法是在可得到的领域上分別训练模型,在神经机器翻译解码过程中,使用平均或加杈重的方式,融合多个翻译模型进行翻译。
​		在数据层面,传统的领域适应方法有基于数据选择的方法、基于数据权重的方法、基于混合模型的方法等。基于数据选择的方法是从可得到的领域外语料中选择定比例的与领域內数据接近的语料来训练翻译系统。通常,叮以通过领域外和领域內语料训练的模型为领域外数据赋予权重。 Axelrod等人[12]用语言模型来给训练语料评分,他提出的利用双语的交叉熵差的评分方法提升了翻译性能。基于数据权重的方法是给训练语料、句子或短语不冋的杈重,来训练统计机器翻译模型。
​		一些统计机器翻译中运用的方法,如基于数据选择的方法等,可以应用在神经村器翻译中,如 Wang[14]在神经机器翻译中釆用了基于数据选择的方法,测量领域外和领域内句子向量的相似程度。但是也有些领域适应的方法,如基于数据权重的方法等,在神经机器翻译中的应用仍是具有挑战性的,因为神经机器翻译不是一个线性模型或其组合。最近提出的将带有权重的句子融入神经机器翻译训练的方法由 Wang[14]、Chen[12]等人提出,他们将神经机器翻译的目标函数更新为带权重的方式,即在计算每个小批的梯度下降中,加入每个句子对应的权重。 Wang[14]用了 Axelrod[12]在传统神经机器翻译的数据选择的方法来给句子加上对应的权重。Chen[12]训练了一个分类器来给每一个句对进行评分。
​		本章提出一种新的基于句子权重的领域适应方法,提出了领域相似度的评估方式和句子权重的生成方法,然后将权重信息通过改变反向传导时梯度的方式融入进NMT模型中。和Wang!l4提出的方法相比,本章提出的方法获得了更优的性能。</p>
<h2 id="33基于句子权重的方法">3.3基于句子权重的方法</h2>
<p>​		本章提出的基于句子权重的方法在两个不同的情景下得到验证:一个是标准的领域适应的情景,即将一个在丰富资源的领域外语料上训练的模型用不充足的领域内数据来适应目标领域;另外一种情景是,低资源的领域翻译的任务,目标领域只存在可以获得的真实的单语数据,构建伪平行双语语料的方式进行训练。对于后者,所有的数据都是来自于同一个领域,但是存在着真实语料与伪语料的不匹配性。对于这样的翻译任务,我们可以将开发集看作领域内数据,将整个训练集看作领域外数据,尽管实际上所有的语料都属于同一领域。</p>
<h3 id="331标准领域适应下基于句子权重的方法">3.3.1标准领域适应下基于句子权重的方法</h3>
<p>​		本节介绍标准领域适应情景下的基于句子权重的方法。我们根据领域外的句子与领域内数据的领域相似性为它们赋予权重。我们将一个领域的语料中的句子分布作为这个领域的表示。我们假设在领域空间中的所有句子向量的平均代表了这个领域的中心。以这种方式,我们定义一个句子和一个领域间的相似性的度量为一个句子和这个领域的中心的相似程度计算。句子向量s是解码端的初始状态的隐层表示。我们可以用欧式距离来计算句子和领域的相似度,我们也提出了一种用JS散度衡量句子和领域间相似度的新方法。
​		我们使用 softmax函数将句子向量转化为一个概率分布。给定句对数为N的领域语料,s是由编码端输出的句子向量转化得到的输入解码端的初始隐层状态。句子的概率计算如公式(3-1)所示:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezapa7aeuj30oi02kwem.jpg" alt="image-20200520224219496" />
​		基于这个概率分布,我们选择JS散度为度量句子与领域间相似度的函数。JS散度度量了两个概率分布的相似度。JS散度是由q和r与它们的平均数的KL散度计算而来。我们使用JS散度公式:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezapmabmkj30oc0223yq.jpg" alt="image-20200520224238656" />
​		D(q‖r)是两个概率分布q和r间的KL散度,是两个概率分布之间“距离”的经典度量方法,定义为:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezapzarnyj30ok02k3yo.jpg" alt="image-20200520224258995" />
​		我们用JS散度依照公式(3-4)给领域外的句子打分
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezaq45zd3j30oc01kjrf.jpg" alt="image-20200520224307618" />
​		其中,JS(σ、)是句子表示σ和目标领域表示之间的JS散度。JS0(a,)是句子表示σ,和领域外数据之间的JS散度。
​		实际上,我们使用双语的JS散度来计算领域相似度,在双语的JS散度中我们考虑到源端和目标端的数据与目标领域的相似度,计算如下:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezaqfv36wj30nw01gdg0.jpg" alt="image-20200520224325964" />
​		w(s)是ax,的最小最大化标准化的输出结果,它的范围在[0,1],评估句子s的权重
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezaqq5tcnj30o002gmxf.jpg" alt="image-20200520224343141" />
带有句子权重的NMT的训练目标函数如下:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezaqx4l2nj30p001oaa5.jpg" alt="image-20200520224353668" /></p>
<h3 id="332伪平行语料训练神经机器翻译中基于句子权重的方法">3.3.2伪平行语料训练神经机器翻译中基于句子权重的方法</h3>
<p>​		在领域适应中,我们测量了领域外数据和领域内数据的相似度,我们同样把基于句子权重的方法用在只有伪平行语料训练的NMT的情景中。伪平行语料是由真实的单语语料和对应的伪语料合成的句对,它可以是源于源端的、源于目标端的,或者是他们的混合[41-42]。在低资源领域翻译任务中,我们衡量训练数据中的句子与开发集的相似度。我们的双语JS散度的方法如公式(3-8)所示:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezarklickj30oc01caa4.jpg" alt="image-20200520224431574" />
​		我们将所有的低资源的训练数据当作领域外的数据。为了提升翻译质量,在这种情景下使用在标准化的概率加1的损失加权的方法,给原有的训练数据一定程度上的奖赏。我们使用如公式(3-9)所示的目标函数:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezarud66jj30oy01uq32.jpg" alt="image-20200520224447038" /></p>
<h2 id="34实验设置与分析">3.4实验设置与分析</h2>
<p>​		本章提出的方法在两个场景下进行评估:标准的领域适应和可以被转换成领域适应任务的伪平行语料的翻译。本小节详细说明了实验设置和实验结果。</p>
<h3 id="341领域造应实验设置">3.4.1领域造应实验设置</h3>
<p>​		对于中英翻译的领域适应任务,我们使用公开数据集LDC新闻语料作为领域外数据, IWSLT2017(主要包含TED演讲)作为目标领域语料。其中,LDC双语语料包含125M句从LDC语料中抽取的句对,有27.9M个中文单词和34.5M个英文单词。IWSLT2017语料包含210k句对的训练集和0.k句对的开发集。我们选择TEDTST2010、 TEDTST2011、TEDTST2012、 TEDTST2014数据集作为测试集。本任务的数据如表3-1所示。
表3-1中英 IWSLT领域适应任务的数据集</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezasht1pzj30ig0bygn5.jpg" alt="image-20200520224525142" />
		我们使用不区分大小写的4- gram NIST BLEU评分作为我们的评估指标,“ mteval-vlb-pl’”脚本来计算BLEU分数。为∫有效训练神经网络,我们将源端(中文)和日标端(英语)词汇限制为最常见的30k字,分別涵盖两个语料库的约97.7%6和993%的单词。所有词汇表外的单词都被特殊字符UNK取代。单词向量的维度为620,隐藏层的大小为1000,所有其他设置与 Bahdanau等人的:作围相同。我们用来训练NMT模型的中文和英文端的句子的最大长度设定为50。另外,在解码过程中,我们使用了東搜索算法并设置了束大小为10。根据在开发集上的最大BLEU值选择模型参数。</p>

<h3 id="342领域适应实验结果">3.4.2领域适应实验结果</h3>
<p>​			实验结果如表3-2所示。其中,“In+out”方法是将领域內和领域外语料混合作为训练语料。我们同样对比∫Wang的基于句了权重的方法(后文简称WM),wang[14]用·个语言模型来给句了评分。“JS”和“ED”是在第3.3.1节中提到的用JS散度和欧式距离度量的领域相似度方法。
表3-2中英 TWSLT领域适应任务的BLEU值结果
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezau05aobj30p207c761.jpg" alt="image-20200520224651484" />
​		从实验结果可以看出,“JS”方法比“In+out”方法提升了0.31-1.68个BLEU值,超出 Wang[14]的基于句子权重的方法039-1.09个BLEU,在四个测试集上的平均结果超出WM0.63个BLEU。本章提出的“ED”的方法的实验结果比“In+Out”方法提升1.38-1.92个BLEU,超出WM1.10-1.71个BLEU,平均获得142个BLEU值的提升</p>
<h3 id="343伪语料神经机器翻译实验设置">3.4.3伪语料神经机器翻译实验设置</h3>
<p>​			我们从电商网站抓取了中英文单语的句子,包括京东、苏宁、Ebay、 Amazon等网站。我们调用 Google翻译API将爬取的中文句子翻译成英文,将爬取的英文句子翻译成中文,构成源端或目标端真实的伪平行语料。经过过滤后的伪语料句对数为720k,中英文均做BPE处理。我们将这些爬取的和机器翻译的语料构成的伪平行双语语料作为电商实验的训练数据。开发集和测试集都来自阿里巴巴,其中开发集500句,测试集1500句。除了本实验模型用的是 Adadelta优化算法,所有模型参数的设置都和34.1节中相同。</p>
<h3 id="344伪语料神经机器翻译实验结果">3.4.4伪语料神经机器翻译实验结果</h3>
<p>​			本实验的基线系统是用全部的伪平行语料训练得到的。我们在实验中同样使用了wang[14]的方法。“JS”方法相比于基线系统获得了0.7个BLEU的提升,超过WM方法044个BLEU。“ED”方法的实验结果同样超过了WM方法。
​		此外,受3.2节中提到的模型叠加方法的启发,我们提出了基于权重的模型叠加的方法。我们依照训练语料与开发集的相似程度,依次训练几个模型。首先我们采用332节中提出的方法为句子计算权重,并将句子按照权重大小排序。我们先用全部的训练语料训练翻译模型。与模型叠加的方法类似,接着从按照权重排好序的句子中,挑选权重较大的一半语料进行下一轮训练。然后再以相同的方式挑选四分之一权重较大的语料继续训练。这种基于权重的模型叠加的方法相比基线系统在测试集上提高了1.15个BLEU。
​		中英电商领域的实验结果如表3-3所示:
表3-3中英电商领域伪语料翻译任务的BLEU值结果</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezavg4ebwj30pc08ymyi.jpg" alt="image-20200520224815295" /></p>
<h3 id="345术语适应的分析">3.4.5术语适应的分析</h3>
<p>​		在领域适应中,我们的日标之一是扩展通用NMT模型以涵盖目标领域的术语和使具备目标领域的表达风格8本小节分析了中英 IWSLT领域适应实验中的术语分布,计算了领域外和领域内的术语在测试集翻译译文中的占比。我们将两个领域的单词的差集构成各个领域的术语表。目标领域的测试集的译文中包含的领域内术语越多、领域外的术语越少,则该方法的领域适应性越好。表3-4的结果显示,在BLEU表现最佳的“ED”方法成功地降低」领域外术语的比例(从0.37%到0.15%),并增加了领域内术语的比率(从44.28%到45.37%),表明了该方法的术语适应的能力
​		表3-4每种方法在测试集译文上的领域术语的占比</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezawjtb5vj30pe06s3zq.jpg" alt="image-20200520224919234" /></p>

<h3 id="346句子表示的可视化分析">3.4.6句子表示的可视化分析</h3>
<p>​		本章提出的相似度测量方法的有效性可以由可视化论证。相似的句子具有相似的向量表示5。在图3-1中,每个点代表着一个句子向量表示的二维投射,句子向量是由经过 softmax标准化的输出σ、。从领域外语料(LDC语料)中随机挑选权重大约为0、0.5和0.9的三组句子,每组20句,从领域内数据中也随机挑选20个句子。
​		从图中可以看出,领域内句子散布在目标领域的中心周围,领域外句子同样散布在领域外中心的周围,验证了3.3.1节中将句子向量表示的平均作为领域的中心点的假设。此外,从句子向量的可视化中可以看到,本章提出的领域相似度衡量方法是合理的,聚在一起的句子有着相似的权重,相距较远的句子权重的差别较大。此外,句子表示越接近目标领域中心,它被赋予的权重越高。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezaxapavuj30i60ee410.jpg" alt="image-20200520225001636" /></p>

<p>图3-1用tSNE将领域内和领域外句子表示可视化。图片中的图标“in”、0、0.5、09分别代表了领域内的句子、领域外权重为0、0.5、09的句子。图标“ In core”
		表示领域内语料的中心,“ out core”表示领域外语料的中心</p>

<h3 id="347翻译实例的分析">3.4.7翻译实例的分析</h3>
<p>​		从表3-5中的两个翻译实例可以更深入地了解本章提出的方法是怎样帮助模型适应日标领域的。表3-5的第个例子中,参考译文里问候语“what’sup”是‖正式场合的表达,“Ih+Out”和“ Wang et al.”|4的方法生成的译文“ ladies and gentlemen”表达较正式。而“Js”方法生成的译文“ you guys”符合非正式场合。“JS”方法将官方正式的LDC凤格的長达向凵语领域风格适应。
​			表3-5中的第二个翻译实例的源句和参考译文是虚拟语气,描述在现实生活中不叮能发生的事情。“ED”和“JS”方法生成的译文都是虚拟语′(且表达的意思与译文·致。另·方面,“hn+Out”和“ Wang et al.”|4的方法都没有学习到正确的语态因为LDC的训练语料是新闻领域,该领域·般不使用虚拟语’的長达,而I领域的長达方式相对随意。“ED”和“JS”方法都做到」使模型问冂语领域适应。
表3-5测试集及翻译样例。“ln+Out’”表示由“n+Out’方法生成的译文,“Wang”表示Wang提出的基于句子权重的方法生成的译文。“JS”、“ED”代表了“JS”、“ED”方法生成的译文。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezaz1aijij30p00kcgq2.jpg" alt="image-20200520225142288" /></p>

<h2 id="35本章小结">3.5本章小结</h2>
<p>​		本章提出了一种基于句子权重的神经机器翻译的领域适应方法,并应用在两个不同的场景。计算得来的相似度评分作为句子的权重进一步融入训练目标函数。实验结果和分析表明,提出的方法在标准的领域适应和伪语料训练的低资源领域翻译两个不同的场景上都获得了提升。在接下来的研究中,我们希望探索更准确的领域相似性度量方法,和其他基于权重的NMT领域适应方法。</p>
<h1 id="第四章-基于自注意力机制的神经机器翻译的多领域适应方法">第四章 基于自注意力机制的神经机器翻译的多领域适应方法</h1>
<h2 id="41引言">4.1引言</h2>
<p>​		从人工和自动评测指标来看,神经机器翻译在翻译质量上已经取得了显著进步佤32,尤其是在新闻领域34。这种成功得益于先进的神经架构和大规模可得到的领域内训练数据。然而,神经机器翻译在领域适应方面仍然面临着挑战。一方面,对于特定领域的翻译,无法得到充足的领域内数据始终是许多语言对翻译面临的问题;另方面,数据的领域多样性会导致特定领域的翻译质量较差,不同领域的句子可能有不同的句法结构和风格,同一个单词在不同领域中也可能表达不同的词义。因此,对于NMT,考虑领域信息是有必要的。
​		本章我们考虑一个领域相关的开放性问题:怎样构建一个领域感知的翻译系统,较好地进行多个领域的翻译。
​		传统的多领域翻译方法是首先训练通用模型,然后在特定领域上微调以最大化其在特定领域上的性能,或者是对不同的领域训练多个模型进行模型融合。这两种方法都需要构建多个特定领域的翻译模型,在工业场景中会增大翻译系统的维护费用
​		本章尝试采用联合学习领域向量的方式训练多领域的神经机器翻译系统。学习到的领域向量表示和单词向量表示·起融入NMT的编码端和解码端。通过这种方式,我们强制NMT模型同时编码和解码语义信息和领域信总,用η个NMT模型进行多个领域的翻译仟务。
​		尽管这种思想对于编码器-解码器框架的NMT是通用的,本章将在日前代表最先进水平的神经网络架构 Transformer上进行领域感知的NMT模型的研究。为了在训练NMT模型时联合学习领域的表示,本章提出了基于领域感知的自注意( domain- aware self- attention,DSA)机制。在DSA中,每个单词有着词的语义向量和·个领域的向量。领域向量是这个词来自的领域的向量化表示,同时领域向量也被用作领域感知的白注意力机制的键和值。为了学习领域向量,我们用每个句了的领域信息作为该句中每个词的领域信忌。本章提出的方法适用于训练数据和测试数据中的句子领域信息可获得的情景。我们将提出的方法在中英和英法任务上均做了实验,实验结果验证了基于自注意力机制的神经机器翻译多领域适应方法的有效性。
​		本章提出的方法的贡献在于,我们为多领域翻译提出了一种基于领域感知的自注意力机制。据我们所知,这是在基于注意力机制的多领域NMT上的首次尝试。同时,实验和分析也验证了我们的模型能够显著提升各领域的翻译效果并且可以学习到训练数据的领域信息。</p>

<h2 id="42相关工作">4.2相关工作</h2>
<p>​		本节总结了最近NMT领域适应和多领域NMT的相关工作,并强调了它们的差异
​		(1)NMT领域适应
​		NMT的领域适应研究中,微调、数据选择、数据加权等方法被提出。 Luong和Manning[4]提出微调的方法,在已经训练好的领域外翻译系统上用领域内的语料继续训练。Chu等人s使用混合微调的方法,在领域内和领域外的混合语料训练的模型基础上进行微调。wang等人采用了数据选择的方法做领域适应,用句子向量衡量句对和领域内语料的相似性。句子加权的方法是在NMT训练中,在批量计算损失时,目标函数根据句子的权重进行更新。Wang等人4提出了两种加权方法:句子加权法和带有动态加权策略的领域加权方法。Chen等人2在更新神经翻译模型参数时,根据领域相似性利用分类器产生的概率来给句子加权。 Zhang和Xong提出了一个领域相似度的衡量标准,度量一个句子与整个领域数据集的相关性,并以权重的形式融入到训练目标函数中。
​		(2)多领域NMT
​		在统计机器翻译中,不同领域训练的翻译模型以加权的方式融合起来。在神经机器翻译中, Sajad等人探索了模型叠加和多领域的多模型融合方法。 Kobus等人s提出了在字向量层用相应的领域特征来对领域控制。 Farajian等人2)为学习算法动态设置超参数,并利用每个测试句子和训练实例之间的相似性来更新通用模型。 Britz2l等人提出了三种模型来联合学习领域的判别和翻译,这三种方法分别是判别的混合模型、对抗的混合模型和目标端标记的混合模型。Zeng等人0的多领域NMT方法是通过构造领域特有的和领域共享的表示来区分和利用词级别的领域,并在训练目标函数中调整目标词的权重。本章提出的方法与他们不同的是,我们自动学习∫领域向量并将其融入自注意力机制中。</p>
<h2 id="43神经机器翻译的多头自注意力机制">4.3神经机器翻译的多头自注意力机制</h2>
<p>​		为了使NMT模型具有不同的注意力结果,或从不同的表示子空间中得到注意力信息,多头自注意力机制被广泛使用。
​		每个多头自注意力层使用h个注意力头。多个头的输出结果融合得到多头自注意力的最终输出
​		给定个n个元素的输入序列x=(x1…,x,),其中x∈R”,维度是d,单头自注意力机制层的输出是输入(即值)的线性转换的加权和,计算公式如(41)所示:
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbdix70cj30os02ct8r.jpg" alt="image-20200520230537384" />
​		注意力的权重a测量查询x和键x,之间的相关度,可以由查询x和键中各个元素k,求内积后通过 softmax函数输出。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbdulcmuj30om05kjrt.jpg" alt="image-20200520230555935" />
​		其中,WO,W,W∈R”“是查询、键、值的映射矩阼,叮以在训练中学习得到。</p>
<h2 id="44基于自注意力机制的神经机器翻译的多领域适应">4.4基于自注意力机制的神经机器翻译的多领域适应</h2>
<p>​		本节首先介绍基于 Transformer的多领域NMT的总体架构,然后详细介绍DSA模型和领域向量的学习方法。</p>
<h3 id="441基于自注意力机制的神经机器翻译的多领域适应的总体架构">4.4.1基于自注意力机制的神经机器翻译的多领域适应的总体架构</h3>
<p>​		基于自注意力机制的NMT的多领域适应的总体架构如图4-1所示,目的是使Transformer对输入序列的领域信息进行编码,并使用这些被编码的领域信息对特定于领域的目标端进行解码。为此,我们对 Transformer进行了两项重要的改变,以便在单个NMT模型中实现多领域翻译。第一个变化是基于领域感知的自注意力机制,其中领域表示被添加到原始的自注意力机制的键和值向量中。特别地,基于领域感知的自注意力机制函数的输出是融入了领域信息的值的加权和。注意力机制的权重是查询和领域感知的键的相关程度。基于领域感知的自注意力机制可以用在编码器、解码器或同时用在编码端和解码端的自注意力层。在我们的初步研究中,我们发现在编码器和解码器的自注意力层中同时使用基于领域感知自注意力机制比其他情况更好。第二个变化是添加一个领域表示学习模块来学习领域向量,这将在第44.3节中介绍。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbf17fydj30h20dgtgi.jpg" alt="image-20200520230704699" /></p>

<p>​		图4-1基于 Transformer的多领域NMT的架构</p>
<h3 id="442领域感知的自注意力机制">4.4.2领域感知的自注意力机制</h3>
<p>​		我们假设序列中的每个元素都有一个领域表示。因此,基于领域感知的自注意力机制中的键和值向量是原始向量和领域表示的组合。我们让与x相同长度的序列z=(21…,x,)成为对应于x的领域表示。维度为d2的z(∈R)是元素x对应的领域表示。如果我们不添加额外的线性变换,则x和z的维度可以相同(即,d.=d)。通过强制所有元素具有相同的领域表示,即21=z2=…=zn,得到句子或文本级别的模型
​		提出的领域感知的自注意力模型如图4-2所示。通过调整公式(4-1)将输入元素x,对应的领域表示包含进来,DSA的输出元素o可以如公式(4-4)计算
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbg1psdnj30oa01wwek.jpg" alt="image-20200520230803028" />
​		直观地,输出将包含来自x的语义信息和来自z的领域信息。我们希望如果在编码器中使用领域感知的臼注意力机制,领域信息可以帮助消除源端单词的歧义,如果它被用在目标端,领域信息能够帮助选择正确的目标词。
​		同样,计算查询和键向量之间的相关度的函数也做了变化,为将领域表示考虑在内,如下所述。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbgmecnsj30ny02c74f.jpg" alt="image-20200520230835953" />
​		其中,,,∈R以、w,w!∈R是要学习的转换矩阵。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbgziqd6j30d20bewgr.jpg" alt="image-20200520230857018" />
​		图4-2基于领域感知的自注意力机制</p>
<h3 id="443句子级领域信号监督的领域表示学习">4.4.3句子级领域信号监督的领域表示学习</h3>
<p>​		基于领域感知的自注意力机制的关键是学习领域的表示z。在本节中,我们提出了句子级领域信号监督的的方法来学习领域的表示。
​		假设我们有来自训练集和测试集的句子的领域ID,我们可以使用这些域DD作为信号来监督领域表示的学习。设N是领域类型的数量。我们随机初始化一组向量表示/={1…,l},其中l∈R是领域i∈{1,}的向量表示。在训练阶段,如果句子的域⑩是i,我们设置z=…=zn=l。我们继续训练整个多领域NMT模型,领域向量={1…,}将一次又一次地更新,类似于字向量,直到收敛。通过这种方式,我们可以自动学习所有领域类型的向量化表示,这些表示将在解码阶段根据源句子的领域ID使用。</p>
<h2 id="45实验设置与分析">4.5实验设置与分析</h2>
<p>​		为了检验所提出的基于自注意力机制在翻译场景下多领域适应的有效性,我们进行了多领域中英和英法翻译的实验。</p>
<h3 id="451实验设置">4.5.1实验设置</h3>
<p>​		对于中文英语(ZHEN)多领域翻译任务,我们使用了来自不同领域的公开数据集:WMT2018的新闻评论数据 news-commentary, UM-Corpus的法律领域Laws和论文领域 Thesis以及 IWSLT2017的口语领域 IWSLT数据。为了构建ZHEN任务的开发集,我们使用了 IWSLT语料库的公开开发集中的所有句子,以及从news-commentary,Laws和 Thesis语料库中随机选择的句子。同样,我们使用了IWSLT,Laws和 Thesis语料库的公开测试集中的所有句子,以及从新闻评论语料库中随机选择的句子来构建我们的测试集。表4-1列出了该任务的训练集、开发集和测试集的数据统计
​		表4-1中英任务上数据统计</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbi6ls5tj30p0098myo.jpg" alt="image-20200520231006135" /></p>

<p>​		对于英语法语(EN-FR)的多领域翻译任务,我们的数据来自公开数据集OPUS, IWSLT2017和WMT2015。训练数据由三个不同的领域组成:欧洲中央银行(ECB),KDE4, IWSLT。我们添加了WMT2015的 News Commentary(此后是 WMT nc),Common Crawl( Common.)和 Europarl语料库作为此任务的通用数据。对于每个领域,我们随机选择500个句对添加到我们的开发集中,并将1000个句子对添加到测试集中。为∫分析系统在通用数据上训练的性能,从WMT2015测试集中随机选择了500和1000个句对作为附加的开发和测试数据集。表4-2列出了该任务数据的统计数据。
​		表4-2英法任务上数据统计
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbipd2zij30pa0aqwg5.jpg" alt="image-20200520231036085" />
​		值得注意的是,来自不同领域的训练数据的数量彼此相当。因此,在我们的实验中,所有领域都被平等对待,而不是一个是丰富资源的领域,另一个是低资源领域。通过这样做,我们可以避免从资源丰富的领域到低资源领域的迁移学习在提高低资源领域翻译质量方面的主导地位。这并不是说我们的模型不能将知识从一个领域转移到另一个领域。相反,我们的模型共享所有领域的参数,这迫使模型跨领域地推广和传递知识。
​		我们使用 tensor2 tensor库来实现我们的多领域NMT系统,包括基线系统和通过提出的基于领域感知的自注意力机制的方法得到增强的系统。我们使用了Adam优化器3,6层的编码器和解码器,8个注意力头,1024维的前馈隐层,并设置了 dropout=0.1。我们设定了领域表示与句子向量的维度相同(d.=d=512)。中英实验的词表大小均设置成3000,英法实验为40000。我们使用与 Vaswani等人相同的学习率预热和衰减策略。解码时我们使用束搜索,束搜索的窗口大小设置为6,并将BLEU-4作为评估指标。</p>

<h3 id="452实验结果">4.5.2实验结果</h3>
<p>​		中文-英语和英语-法语任务的结果分别显示在表43和表44中。我们将本章提出的模型与以下两个系统进行了比较
​		(1)基线系统是 Transformer,它是在混合的来自不同领域的所有训练数据上训练得到。
​		(2)另一个系统是 DomainTag,这是一个在所有领域数据上训练的 Transformer系统,训练数据的每个句子有领域ID的标注。在Tars和 Fishell36l之后,我们将领域视为标记,并将相应的领域ID附加到训练集的每个句子中。
​		在中文-英语任务中,本章提出的基于自注意力机制的NMT多领域适应方法(DSA-NMT)在所有领域上获得了超过基线系统上的137BLEU的平均提升。这些结果表明,提出的基于领域感知的自注意力模型能够将领域信息融入到NMT中,并且能够使用单个NMT模型进行多领域翻译。
​		遗憾的是,将领域标记附加到训练数据 Domain Tag方法6,对于 Transformer不起作用。它甚至比基线系统差得多。Tars和 Fishel36在基于RNN的NMT系统上使用DomainTag方法,而我们在基于注意力机制的 Transformer上使用它。向序列附加标记的方法可能不适合注意力机制的架构。
​		在英语法语任务中,遵循 Farajian等人四2的做法,我们使用WMT数据(包括WMT nc, CommonE., Europarl)作为额外的资源丰富的领域,并将此领域数据添加到我们的训练数据中,以训练鲁棒的NMT系统。表4-4中的结果显示了与我们在中英翻译任务中观察到的类似趋势。基于自注意力机制的NMT多领域适应方法在基线系统的基础上实现了三个领域上平均1.48BLEU的提升。
​		表4-3中英任务上不同模型的实验结果</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbk5t086j30oc06cjss.jpg" alt="image-20200520231200011" /></p>

<p>​		表4-4英法任务上不同模型的实验结果
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbkgfll4j30oe06gwfj.jpg" alt="image-20200520231217261" /></p>

<h3 id="453实验分析">4.5.3实验分析</h3>
<p>​		我们对编码器学习的句子表示进行了分析。我们使用tSNE3将中英文测试集中源端句了的表示投影到二维空间中。图4-3显示了基线系统和领域感知的自注意力机制模型所学习的这些表示。我们可以观察到基线系统难以区分句了所属的不同领域。但是,我们的模型能够将属于一个领域的句子聚集在··起,且不同领域间只有较明确的边界。我们相信,我们的模型所学习的领域信息可以使解码器在翻译中受益。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gezbl232c9j30ic0os7en.jpg" alt="image-20200520231251889" /></p>

<p>​		图4-3基线系统(a)和基于领域感知的自注意力机制的系统(b)学习的源句表示的可视化。紫色: Thesis,深蓝色:News,绿色: IWSLT,黄色:Laws</p>
<h2 id="46本章小结">4.6本章小结</h2>
<p>​		在本章中,我们提出了一个新的多领域NMT框架,该框架使用单个模型处理多个领域,联合学习领域表示并融入到领域感知的自注意力机制中。不同于以往的NMT领域适应的相关研究,我们在目前代表最先进水平的神经网络架构 Transformer上进行多领域翻译的研究,据我们所知,这是在基于自注意力机制的多领域神经机器翻译上的首次尝试。中文-英语和英语-法语的实验表明,我们提出的框架优于基于Transformer和其他以往多领域NMT方法的基线系统。进一步的分析表明,我们的多领域模型可以学习到句子的领域信息并把它们聚类到对应的领域中。</p>

<h1 id="第五章-基于无监督单词级别的神经机器翻译的多领域适应方法">第五章 基于无监督单词级别的神经机器翻译的多领域适应方法</h1>

<h2 id="51-引言">5.1 引言</h2>
<p>​		将机器翻译(MT)更普遍地应用到工业翻译中面临着两个相互关联的问题。方面,MT技术应该能够保证高水平的灵活性,能够在广泛的使用场景(语言组合、流派、领域)中提供高质量的输出。另一方面,实现这一目标所需的基础设施应具有足够的可扩展性,以便以合理的成本实现MT的工业部署。
​		第一个问题是MT领域适应中众所周知的问题:翻译性能受到训练和测试数据之间匹配性的限制。本章讨论的场景的输入数据来自各种不同的领域,在通用平行语料库上训练的模型会受到数据多样性的影响。实际上,当训练实例的距离增加时,处理来自不同领域的句子变得越来越困难。翻译系统处理的领域越多,翻译质量下降的机率就越高。为了解决这个问题,MT系统应该足够灵活,以适应不同数据之间的各种语言差异(例如词汇、结构)。
​		第二个问题更切合实际:在模型缺乏灵活性的情况下,多领域的翻译场景需要多个特定领域的系统架构,每个系统都在给定领域经过微调来最优化性能。然而,该解决方案有两个明显的缺点:1)特定领域的模型只能适用于已知领域信息的句子输入,以便每个句子由相应领域的模型处理,以及2)每次涉及一个新的领域时,必须使用特定领域的数据再训练一个专用模型。然而,在现实世界的应用场景中,翻译请求很少带有领域信息,领域的概念本身就是模糊的,特定领域的数据也很难获得。同时,架构的成本和可扩展性是最令人关注的。考虑到维护成本和架构可扩展性,一个更好的解决方案是依靠单一模型,能够无监督地适应不同领域的输入数据。
​		我们研究了神经机器翻译(NMT)在真实的应用场景中通常需要满足的三个条件。首先,我们使用来自许多不同领域并且没有预定义顺序的句子输入流来操作。其次,句子没有领域信息。第三,输入流应该由单个通用NMT模型处理。为了解决当前NMT技术在这种无监督多领域设置中的弱点,我们基于领域感知的自注意力机制,探索了不同的领域表示学习方法,无监督地学习单词级别的领域表示,并将领域表示集成到编码器和解码器中。我们提出了的领域表示学习方法包括:1)基于领域注意力网络的单词级别的无监督学习和2)加入辅助的损失的引导学习。这些学习方法允许我们的多领域NMT在训练集和测试集句子的领域信息未知的情景下以无监督的设置工作。如果我们在训练数据上具有单词级别的领域信号,则可以使用引导学习方法,单词级别的领域信号可以由外部领域检测器提供。无监督的领域学习方法不需要关于训练和测试数据的领域结构的任何先验知识,它可以在训练NMT模型的同时学习单词级的领域结构。中英的实验表明,我们的多领域模型优于基于 Transformer的其他多领域NMT方法的性能。进一步的分析表明,即使没有关于领域结构的先验知识,我们的模型也能够学习到领域信息,并将句子按领域聚集起来。
​		本章的主要贡献在于,我们提出了不同的方法与领域感知的自注意力机制模型起来学习单词级别的领域表示,它允许我们的NMT系统在没有语料的领域信息的情景下无监督地工作。同时,实验和分析表明,我们的模型能够在基线系统上实现显著的改进,并且确实学习到了训练数据的领域结构。</p>
<h2 id="52-相关工作">5.2 相关工作</h2>
<p>​		为了解决NMT的领域适应问题,最近,研究人员进行了许多建设性和深入的研究[18-21]。然而,大多数研究主要侧重于在NMT中利用句了的领域信息,这些方法不适用于句了领域信息很难得到的真实应用场景,而且忽略了更细粒度的词语级別的领域信息。
​		对于无监督的多领域NMT的研究主要有 Farajian等人[22]的工作。他们提出了一种基于实例的适应方法,通过利用训练实例和每个测试句之间的相似性,动态设置学习算法的超参数并即时史新通用模型。
​		单词级別的多领域NMT的相关研究王要足Zeng等人的:作。句了中的单词在不同程度上与其领域相关,因此它们将对多领域NMT建模会产生不冋的影响。基于这种直觉,Zeng等人1提是出的方法致力于区分和利用词级別的领域上下文构建多领域NMT,将NMT与单语的基于注意力的领域分类任务联合建模。zeng等人的方法是构建领域分类器和对抗的领域分类器来产生句子長示,并生成两个门控单元得到领域特冇的和领域共享的表示,然后利用日标端领域分类器得到的注意力权重来调整训练目标中目标词的权重,使领域相关词在模型训练中具有更大的影响。</p>
<h2 id="53基于无监督单词级别的神经机器翻译的多领域适应">5.3基于无监督单词级别的神经机器翻译的多领域适应</h2>
<p>​		在本节中,我们将详细介绍所提出的基于无监督单词级别的神经机器翻译的多领域适应模型。我们在基于领域感知的自注意力机制的模型基础上,改变了领域表示的获取方式,提出了多种单词级别的领域表示的学习方法以适应无监督的设定。
​		基于无监督单词级别的神经机器翻译的多领域适应模型架构如图5-1所示。与基于领域感知的自注意力机制的多领域NMT架构不同的是,加入了词级别的领域表示学习的模块,而不是使用句子的领域ID学习句子级别的有监督的领域表示。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf06v185brj30fo0emdle.jpg" alt="image-20200521171458330" />
​		图5-1基于无监督单词级别的神经机器翻译多领域适应系统架构图</p>
<h3 id="531基于领域注意力网络的单词级别的无监督学习">5.3.1基于领域注意力网络的单词级别的无监督学习</h3>
<p>​		在许多情况下,我们不知道句子或文本来自哪个领域。缺少领域信息则无法使用监督学习方法来学习领域向量表示。为此,本章提出了一种无监督的领域表示学习方法。类似于44.3节中提到的句子级别的监督学习方法,我们初始化一组领域向量m={m1,m、},其中m∈R,N是预定义的领域的数量的超参数。我们将基于自注意力机制模型中的元素x的领域表示z,表示为m中N个的领域向量的加权和,而不是将z,指定为m中的某一个领域向量。
​		为了学习领域混合模型的权重,我们构建了一个领域注意力网络(如图5-2所示),允许每个元素x与m中的所有领域表示做注意力计算。混合的领域权重通过相似性函数计算的注意力权重得到,其中查询是来自x的元素,键是领域向量。基于点积函数的相似性6计算如下。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf06vw7oxpj30om06iq3e.jpg" alt="image-20200521171550270" /></p>

<p>利用这些权重,计算领域表示z,的公式如(5-3)所示。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf06w4ha5tj30oa02adfw.jpg" alt="image-20200521171603631" />
其中,WQ∈R,WF∈R,W∈R”是参数矩阵。
		在领域向量集m上使用基于领域注意力网终的领域表示具有两个好处。首先,除了超参数N之外的所有参数都可以通过端到端的Transformer模型进行调整。因此,叮以以无监檡的方式学习领域感知的自注意力机制中的领域向量m和领域表示z。其次,通过领域注意力网终,我们可以将单词聚类到特定的领域中,并用最相关的单词叮视化领域(请参阅第5.4节中的分析)。如果没有领域注意力网络,我们仍然叮以通过类似于学习单词向量的方式来学习领域表示z。但是,在这种情况卜,领域的数量与词汎表中的单词数量相同。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf06wsyss8j30k20b4gqw.jpg" alt="image-20200521171642831" /></p>

<p>​		图5-2在无监督学习方法中使用的领域注意力网络的体系结构。V,k分别表示值和键向量</p>
<h3 id="532加入辅助的损失的引导学习">5.3.2加入辅助的损失的引导学习</h3>
<p>​		在第5.3.1节的无监督方法中,领域表示与NMT模型联合学习。但是,我们也可以使用现成的独立于NMT模型的方法来学习训练数据中每个单词的领域表示。例如,我们可以学习在训练数据上训练主题模型,并将分配给单词的主题的分布视为该单词的领域表示。这为训练数据的领域相关的分布提供了不同的视角,可能有助于我们的模型。为了结合来自外部的领域表示学习模型的信息,我们引入了基于无监督方法的引导的领域注意力学习方法。基本思想是将从领域注意力网络计算的领域表示调
整为接近由外部模型学习的领域表示。
​		因此,我们引入辅助损失△来衡量领域注意力网络学习的领域表示z与外部领域模型学习的领域表示2之间的不一致。训练的最终目标是最小化下述损失。
<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf06xj8ig3j30og024mxa.jpg" alt="image-20200521171724778" />
​		其中λ&gt;0是一个超参数,平衡翻译似然函数和领域表示间的差异对训练的影响。
​		由于辅助的损失仅在训练阶段使用,因此在测试阶段不需要外部领域信号的引导。在本研究中,我们使用Chen等人提出的分布向量空间模型作为外部模型,并定义Δ为两个领域表示的交叉熵。Chen等人使用分布向量空间模型来学习跨子语料库的每个短语对(s,t)的分布。与它们类似,我们将单词的领域向量的每个维度定义为由tf-idf测量的单词对特定领域(训练数据的子语料库)的重要程度。</p>
<h2 id="54实验设置与分析">5.4实验设置与分析</h2>
<p>​		我们在中英任务上验证了本章提出的基于无监督单词级别的神经机器翻译的多领域适应模型的有效性。</p>
<h3 id="541实验设置">5.4.1实验设置</h3>
<p>​		本章实验将超参数λ设置为1,并且在我们的实验中将领域N的数量设置为4。其它的设置与45.1节的中英实验致。</p>
<h3 id="542实验结采">5.4.2实验结采</h3>
<p>​		表5-1显示了本章实验在屮英任务上的结果。我们将基于领域注意力网络的单词级别的无监督学习( Word-Level Unsupervised learning)和加入辅助的损失的引导学习( uided Learning)模型与基线系统、 Domainπεg系统进行了比较。基线系统是基于Transformer架构,由所有领域的混合语料训练得到。 DomainTag系统同样基于Transformer架构,按照Tars和 Fishell3的做法,将相应的领域ID附加到每个训练句子
​		在中英任务中,本章提出的基」领域注意力网终的单词级別的无监督学习和加入辅助的损失的引导学习在基线系统上实现了平均0.59和1.26BLEU点的提升。结果表明,提出的基于无监怪单词级別的神经机器翻译多领域适应方法能够学习到词的领域表示,并用单个NMT模型实现跨领域的翻译性能的提升。各种方法的提升幅度是合理的。加入辅助的损失的引导学习比无监督学习获得了史高的性能,表明外部有用的领域信息可以改进无监督的领域表示的学习。遗憾的是,将领域标记附加到训练数据的 DomainTag方法在 Transfomer上的效果不如基线系统。</p>

<p>表5-1 中英任务上不同模型的实验结果</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf06z2hf9fj30pq09adhz.jpg" alt="image-20200521171853554" /></p>

<h3 id="543词语领域分布的分析与可视化">5.4.3词语领域分布的分析与可视化</h3>
<p>​		我们进一步地通过翻译和数据来分析领域感知自我关注模型产生的影响。特别地,我们感兴趣的是1)我们可以为领域学习什么,以及2)联合学习领域表示的NMT模型会发生什么
​		本小节分析了中英翻译任务中加入辅助的损失的引导学习模型学到的领域-词对齐。对于每个源端的词,领域自注意力网络计算该词与学习得到的N个领域向量中每一个的相关权重。此权重测量单词与相应领域的相关程度。我们计算了每个单词对应的领域注意力权重在六层自注意力机制层中的平均值。然后,我们将每个单词对应于各领域的平均权重标准化。通过这种方式,我们可以获得每个领域的词分布。
​		在表5-2中,我们展示了根据领域词分布得到的与该领域最接近的前10个单词。从表中可以清楚地看到,我们的领域关注力网络能够将单词集中到有意义的领域中。我们还发现功能词通常由所有领域共享。</p>

<p>​		除了这些特定领域的单词之外,我们还可以在图53所示的例句中看到单词和领域之间的注意力。我们可以看到特定领域的单词,如“肾上腺素”(isoprenaline),“氨甲酰胆碱”( carbachol)对 Thesis论文领域具有非常高的注意力,而对领域不敏感的功能词如“与”(and),“的”(of)则在所有领域上均匀分布。
表5-2通过加入辅助的损失的引导学习模型学习到的每个领域的前10个单词</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf070eqwukj30pa0uatq5.jpg" alt="image-20200521172010953" />		图5-3在源端句子实例中单词和领域之间的关注力可视化。</p>
<h3 id="544句子表示的可视化">5.4.4句子表示的可视化</h3>
<p>​		本小节进一步对编码器学习的句子表小进行了分析,使用t-SNEN将中英文测试集中源端句子的表示投影到2D空间中。基线系统和加入辅助的损失的引导学习模型所学习的句子表示的可视图如图54所示。从图中可以看出,基线系统难以区分不同领域的句子。但是,我们的模型能够将各领域集群中的句子聚集起来并且不同领域间的句子间具有明确边界。我们相信,我们的模型所学习的领域信息可以在词义消歧上和翻译中使解码器受益。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf071wk7u2j30ga0nmwn0.jpg" alt="image-20200521172136865" /></p>

<p>图5-4基线系统(a)和基于无监督单词级别的神经机器翻译多领域适应系统(b)学习的源句表示的可视化。紫色: Thesis,深蓝色:News,绿色: IWSLT,黄色:Laws</p>

<h2 id="55本章小结">5.5本章小结</h2>

<p>​		在本章中,我们提出了一个无监督单词级别的神经机器翻译多领域适应模型,使用单个模型处理多个领域的输入,能够无监督地适应不同领域的输入数据。我们提出了不同的方法与领域感知的自注意力机制模型一起来学习单词级別的领域表示,包括基于领域注意力网络的单词级别的无监督学习、加入辅助的损失的引导学习。它允许我们的NMT系统在没有语料的领域信息的情景下无监督地工作。实验验证了提出的模型的有效性。深入的分析表明,即使没有关于领域结构的先验知识,我们的无监督单词级別的神经杋器翻译多领域适应模型可以将单词和句子聚类到领域中。</p>
<h1 id="第六章-总结与展望">第六章 总结与展望</h1>
<p>​		本章主要对本文开展的工作进行总结,并对后续的工作进行展望。</p>
<h2 id="61-工作总结">6.1 工作总结</h2>
<p>​		本文主要开展面向神经机器翻译的领域适应方法研究,旨在提升神经机器翻译系统在数据多样性下的领域适应性。在神经机器翻译模型中,一方面,数据驱动的机器翻译技术的性能在很大程度上取决于训练和测试数据之间的领域匹配程度;另一方面,无法得到充足的领域内数据始终是许多语言对翻译面临的问题。不同领域的数据差异很大,跨领域的翻译质量较差,针对以上问题,本文开展了一系列面向神经机器翻译的领域适应方法研究,通过使神经机器翻译系统适应目标领域来提升特定领域的翻译性能。具体研究内容如下
​		(1)展开基于句子权重的神经机器翻译的领域适应方法研究。提出了一种基于句子权重的领域适应方法,适用于标准的领域适应和低资源领域伪语料翻译两个不同的场景。领域外的语料中,有一部分接近领域内的语料能帮助领域内的翻译,但与领域内语料相差较远的语料可能会降低领域内的翻译效果。针对这一问题,我们提出了种领域相似度的度量方法,将相似程度转化为句子的权重融入神经机器翻译的训练,提髙神经机器翻译的领域适应性。实验结果和分析表明,提出的方法在以上两个情景下都获得了验证。
​		(2)展开基于自注意力机制的神经机器翻译的多领域适应方法研究。我们考虑个领域相关的开放性问题:如何构建一个领域感知的翻译系统,能较好地进行多个领域的翻译。我们提出了一种基于领域感知的自注意力机制,采用联合学习领域向量的方式训练多领域的神经机器翻译系统。实验和分析验证了我们的模型能够显著提升各领域的翻译效果并且可以学习到训练数据的领域信息。
​		(3)展开基于无监督单词级别的神经机器翻译的多领域适应方法研究。在现实的应用场景中,特定领域的语料通常难以获得,翻译请求很少带有领域信息,我们希望多领域的翻译系统能在句子的领域信息未知的情景下无监督地工作。我们提岀了无监督单词级别多领域神经机器翻译模型,通过基于领域注意力网络的无监督学习、加入辅助的损失的引导学习等方法与领域感知的自注意力机制模型一起学习单词级别的领域表示。实验和分析表明,我们的多领域模型能够提升各领域的翻译质量,并且可以学习到句子的领域信息并把它们聚类到对应的领域中。</p>
<h2 id="62-研究展望">6.2 研究展望</h2>
<p>​		在本文中,我们提出了三个有效的模型来提升神经机器翻译系统的领域适应性。实验结果证明这三种模型都能够提高神经机器翻译系统在目标领域或多领域的翻译质量,更好地适应特定领域的翻译句子结构和风格。但研究工作中仍然存在一些需要进一步深入探讨的地方,具体表现为:
​		(1)在基于句子权重的神经机器翻译的领域适应方法研究中,我们提出了基于JS散度等相似度函数的领域相似度的评价标准。在后续工作中,我们将尝试其他的领域相似性度量方式,并探索其他将权重融入神经翻译模型的有效方式。
​		(2)在基于自注意力机制的神经机器翻译的多领域适应方法研究中,我们提出了在编码器和解码器的自注意力机制中融入领域表示的方法。在后续:作中,我们将进·步探索领域信息与神经翻译模型的结合方式,并硏究影响不冋领域的翻译性能提升幅度的相关因素。
​		(3)在基于无监督单词级別的神经机器翻译的多领域适应方法研究中,提出的加入辅助的损失的引导学习方法采用了分布向量空间模型来获取外部领域信号。在后续作中,我们将用题模型的方法获取单词的主题分布,进步探索外部领域信号对多领域神经机器翻译的影响。</p>

<h1 id="参考文献">参考文献</h1>

<ul>
  <li>[1] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk and Yoshua Bengio Learning phrase representations using Rnn encoder-decoder for statistical machine translation [A]. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing [C]. Pennsylvania: Association for Computational Linguistics, 2014</li>
  <li>[2] Ilya Sutskever, Oriol Vinyals and Quoc V. Le. Sequence to sequence learning with neural networkS [A]. Advances in neural information processing systems [C] Massachusetts: Mit Press. 2014.</li>
  <li>[3] Dzmitry Bahdanau, Kyunghyun Cho and Yoshua Bengio Neural Machine Translation by Jointly Learning to Align and Translate [A]. 3rd International Conference on Learning Representations [C]. San Diego, CA, USA, 2015</li>
  <li>[4] Minh-Thang Luong, Hieu Pham and Christopher D Manning. <strong>Effective Approaches to Attention-based Neural Machine Translation</strong> [A]. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing [C]. Pennsylvania Association for Computational Linguistics, 2015</li>
  <li>[5] Yonghui Wu, Schuster Mike, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, ukasz Kaiser, Stephan Gouws Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals G s Corrado, Macduff Hughes and Jeffrey Dean. Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation [J]. ar Xiv preprint ar Xiv: 1609.08144, 2016</li>
  <li>[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser and Illia Polosukhin. Attention is all you need [A]. In Advances in Neural Information Processing Systems [C]. Massachusetts: MIT Press, 2017.</li>
  <li>[7] Jonas Gehring, Michael Auli, David Grangier, Denis Arats and Yann Dauphin Convolutional sequence to sequence learning [A. In Proceedings of Intermational Conference on Machine Learning(ICML)[C]. Sydney: ACM, 2017</li>
  <li>[8] Minh-Thang Luong and Christopher D. Manning. Stanford neural machine translation systems for spoken language domains [A]. In Proceedings of the International Workshop on Spoken Language Translation [C]. Da Nang, 2015</li>
  <li>[9] Barret Zoph, Deniz Yuret, Jonathan May and Kevin Knight. Transfer learning for low-resource neural machine translation [A]. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [C]. Pennsylvania: Association for Computational linguistics, 2016</li>
  <li>[10]Antonio Valerio Miceli Barone, Barry Haddow, Ulrich Germann and Rico Sennrich Regularization techniques for fine-tuning in neural machine translation [A]. In Proceedings of the 2017 Conference on Empirical Methods in Natural language Processing [C]. Pennsylvania: Association for Computational Linguistics, 2017</li>
  <li>[11]Robert C Moore and william Lewis. Intelligent selection of language model training data [A]. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics( Volume 2: Short Papers)[C]. Pennsylvania: Association for Computational Linguistics, 2010</li>
  <li>[12]Amittai Axelrod, Xiaodong He and Jianfeng Gao. Domain adaptation via pseudo in-domain data selection [A]. In Proceedings of the conference on empirical methods in natural language processing [C]. Pennsylvania: Association for Cor Linguistics. 2011</li>
  <li>[13 Boxing Chen, Colin Cherry, George Foster and Samuel Larkin. Cost weighting for neural machine translation domain adaptation [A]. In Proceedings of the First Workshop on Neural Machine Translation [C]. Pennsylvania: Association for Computational Linguistics, 2017</li>
  <li>[14]Rui Wang, Masao Utiyama, Lemao Liu, Kehai Chen and Eiichiro Sumita. <strong>Instance weighting for neural machine translation domain adaptation</strong> [A]. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing [C] Pennsylvania: Association for Computational Linguistics, 2017</li>
  <li>[15]Chenhui Chu, Raj Dabre and Sadao Kurohashi. An empirical comparison of domain adaptation methods for neural machine translation [A]. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics [C]. Pennsylvania: Association for Computational Linguistics, 2017</li>
  <li>[16]Markus Freitag and Yaser Al-Onaizan. Fast domain adaptation for neural machine translation [J]. arXiv preprint ar Xiv: 1612.06897, 2016</li>
  <li>[17]Rui Wang, Andrew Finch, Masao Utiyama and Eiichiro Sumita Sentence embedding for neural machine translation domain adaptation [A]. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)[C]. Pennsylvania: Association for Computational Linguistics, 2017.</li>
  <li>[18]Catherine Kobus, Josep Crego and Jean Senellart. Domain control for neural machine translation [A]. Proceedings of the International Conference Recent Advances in Natural Language Processing [C]. Varna, Bulgaria, 2017.</li>
  <li>[19]Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Yonatan Belinkov and Stephan Vogel Neural machine translation training in a multi-domain scenario [A]. In Proceedings of the Twelfth International Workshop on Spoken Language Translation(IWSLT)[C] 2017</li>
  <li>[20]Jiali Zeng, Jinsong Su, Huating Wen, Yang Liu, Jun Xie, Yongjing Yin and Jianqiang Zhao. Multi-domain neural machine translation with wordlevel domain context discrimination [A]. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing [C]. Pennsylvania: Association for Computational Linguistics, 2018.</li>
  <li>[21]Denny Britz, Quoc Le and Reid Pryzant Effective domain mixing for neural machine translation [A]. In Proceedings of the Second Conference on Machine Translation [C] Pennsylvania: Association for Computational Linguistics, 2017.</li>
  <li>[22]M. Amin Farajian, Marco Turchi, Matteo Negri and Marcello Federico Multi-domain neural machine translation through unsupervised adaptation [A]. In Proceedings of the Second Conference on Machine Translation [C]. Pennsylvania Association for Computational Linguistics, 2017</li>
  <li>[23]Junyoung Chung, Caglar Gulcehre, Kyung Hyun Cho and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling [A]. Presented in NIPS 2014 Deep Learning and Representation Learning Workshop [C]. Massachusetts MIT Press. 2014</li>
  <li>[24]Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu BLEU: A Method for Automatic Evaluation of Machine Translation [A]. Isabelle P. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics [C]. Pennsylvania Association for Computational linguistics, 2002</li>
  <li>[25]刘群.统计机器翻译综述[J.中文信息学报,2003,17(4):1-12</li>
  <li>[26]Jimmy Lei Ba, Jamie Ryan Kiros and Geoffrey E Hinton. Layer normalization [J] arXiv preprint ar Xiv: 1607. 06450, 2016</li>
  <li>[27]Jianhua Lin. Divergence measures based on the shannon entropy [J]. IEEE Transactions on Information theory, 1991, 37: 145-151</li>
  <li>[28]Lillian Lee. On the effectiveness of the skew divergence for statistical language analysis [A]. Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics [C]. Society for Artificial Intelligence and Statistics, 2001</li>
  <li>[29]Barbara Plank and Gertjan Van Noord. Effective measures of domain similarity for parsing [A]. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies- Volume 1 [C] Pennsylvania: Association for Computational linguistics, 201</li>
  <li>[30]Sebastian Ruder, Parsa Ghaffari and John G Breslin. Knowledge adaptation: Teaching to adapt []. ar Xiv preprint ar Xiv: 1 702.02052, 2017.</li>
  <li>[31]Andreas Stolcke. SriIm-an extensible language modeling toolkit [A]. In Proceedings of international conference on spoken language processing [C]. ISCA 2002</li>
  <li>[32]Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho and Yoshua Bengio. How to construct deep recurrent neural networks [A]. 2nd Intermational Conference on Learning Representations [C]. Banff, AB, Canada, 2014</li>
  <li>[33]Benjamin Marie, Rui Wang, Atsushi Fujita, Masao Utiyama and Eiichiro Sumita Nict’s neural and statistical machine translation systems for the wmt18 news translation task [A]. In Proceedings of the Third Conference on Machine Translation Shared Task Papers [C]. Pennsylvania: Association for Computational Linguistics 2018.</li>
  <li>[34]Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, Will Lewis, Mu Li Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang and Ming Zhou. Achieving human parity on automatic chinese to english news translation [U]. ar Xiv preprint ar Xiv: 1803.05567, 2018</li>
  <li>[35]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization [A] 3rd International Conference on Learning Representations [C]. San Diego, CA, USA, 2015</li>
  <li>[36]Sander Tars and Mark Fishel. Multi-domain neural machine translation [A]. In Proceedings of the 21st Annual Conference of the European Association for Machine Translation [C]. European Association for Machine Translation, 2018</li>
  <li>[37]Shiqi Zhang and Deyi Xiong. Sentence weighting for neural machine translation domain adaptation [A]. In Proceedings of the 27th International Conference on Computational Linguistics [C]. Pennsylvania: Association for Computational LinguisticS, 2018</li>
  <li>[38]Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne []. Journal of machine learning research, 2008: 2579-2605</li>
  <li>[39]Boxing Chen, Roland Kuhn and George Foster. A comparison of mixture and vector space techniques for translation model adaptation [A]. In Proceedings of the 1lth conference of the association for machine translation in the Americas, volume 1 [C] Association for Machine Translation in the Americas. 2014</li>
  <li>[40]Chinea-Rios Mara, Peris Alvaro and Casacuberta Francisco Adapting neural machine translation with parallel synthetic data [A]. Proceedings of the Second Conference on Machine Translation [C]. Pennsylvania: Association for Computational Linguistics 2017.</li>
  <li>[41]Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar. Transductive learning for statistical machine translation [A]. In Proceedings of the Annual Meeting of the Association for Computational Linguistics [C]. Pennsylvania: Association for Computational Linguistics, 2007</li>
  <li>[42]Hua Wu, Haifeng Wang and Chengqing Zong. Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora [A]. In Proceedings of the Annual Meeting of the Association for Computational Linguistics [C]. Pennsylvania: Association for Computational Linguistics, 2008: 993-1000</li>
  <li>[43]Rico Sennrich, Barry Haddow and Alexandra Birch. Improving neural machine translation models with monolingual data [A]. In Proceedings of the Annual Meeting
f the association for Computational Linguistics [C]. Pennsylvania: Association for Computational Linguistics, 2016</li>
  <li>[44]Alberto Poncelas, Dimitar Shterionov, Andy Way, Gideon Maillette de buy Wenniger and Peyman Passban. Investigating backtranslation in neural machine translation [JI ar Xiv preprint ar Xiv: 1804.06189, 2018.</li>
  <li>[45]Xing Niu, Michael Denkowski and Marine Carpuat. Bi-directional neural machine translation with synthetic parallel data [A]. Proceedings of the 2nd Workshop on Neural Machine Translation and Generation [C]. Association for Computational
Linguistics, 2018</li>
  <li>[46]Hany Hassan, Mostafa Elaraby and Ahmed Y. Tawfik. Synthetic Spoken Data for Neural Machine Translation [A]. The International Workshop on Spoken Language Translation [C]. 2017.</li>
  <li>[47Jiatao Gu, Hany Hassan, Jacob Devlin and Victor O. K. Li Universal neural machine translation for extremely low resource languages [A]. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies [C]. New Orleans, Louisiana, USA, 2018</li>
  <li>[48]Jiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li and Kyunghyun Cho Meta-learning for low-resource neural machine translation [A]. Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing [C]. Brussels, Belgium, 2018</li>
  <li>[49]Surafel Melaku Lakew, Quintino F. Lotito, Matteo Negri, Marco Turchi and Marcello Federico. Improving Zero-Shot Translation of Low-Resource Languages [J]. ar Xiv preprint arXiv: 1811.01389, 2018</li>
  <li>[50]Sergey Edunov, Myle Ott, Michael Auli and David Grangier. Understanding Back-Translation at Scale [A]. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing [C]. Brussels, Belgium, 2018</li>
  <li>[51]Quoc V Le and Tomas Nikolov. Distributed representations of sentences and documents [A]. In Proceedings of the 31th International Conference on Machine Learning [C]. Beijing, China, 2014</li>
  <li>[52]Solomon Kullback. Information Theory and Statistics [M]. Mineola: Dover Publications Inc. 1968</li>
  <li>[53]Ivan Dokmanic, Reza Parhizkar, Juri Ranieri and Martin Vetterli. Euclidean distance matrices: essential theory, algorithms, and applications [J]. IEEE Signal Processing agazine.2015,32:12-30.</li>
  <li>[54]Yong Cheng, Qian Yang, Yang Liu, Maosong Sun and Wei Xu Joint Training for Pivot-based Neural Machine Translation [A]. In Proceedings of the 26th International Joint Conference on Artificial Intelligence [C]. San Francisco, CA: Morgan Kaufmann,</li>
  <li>[55Jaehong Park, Jongyoon Song and Sungroh Yoon. Building a neural machine translation system using only synthetic parallel data [J]. ar Xiv preprint arXiv:1704.00253,2017</li>
  <li>[56]Cong Duy Vu Hoang, Philipp Koehn, Gholamreza Haffari and Trevor Cohn. Iterative Back-Translation for Neural Machine Translation [A]. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation [C]. 2018</li>
  <li>[57]Stuart J. Russell and Peter Norvig. Artificial intelligence: a modern approach [M] Malaysia: Pearson Education Limited, 1995</li>
  <li>[58]Ramon P. Neco and Mikel L Forcada. Asynchronous translations with recurrent neural nets [A]. Proceedings of International Conference on Neural Networks(ICNN97)[C]
1997</li>
  <li>[59]Yoshua Bengio, Rejean Ducharme, Pascal Vincent and Christian Jauvin. A neural probabilistic language model []. Journal of Machine Learning Research, Volume 3 1137-1155,2003</li>
  <li>[60]Robert C Moore and William Lewis. Intelligent selection of language model training data [A]. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics(Volume 2: Short Papers)[C]. Pennsylvania: Association for Computational linguistics, 2010</li>
  <li>[61]Kevin Duh, Graham Neubig, Katsuhito Sudoh and Hajime Tsukada. Adaptation data selection using neural language models: Experiments in machine translation [A]. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics Volume 2: Short Papers)[C]. Pennsylvania: Association for Computational Linguistics, 2013</li>
  <li>[ 62]Cuong Hoang and Khalil Simaan. Latent domain translation models in mix-of-domains haystack [A]. In Proceedings of the 25th International Conference on
Computational Linguistics [C]. Pennsylvania: Association for Computational Linguistics, 2014.</li>
  <li>[63]Nadir Durrani, Hassan Sajjad, Shafiq joty, Ahmed Abdelali and Stephan Vogel. Using joint models for domain adaptation in statistical machine translation [A]. In Proceedings of MT Summit XV [C]. 2015</li>
  <li>[64]Masao Utiyama and Hitoshi Isahara. Reliable measures for aligning japanese-english news articles and sentences []. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics [C]. Pennsylvania: Association for
Computational linguistics, 2003</li>
  <li>[65]Patrik Lambert, Holger Schwenk, Christophe Servan and Sadaf Abdul-Rauf. Investigations on translation model adaptation using monolingual data [A]. In Proceedings of the Sixth Workshop on Statistical Machine Translation [C]
Pennsylvania: Association for Computational Linguistics, 2011</li>
  <li>[66]Benjamin Marie and Atsushi Fujita. Efficient extraction of pseudo-parallel sentences from raw monolingual data using word embeddings [A]. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics(Volume 2: Short Papers)[C]. Pennsylvania: Association for Computational Linguistics, 2017.</li>
  <li>[67]Nadir Durrani, Hassan Sajjad, Shafiq Joty, Ahmed Abdelali and Stephan Vogel. USing joint models for domain adaptation in statistical machine translation [A]. In Proceedings of MT Summit XV [C]. 2015</li>
  <li>[68]Kenji Imamura and Eiichiro Sumita Multi-domain adaptation for statistical machine translation based on feature augmentation [A]. In Proceedings of the 12th Conference
of the Association for Machine Translation in the Americas [C]. 2016</li>
  <li>[69]Xinpeng Zhou, Hailong Cao and Tiejun Zhao. Domain adaptation for SMT using sentence weight [A]. In Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data [C]. Springer, 2015</li>
  <li>[70]Anthony Rousseau, Fethi Bougares, Paul Deleglise, Holger Schwenk and Yannick Esteve. Liums systems for the iwslt 2011 speech translation tasks [A]. In International Workshop on Spoken Language Translation [C]. San Francisco, USA, 2011</li>
  <li>[71]Sebastien Jean, Kyunghyun Cho, Roland Memisevic and Yoshua Bengio. On using very large target vocabulary for neural machine translation [A]. In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing(Volume 1: Long Papers)[C]. Pennsylvania: Association for Computational Linguistics. 2015</li>
</ul>

<h1 id="攻读硕士学位期间取得的科研成果">攻读硕士学位期间取得的科研成果</h1>

<p>1.第一作者论文:</p>

<ul>
  <li>[1] Shiqi Zhang, Deyi Xiong Sentence Weighting for Neural Machine Translation Domain Adaptation [A]. Proceedings of the 27th International Conference on Computational Linguistics [C]. Pennsylvania: Association for Computational Linguistics, 2018 381-3190.CCF-B类会议论文</li>
  <li>[2] Shiqi Zhang, Deyi Xiong. Domain-Aware Self-Attention for Multi-domain Neural Machine Translation [A]. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing[C]CCFB类会议论文,在投中</li>
</ul>

<p>2.专利成果</p>

<ul>
  <li>[1]熊德意,张诗奇.一种相似网站査找方法、系统、设备及介质.申请号:
201711033737.3</li>
  <li>[2]熊德意,张诗奇.将句子权重融入神经机器翻译的领域适应方法.申请号:2018103258189</li>
  <li>[3]熊德意,张诗奇基于自注意力机制的多领域神经机器翻译方法.申请号:201910344013.3</li>
</ul>

<p>3.软件著作权</p>

<ul>
  <li>[1]熊德意,张诗奇相似网站的自动发现系统,登记号:2018SR070175</li>
</ul>

<p>4.参与科研项目:</p>

<ul>
  <li>[1]国家自然科学基金优秀青年基金项目,No.61622209,统计机器翻译</li>
  <li>[2]国家自然科学基金青年基金项目,No.61403269.基于词汇语义的统计机器翻译研究</li>
  <li>[3]江苏省自然科学基金青年基金项目,No.BK2014035.基于分布式语义的统计机器翻译关键技术研究.</li>
</ul>

<h1 id="致谢">致谢</h1>

<p>​		养天地之正气,法古今之完人。三年的时光转瞬而逝,我的研究生生活也接近尾声。这三年里,我丰富了专业知识,坚定了做事的态度,拥有了与学术界研究者交流的机会,对自己有了更深的认识和人生规划。在这里,我想对帮助我的老师、同学和家人表达最诚挚的感谢。
​		感谢各位老师的带领和计算机科学与技术学院浓厚的学术氛围,在这段时光里让我得以窥知科研一二。在科研方法方面,追踪研究领域的最新动态,归纳总结,找到可能的突破点,最好能融会贯通,将其他研究领域的思想用到自己的研究中来。研究越深入,越能看到自己思路的浅薄,越能不停思考不停尝试。在论文写作方面,做到逻辑清晰,思考实验方法的物理意义,多角度深入分析。在科研氛围上,与同学主动交流各领域的研究动态和思想。科硏的过程对生活工作习惯的养成也有所裨益,明确要解决的问题,学会时间管理,并且打磨耐心,循序渐进。
​		首先,感谢我的导师熊德意教授。感谢熊老师的引领,为我们提供良妤的科研环境,并以渊博的学识、严谨求真的态度、廾井有条的做事风格影响我们,言传身教带给我们对待科学研究的正确认知,对我们的研究方向和进程即时引导指正。熊老师向我们传授要善于发现研究点中未被开垦的蓝海,鼓励“天下武功唯快不破”,也强调在深入钻研一个方向时,要“徐行则不困,挫足于实地则不危”。感谢熊老师不断的激励和悉心的指导,让我不断成长和进步。
​		同时,感谢贡正仙老师。贞老师待人温和谦逊,在科研和教学上都认真严谨,对学生寄予殷股期望。贞老师在我入学之初给了我很多学习上的帮助,事无巨细地帮我解决研究上遇到的问题,让我对科研这项作有了最初的认知。
​		感谢张民,周国栋,朱巧明,陈文亮,段湘煜,李正华,姚建民,洪宁宇,孔芳,王红玲,钱龙华,李寿山,李军辉和李培峰等臼然语言处理实验室的所冇ξ师。他们对学术研究的热情,激励我时刻向他们看齐。
​		感谢身边的同学。感谢李方圆师姐,唐海庆师姐,奚浏师姐,邝少辉师兄给予我的作生活上的帮助;感谢同窗好友徐雪茕、江琪、边丽娜、朱芬红、何云琪、J颖、蔡了龙、韩冬、王坤,我们玍帮互助、共冋进步;感谢冋门师弟师妺谭新、曹骞、张培、秦文杰、黄佳跃、王涛、全俊、邱嘉作、张诗安、敬毅民,祝学业顺利,前程似锦。
​		感谢我的父母和家人,感谢他们在我成长道路上的引导、陪伴和支持。
​		最后,向参与评审和答辩的各位专家老师致以最诚挚的感谢。衷心感谢老师们的批评指正。</p>
:ET