I"<h1 id="1---sequence-to-sequence-learning-with-neural-networks">1 - Sequence to Sequence Learning with Neural Networks</h1>
<h3 id="1-spacy">1. spacy</h3>
<p><a href="https://spacy.io/">spaCy</a> 是一个 Python 和 CPython 的 NLP 自然语言文本处理库。
所有需要了解和学习的内容都在网站上可以找到，写的非常清楚，不用再搬运过来了。
<a href="https://blog.csdn.net/u012436149/article/details/79321112">使用 spacy 进行自然语言处理（一）</a>，这篇文章用中文讲解了spacy的用法，回头用的到的时候就可以好好看看了。</p>

<h3 id="2-torchnnfunction">2. TORCH.NN.FUNCTION</h3>
<p><a href="https://pytorch.org/docs/stable/nn.functional.html">torch.nn.function</a></p>

<h3 id="3-randomseed的使用">3. random.seed()的使用</h3>
<p><a href="https://blog.csdn.net/linzch3/article/details/58220569">random.seed()的使用</a>
seed( ) 用于指定随机数生成时所用算法开始的整数值。 
1.如果使用相同的seed( )值，则每次生成的随即数都相同； 
2.如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。 
3.设置的seed()值仅一次有效</p>

<h3 id="4-torchmanual_seed">4. torch.manual_seed()</h3>
<p><a href="https://blog.csdn.net/qq_34690929/article/details/79923602">torch.manual_seed()</a> 且每次生成的数据都是一样的。</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.manual_seed(args.seed) #为CPU设置种子用于生成随机数，以使得结果是确定的 
if args.cuda: 
    torch.cuda.manual_seed(args.seed)#为当前GPU设置随机种子；如果使用多个GPU，应该使用torch.cuda.manual_seed_all()为所有的GPU设置种子。
</code></pre></div></div>

<h3 id="5-分词的练习">5. 分词的练习</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 分词函数的练习
strings = 'You are the best!'
def tokenize(strings):
    words = strings.split(' ')
    return [word for word in words]

def tokenize2(strings):
    words = strings.split(' ')
    return [word for word in words][::-1]
tokenize(strings), tokenize2(strings)

&gt; (['You', 'are', 'the', 'best!'], ['best!', 'the', 'are', 'You'])
</code></pre></div></div>

<h3 id="6-torchtextdata以及torchtextdatasets">6. torchtext.data以及torchtext.datasets</h3>
<p><a href="https://blog.csdn.net/u012436149/article/details/79310176">torchtext</a> 这篇博客写得好哇，是torchtext的入门级文章。
<strong>torchtext.data</strong>
torchtext.data.Example : 用来表示一个样本，数据+标签
torchtext.vocab.Vocab: 词汇表相关
torchtext.data.Datasets: 数据集类，<strong>getitem</strong> 返回 Example实例
torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）
创建 Example时的 预处理
batch 时的一些处理操作。
torchtext.data.Iterator: 迭代器，用来生成 batch
<strong>torchtext.datasets</strong>: 包含了常见的数据集.</p>

<p>TorchText 的数据预处理流程为：</p>
<ol>
  <li>定义样本的处理流程，field。</li>
  <li>加载corpus，都是string</li>
  <li>创建词汇表</li>
  <li>将处理后的数据进行batch操作</li>
</ol>

<p>一个简单的小例子帮助我理解Torchtext的处理流程，还是感觉好难哇T_T，等回头用到了再好好学吧T_T.</p>

<h3 id="7-torchnnembedding">7. torch.nn.Embedding</h3>
<p><a href="https://www.cnblogs.com/lindaxin/p/7991436.html">torch.nn.Embedding</a>
在pytorch里面实现word embedding是通过一个函数来实现的:nn.Embedding
embeds = nn.Embedding(2, 5) # 这里的2表示有2个词，5表示5维度，其实也就是一个2x5的矩阵，所以如果你有1000个词，每个词希望是100维，你就可以这样建立一个word embedding，nn.Embedding(1000, 100)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
 
word_to_ix = {'hello': 0, 'world': 1}
embeds = nn.Embedding(2, 5)
hello_idx = torch.LongTensor([word_to_ix['hello']])
hello_idx = Variable(hello_idx)
hello_embed = embeds(hello_idx)
print(hello_embed)
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Variable containing:
 0.4606  0.6847 -1.9592  0.9434  0.2316
[torch.FloatTensor of size 1x5]
</code></pre></div></div>

<h3 id="8-torchnnlstm函数维度详解">8. torch.nn.LSTM()函数维度详解</h3>
<p><a href="https://blog.csdn.net/m0_37586991/article/details/88561746">torch.nn.LSTM()函数维度详解</a></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
import torch.nn as nn
lstm = nn.LSTM(10, 20, 2)
x = torch.randn(5, 3, 10)
h0 = torch.randn(2, 3, 20)
c0 = torch.randn(2, 3, 20)
output, (hn, cn)=lstm(x, (h0, c0))

&gt;&gt;
output.shape  torch.Size([5, 3, 20])
hn.shape  torch.Size([2, 3, 20])
cn.shape  torch.Size([2, 3, 20])
</code></pre></div></div>
<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/2019-08-15-LSTM_dim.jpg" alt="LSTM_dim.jpg" />
举个例子：
对句子进行LSTM操作</p>

<p>假设有100个句子（sequence）,每个句子里有7个词，batch_size=64，embedding_size=300</p>

<p>此时，各个参数为：
input_size=embedding_size=300
batch=batch_size=64
seq_len=7</p>

<p>另外设置hidden_size=100, num_layers=1</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
import torch.nn as nn
lstm = nn.LSTM(300, 100, 1)
x = torch.randn(7, 64, 300)
h0 = torch.randn(1, 64, 100)
c0 = torch.randn(1, 64, 100)
output, (hn, cn)=lstm(x, (h0, c0))

&gt;&gt;
output.shape  torch.Size([7, 64, 100])
hn.shape  torch.Size([1, 64, 100])
cn.shape  torch.Size([1, 64, 100])
</code></pre></div></div>

<h3 id="9-torchnndropout">9. torch.nn.dropout</h3>
<p><a href="https://blog.csdn.net/u014532743/article/details/78453990">torch.nn.dropout</a></p>
<ul>
  <li>注1：设置 Dropout 时，torch.nn.Dropout(0.5), 这里的 0.5 是指该层（layer）的神经元在每次迭代训练时会随机有 50% 的可能性被丢弃（失活），不参与训练，一般多神经元的 layer 设置随机失活的可能性比神经元少的高</li>
  <li>注2：由于要 Dropout，以及凸显过拟合现象，所以 layer 的神经元设置多些，这里为 300</li>
</ul>
:ET