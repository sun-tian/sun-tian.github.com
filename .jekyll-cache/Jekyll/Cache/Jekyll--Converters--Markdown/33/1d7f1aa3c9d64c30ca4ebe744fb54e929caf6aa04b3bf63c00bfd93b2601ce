I"X<h2 id="transformer模型详解">Transformer模型详解</h2>

<p>原论文地址：<a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></p>

<p>论文翻译中英文对照：<a href="https://www.yiyibooks.cn/yiyibooks/Attention_Is_All_You_Need/index.html">Attention is all you need中英文对照翻译</a></p>

<p>中文笔记参考：<a href="https://zhuanlan.zhihu.com/p/48508221">详解Transformer</a>
            <a href="https://fancyerii.github.io/2019/03/09/transformer-illustrated/">Transformer图解</a></p>

<p>英文笔记参考：<a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>

<p>PyTorch复现代码：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#">The Annotated Transformer</a></p>

<p>TensorFlow复现代码：<a href="https://github.com/tensorflow/tensor2tensor">tensorflow/tensor2tensor</a></p>

<p>学习笔记：
<img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190827-Transformer_script_1.jpg" alt="Transformer_script_1.jpg" /></p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190827-Transformer_script_2.jpg" alt="Transformer_script_2.jpg" /></p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190827-Transformer_script_3.jpg" alt="Transformer_script_3.jpg" /></p>
:ET