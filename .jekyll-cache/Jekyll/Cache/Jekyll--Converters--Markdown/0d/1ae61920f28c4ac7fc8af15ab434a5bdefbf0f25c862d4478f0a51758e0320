I"rH<h1 id="2017-藏汉神经网络机器翻译研究">2017 藏汉神经网络机器翻译研究</h1>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200720164447.jpg" alt="image-20200514150956733" /></p>

<h1 id="摘要">摘要</h1>

<p>​		神经网络机器翻译是最近几年提出的机器翻译方法，在多数语言对上逐渐超过了统计机器翻译方法， 成为当前机器翻译研究前沿热点。<strong>该文实验了基于注意力的藏汉神经网络机器翻译，采用迁移学习方法缓 解藏汉平行语料数量不足问题</strong>。实验结果显示，该文提出的迁移学习方法简单有效，相比短语统计机器翻译方法，提高了 3 个 BLEU 值。从译文分析中可以看出藏汉神经网络机器翻译的译文比较流畅，远距离调 序能力较强，同时也存在过度翻译、翻译不充分、翻译忠实度较低等神经网络机器翻译的共同不足之处。</p>

<h1 id="关键词">关键词</h1>

<p>​		藏语；神经网络机器翻译；注意力机制；循环神经网络；迁移学习</p>

<h1 id="1-引言">1 引言</h1>

<p>​		机器翻译研究如何利用计算机自动地实现不同语言之间的相互转换，是自然语言处理的 重要研究领域。机器翻译方法可以分为基于规则的机器翻译、基于实例的机器翻译、统计机 器翻译，以及当前的神经网络机器翻译等。互联网与移动互联网的兴起，为机器翻译研究提 供了巨大的应用空间。产业界如谷歌、微软、百度等公司都对机器翻译投入巨大资源进行研究，同时也对外提供了相关的翻译服务，如谷歌翻译、百度翻译等。国内外的科研机构同样 把该领域作为重点的研究方向。机器翻译成为当前自然语言处理研究的热点之一，不仅具有研究价值，同样具有实用价值。</p>

<p>​		神经网络起源于 20 世纪 40 年代，发展于 50-70 年代，由于种种限制 80-90 年代为神经网络研究的低潮期 [1] 。2006 年，Hinton 等人 [2] 解决了神经网络的训练难题，此后，随着计算能力的提高，以及训练数据量的增加，神经网络成功应用在多个领域。近年来，神经网络在 图像识别、语音识别等领域取得巨大成功，同时学者们也将该技术应用在自然语言处理任务 上，如语言模型、词语表示、序列标注等任务，并取得了令人鼓舞的成绩 [3-5] 。</p>

<p>​		在机器翻译研究上，神经网络机器翻译实现源语言到目标语言的直接翻译，极大地提高 了翻译效果，是目前机器翻译研究的前沿热点。基于神经网络的机器翻译方法源于 20 世纪 90 年代 [6] ，由于资源限制并没有成为主流方法。在深度学习兴起之后，神经网络通常用于统计机器翻译的词对齐、翻译规则抽取等 [7] 。从 2014 年开始，Sutskever、Cho、Jean 等人 [8-11] 提出单纯采用神经网络实现机器翻译的新方法，称为神经网络机器翻译。该方法实现了源语 言到目标语言的直接翻译，并在多个语言对上超过了传统的统计机器翻译方法 [12] ，逐渐成 为当前主流的机器翻译方法。</p>

<p>​		藏语机器翻译相关研究主要集中在统计机器翻译，以及藏语机器翻译相关基础研究。如藏语统计机器翻译短语抽取 [13] 、基于短语的统计翻译等 [14-15] ；基于树到串的藏语机器翻译 [16]； 规则和统计相结合的汉语到藏语机器翻译 [17] ；面向藏语机器翻译的动词处理 [18] 、短语句法 研究 [19] 、功能组块识别 [20] 、藏文数词识别与翻译 [21] 等。从整体上看，藏语机器翻译研究较为滞后，有关神经网络机器翻译在藏语上的应用及实际效果，并没有见到公开发表。</p>

<p>​		本文后续部分安排如下：第二部分详细介绍神经网络机器翻译以及迁移学习在藏汉神经 网络机器翻译上的应用，第三部分进行实验与分析，第四部分为全文的总结和下一步工作安排。</p>

<h1 id="2-神经网络机器翻译">2 神经网络机器翻译</h1>

<table>
  <tbody>
    <tr>
      <td>​		机器翻译把翻译问题看作求解概率问题，即给定源语言 s，求目标语言 t 在源语言 s 出 现下的条件概率𝑝(𝒕</td>
      <td>𝒔)，模型参数从双语平行语料中学习到。神经网络机器翻译在翻译建模 上完全采用神经网络完成源语言到目标语言的直接翻译 ，不需要经过统计机器翻译的词对 齐、翻译规则抽取、调序等步骤。本小节简单介绍神经网络机器翻译，以及迁移学习方法在 藏汉神经网络机器翻译上的应用。</td>
    </tr>
  </tbody>
</table>

<h2 id="21-基本神经网络机器翻译模型">2.1 基本神经网络机器翻译模型</h2>

<p>​		神经网络机器翻译源于序列到序列学习 [8-10] ，本文以蒙特利尔大学提出的翻译模型为例 说明 [9-10] 。编码器解码器模型（Encoder-Decoder）是神经网络机器翻译模型之一，编码器读 取源语言句子，编码为维数固定的向量，解码器读取该向量，依次生成目标语言词语序列， 如图 1 所示。神经网络机器翻译模型主要部分如下所示，x 表示输入， h 表示隐藏状态， y 表示输出。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges00fgji8j30dm0ckq40.jpg" alt="image-20200514151329855" /></p>

<p><strong>编码器</strong>：编码器读取输入 x = (x 1 , x 2 , …, x I )，将其编码为隐藏状态 h = (h 1 , h 2 ,…, h I )，编 码器通常采用循环神经网络（Recurrent Neural Network, RNN）实现，更新方式如下所示：</p>

<p><img src="/Users/suntian/Library/Application Support/typora-user-images/image-20200514151401127.png?lastModify=1595234692" alt="image-20200514151401127" /></p>

<p>c 为源语言句子表示，f 和 q 是非线性函数。</p>

<p><strong>解码器</strong>：在给定源语言表示 c 和前驱输出序列{y 1 , … , y 标语词𝑦 𝑡 ，如下所示：</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges01xqnbxj309y02k74b.jpg" alt="image-20200514151457177" /></p>

<p>y = (y 1 , y 2 , … ,𝑦 𝑇 )，解码器通常同样采用循环神经网络，形式如下所示：</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges02ceoxij30b0012749.jpg" alt="image-20200514151519982" /></p>

<p>𝑔是非线性函数用来计算 y t 的概率，s t 是解码器的隐藏状态。</p>

<p><strong>模型训练</strong>：编码器和解码器可以进行联合训练，形式如下：</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges02nwhu3j309o02qq30.jpg" alt="image-20200514151538598" /></p>

<p>𝜃是模型的参数，通常采用梯度下降法进行计算，(x n , y n )表示双语训练语料。</p>

<h2 id="22-基于注意力的神经网络机器翻译">2.2 基于注意力的神经网络机器翻译</h2>

<p>​		普通神经网络机器翻译模型将源语言句子表示成一个固定向量，该方法存在不足之处， 比如大小固定的向量并不能充分表达出源语言句子语义信息。基于注意力机制的神经网络机 器翻译将源语言句子编码为向量序列，在生成目标语言词语时，通过注意力机制动态寻找与 生成该词相关的源语言词语信息 [14] ，因而大大增强了神经网络机器翻译的表达能力，在实 验中显著提高了翻译效果，如图 2 所示。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges03m18pqj30ay0aedgh.jpg" alt="image-20200514151633829" /></p>

<p>​		当采用注意力机制时，公式 3 重新定义为：</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges0406dbhj30b6018mx5.jpg" alt="image-20200514151655928" /></p>

<p>​		𝑠 𝑡 是 t 时刻循环神经网络的隐藏状态，由如下公式得出：</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges048kg43j306e0100sn.jpg" alt="image-20200514151709450" /></p>

<p>​		𝑔、𝑓是非线性函数，上下文向量(Context Vector) c t 依赖于源语言编码序列(h 1 , h 2 ,…, h I )， h i 包含第 i 个输入词上下文信息。c t 计算方法如下：</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges04ej3dzj304o02qglj.jpg" alt="image-20200514151719159" /></p>

<p>​		𝛼 𝑡𝑗 是 h j 的权重，计算方法如下：</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges04og4ukj306c022glm.jpg" alt="image-20200514151735337" /></p>

<p>​		𝑒 𝑡𝑗= 𝑎(𝑠 𝑡−1 , ℎ 𝑗 )是对齐模型，计算出 t 时刻生成词与第 j 个源语言词的匹配程度。</p>

<p>​		基于注意力的神经网络机器翻译在解码时只关注源语言句子中一部分区域，能够增强长 句子表示能力。相比普通的神经网络机器翻译，该方法在解码时融合了更多的源语言端信息， 可以显著提升机器翻译效果，是目前神经网络机器翻译的主流方法。</p>

<h2 id="23-资源稀缺条件下的藏汉神经网络机器翻译">2.3 资源稀缺条件下的藏汉神经网络机器翻译</h2>

<p>​		在训练语料较少情况下，神经网络机器翻译效果显著低于统计机器翻译方法，这在多个 语言对上得到验证 [23-24] 。迁移学习 [22] 能够将学习到的知识应用到相近任务上，可以减少应 用任务的训练数据量。在神经网络机器翻译中，迁移学习主要用在资源稀缺语言翻译上，可 以将资源丰富语言对的翻译模型参数，如英汉、英法等语言，迁移到资源稀缺语言上，如藏 语到汉语的翻译等，并在一些实验中得到了实际验证。</p>

<p>​		藏汉机器翻译同样存在训练语料不足问题，为此本文根据 Zoph 等人 [23] 思想，<strong>首先，采 用大规模英汉平行语料训练得到英语到汉语神经网络机器翻译模型。然后，在训练藏语到汉语神经网络机器翻译模型时，采用英汉翻译模型参数初始化藏汉翻译模型参数。最后，对由英汉翻译模型参数初始化后的模型，采用藏汉平行语料进行训练</strong>。</p>

<p>​		与 Zoph 等人 [23] 方法不同的是，本文方法对藏汉翻译模型的所有参数均采用英汉模型参数初始化，且在初始化时不要求两种翻译模型的汉语词向量（Word Embedding）一致。这种迁移学习方法简单，对翻译模型本身不做任何改变，并具有语言无关优点，因而更加适合训练语料极度缺少的特殊情况。</p>

<h1 id="3-实验与分析">3 实验与分析</h1>

<h2 id="31-实验设置">3.1 实验设置</h2>

<p>​		本实验采用的平行语料为 2011 年机器翻译研讨会（CWMT 2011）藏汉机器翻译评测语料，训练语料 10 万句，测试语料 650 句。迁移学习采用英汉平行语料，共 125 万句，从 LDC 语料中抽取得到。为了相对公平的对比，统计机器翻译语言模型训练采用大小 100M 的汉语文本语料，神经网络机器翻译不采用额外单语语料。</p>

<p>​		对训练语料句子长度限制为 50 词以下，双语词向量维数为 620，隐藏层大小为 1000， 解码时 Beamsize 大小为 10，系统选择 adadelta 方法 [28] 优化参数，训练中 mini-batch 大小为 80 句，Dropout [29] 设置为 0.5。为了减少未登录词问题，藏语、汉语词典大小设置为 3 万， 约覆盖 99%的词语。语料训练轮数最多为 60 轮。</p>

<p>​		本文采用的基线系统为东北大学开发的 Niutrans 短语统计机器翻译系统 [22] ，用“Niutrans” 表示。神经网络机器翻译系统为本课题组基于 Bahdanau [23] 工作所开发的基于注意力的神经 网络机器翻译系统，循环单元采用门限循环单元（Gated Recurrent Units, GRU） [9] ，用“NMT” 表示。本文提出的采用迁移学习实现的藏汉神经网络机器翻译系统用“NMT+Trans”表示。</p>

<p>​		由于测试语料限制，在统计机器翻译中，测试语料同样作为训练短语翻译模型的开发集， 即统计机器翻译结果是开发集上的结果。在神经网络机器翻译中测试语料同样作为开发集， 但是不影响到翻译模型，仅用于选择最优的翻译模型。以 BLEU-4 作为评测标准，采用基于 词的评测。</p>

<h2 id="32-主要实验结果">3.2 主要实验结果</h2>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges0alkulnj30am0600t0.jpg" alt="image-20200514152316177" /></p>

<p>​		从实验结果来看基线神经网络机器翻译系统的 BLEU 值低于短语统计机器翻译系统。 因为训练数据量较少，神经网络机器翻译模型并不能获得高质量的词汇语义映射关系。在数据量较少情况下，神经网络机器翻译效果低于短语统计机器翻译，这种情况同样出现在英汉、 英法等其他语言翻译中 [23-24] 。</p>

<p>​		本文通过迁移学习将语料资源规模较大的英汉神经网络机器翻译模型参数迁移到藏汉神经网络机器翻译中。相比基线神经网络机器翻译，本文方法提高了 6 个 BLEU 值，同时 也显著超过了短语统计机器翻译系统结果。</p>

<p>​		根据 Liu [24] 的实验，在 30 万句的汉英语料上，神经网络机器翻译比短语统计机器翻译 低了 10 个 BLEU 值。本文的平行语料更少，但是神经网络机器翻译与统计机器翻译效果仅 相差了 3 个 BLEU 值。我们认为是该训练语料题材很集中，且测试集与训练集题材也很一 致。因此，两种翻译方法并没有出现巨大差距。</p>

<h2 id="33-模型训练轮数对翻译结果影响">3.3 模型训练轮数对翻译结果影响</h2>

<p>​		在神经网络机器翻译中，要对训练语料整体训练多次，平行语料完整训练一遍称为一轮 （Epoch），通常经过 20 轮左右的训练，即可得到稳定的模型。因为藏汉平行语料较少，在常规训练轮数下，并不能得到稳定的结果。因此，本文对语料训练轮数对最终翻译结果的影响进行实验，结果如图 3 所示，实验采用基线神经网络机器翻译系统。</p>

<p><img src="/Users/suntian/Library/Application Support/typora-user-images/image-20200514152548610.png?lastModify=1595234692" alt="image-20200514152548610" /></p>

<p>​		可以看出，模型迭代到 30 轮时，取得了 28.37 的 BLEU 值，从 30 轮开始 BLEU 值缓慢 增长，直到迭代到 60 轮时取得最好翻译效果。为了防止过拟合，本文实验迭代轮数不超过 60。</p>

<h2 id="34-翻译结果示例">3.4 翻译结果示例</h2>

<p>​		本节对统计机器翻译和神经网络机器翻译的译文进行简单对比，主要从翻译忠实度、翻译流利程度、翻译不充分（部分词或短语没有被完整翻译）、过度翻译（部分词或短语被多 次翻译） [27] 等标准进行对比。对比结果如表 2 所示，“SMT”表示基线统计机器翻译系统译 文，“NMT+Trans”表示本文用迁移学习实现的藏汉神经网络机器翻译系统的译文，“UNK” 符号表示未登录。空格表示藏语、汉语词语之间的切分标志。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges0elstuwj30qe0j643f.jpg" alt="image-20200514152707637" /></p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges0evm43hj30qi03edg7.jpg" alt="image-20200514152723526" /></p>

<p>​		从翻译示例 1 可以看出，神经网络机器翻译相比统计机器翻译，可以很好地处理汉语语 序问题，翻译结果很流畅。</p>

<p>​		从翻译示例 2 可以看出，相对参考译文“本法 有关 规定”，神经网络机器翻译系统翻 译成“本 法律 有关 规定”，统计机器翻译系统的翻译结果为“本法 有关 规定”，能够看 出神经网络机器翻译具有很强的词义联想能力。</p>

<p>​		从翻译示例 3 可以看出，虽然神经网络机器翻译的译文很流畅，但是存在与原文语义相 差较大的问题，翻译忠实度较低，并且存在重复翻译现象。</p>

<p>​		翻译示例 4 句型结构比较复杂，神经网络机器翻译显然能够更好的理解原文意思，并且 翻译结果和更为流利。</p>

<p>​		翻译示例 5 中出现了未登录词，神经网络机器翻译和统计机器翻译均未能够准确翻译出 “账簿”这个词，且神经网络机器翻译出现严重的翻译不充分现象，即原文多个词语并没有 翻译被完整翻译出来。</p>

<p>​		从以上翻译结果可以看出来，藏汉神经网络机器翻译虽然存在神经网络机器翻译的不足 之处，如翻译不充分、过度翻译，对原文的翻译忠实度较低等缺点。但是从整体上看翻译结 果较为流畅，对翻译调序处理效果很好，且对原文语义理解能力更强，译文质量整体上优于 统计机器翻译方法。这种实验结果符合神经网络机器翻译在其他语言上应用的效果。</p>

<h1 id="4-结论与下一步工作">4 结论与下一步工作</h1>

<p>​		本文研究了神经网络机器翻译在藏汉翻译上的应用，并采用迁移学习方法将英汉神经网 络机器翻译模型参数迁移到藏汉神经网络机器翻译模型中。该方法显著提高了基线神经网络 机器翻译系统效果，并超过了短语统计机器翻译，取得了最好的翻译结果。</p>

<p>​		从实验结果可以得出，藏汉神经网络机器翻译同样具有神经网络机器翻译的共同优点和 不足之处。由于语料资源较少，基本的神经网络机器翻译模型效果仍然低于统计机器翻译方 法。本文提出的迁移学习方法简单有效，能够使藏汉神经网络机器翻译显著超过短语统计机 器翻译方法。该方法对神经网络模型不做任何改变，并具有语言无关性，理论上可以应用到 维吾尔语、蒙古语等其他资源稀缺语种的神经网络机器翻译中。</p>

<p>​		由于藏汉平行语料较少，语料资源是藏汉机器翻译的最大障碍。在后续研究中，我们将 采用其他方法提高资源稀缺条件下的藏汉神经网络机器翻译效果，以及研究本文方法在其他语言上的应用。</p>
:ET