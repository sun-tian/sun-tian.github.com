---
layout:     post           # 使用的布局（不需要改）
title:      Fairseq新能源机器翻译实验           # 标题 
subtitle:   Fairseq新能源机器翻译实验 #副标题
date:       2020-07-14             # 时间
author:     甜果果                    # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200715211802.png   #背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - nlp
    - paper
    - 机器翻译
    - 专利
    - 新能源

---

# Fairseq新能源机器翻译实验

# 一、数据介绍

新能源领域的专利文本，句子经过WIPO翻译，长度控制在100个词以内，经过初步的数据清洗（非中英句对、首末符号等），去重后，有中英句对116095条。

```
一种风电出力仿真模拟模型，其特征在于，首先根据多风电场历史实测风速数据或者风电场历史出力数据转化，拟合风速Weibull分布的尺度参数c与形状参数k、风电场风速序列自相关系数、多风电场之间风速相关系数矩阵
The wind power output simulation model is characterized by comprising the following steps: firstly, converting historical actually measured wind speed data or wind power field historical output data of a wind power plant, fitting a scale parameter C of the wind speed Weibull distribution with a shape parameter K, a wind power plant wind speed sequence self-correlation coefficient and a wind speed correlation coefficient matrix between the wind power plants
```

# 二、实验

## 1. 数据准备

1. 准备中英双语文本 116095 pairs [2]ne_zh_ok_clean.txt和[2]ne_en_ok_clean.txt
2. 中文分词、英文Moses

    1.  中文（哈工大语言云pyltp，未加术语表）
    2.  [英文](https://blog.csdn.net/Elenore1997/article/details/89483681)

Normalize punctuation

```
perl /root/tian/fairseq/examples/ne_WIPO_MT/mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l en < /root/tian/fairseq/examples/ne_WIPO_MT/data/[2]ne_en_ok_clean.txt > /root/tian/fairseq/examples/ne_WIPO_MT/data/[2]ne_en_ok_clean.norm.en
```

Tokenizer

```perl
perl /root/tian/fairseq/examples/ne_WIPO_MT/mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l en < /root/tian/fairseq/examples/ne_WIPO_MT/data/[2]ne_en_ok_clean.norm.en > /root/tian/fairseq/examples/ne_WIPO_MT/data/[2]ne_en_ok_clean.norm.tok.en
```

4.  分成6个文件

    | 1        | 2        | 3       |
    | -------- | -------- | ------- |
    | train.zh | valid.zh | test.zh |
    | train.en | valid.en | test.en |
    | 112095   | 2000     | 2000    |

    运行prepare_6_files.py脚本

# 2. sub-BPE处理

​		把subword-nmt和apply_bpe粘过来，并且将位置导出（是在data文件夹的外面）

```
export PYTHONPATH=$(pwd)/subword-nmt:$PYTHONPATH
```

​		打开ne_test文件夹，然后6个文件放在ne_test下，然后导出位置，接下来

创建词汇表

```python
python -m learn_joint_bpe_and_vocab --input train.zh train.en -s 32000 -o bpe.codes --write-vocabulary bpe.vocab.zh bpe.vocab.en
```

将字节对编码应用于我们的训练、开发和测试数据

```python
mkdir ne_WIPO_BPE_zh_en
python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.zh --vocabulary-threshold 50 < train.zh > ne_WIPO_BPE_zh_en/train.zh
python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.en --vocabulary-threshold 50 < train.en > ne_WIPO_BPE_zh_en/train.en
python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.zh --vocabulary-threshold 50 < valid.zh > ne_WIPO_BPE_zh_en/valid.zh
python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.en --vocabulary-threshold 50 < valid.en > ne_WIPO_BPE_zh_en/valid.en
python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.zh --vocabulary-threshold 50 < test.zh > ne_WIPO_BPE_zh_en/test.zh
python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.en --vocabulary-threshold 50 < test.en > ne_WIPO_BPE_zh_en/test.en

```

## 3. 二值化

​		需要退到fairseq文件夹下去运行命令

```python
TEXT=examples/ne_WIPO_MT/ne_WIPO_BPE_zh_en
fairseq-preprocess --source-lang zh --target-lang en --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/ne_WIPO.tokenized.zh-en --workers 20
```

​		在data-bin下会生成二值化数据

## 4. 训练

​	下面训练一个Transformer模型，训练之前别忘了开screen

```python
CUDA_VISIBLE_DEVICES=1 fairseq-train data-bin/ne_WIPO.tokenized.zh-en --arch transformer --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 4096 --eval-bleu --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --no-progress-bar --log-interval 20 --save-dir checkpoints_WIPO --keep-interval-updates 20 --tensorboard-logdir ne_WIPO_MT-transformer | tee ne_WIPO_MT-transformer.log
```

## 5. 怎么进行翻译呢？

Once your model is trained, you can generate translations using [fairseq-generate](https://fairseq.readthedocs.io/en/latest/command_line_tools.html#fairseq-generate) **(for binarized data)** or [**fairseq-interactive**](https://fairseq.readthedocs.io/en/latest/command_line_tools.html#fairseq-interactive) **(for raw text)**:

```python
fairseq-generate data-bin/ne.tokenized.zh-en --path checkpoints_ne2/checkpoint_best.pt --beam 5 --remove-bpe > predict_test.txt
```

​		怎么交互式的进行翻译呢？

```python
fairseq-interactive data-bin/ne.tokenized.zh-en --path checkpoints_ne2/checkpoint_best.pt --beam 5 --remove-bpe
```

![image-20200713163216226](https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200713163216.png)

## 6. 用tensorboard查看运行进度

```
tensorboard --logdir=./ne_WIPO_MT-transformer
```

