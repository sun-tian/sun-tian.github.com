---
layout:     post           # 使用的布局（不需要改）
title:      Scrapy获取网站词汇表           # 标题 
subtitle:   Scrapy获取网站词汇表 #副标题
date:       2019-11-16             # 时间
author:     甜果果                    # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/post-bg-ios9-web.jpg    #背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 技术
    - 爬虫
    - python

---

# Scrapy获取网站词汇表

![img](https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200725220747)

## 1、scrapy爬虫的创建

　　在vscode的Terminal中输入以下命令：

　　　　创建scrapy项目：scrapy startproject bio

　　　　进入到项目目录中：cd bio

　　　　创建一个新的spider：scrapy genspider bio2 yingyucihui.scientrans.com

第一个参数是 Spider 的名称， 第二 个参数是网站域名 。 执行完毕之后， spiders 文件夹中多了一个 bio2.py ， 它就是刚刚创建的 Spider, 内容如下所示：

```python
# -*- coding: utf-8 -*-
#http://yingyucihui.scientrans.com/shengwucihui/index.html

import scrapy
from bio.items import BioItem
from scrapy import Request

class Bio2Spider(scrapy.Spider):
    name = 'bio2'
    allowed_domains = ['yingyucihui.scientrans.com']
    start_urls = ['http://yingyucihui.scientrans.com/shengwucihui/238_65_1.html']

    def parse(self, response):
        pass

```



## 2、scrapy爬虫代码编写

### 2.1items文件编写

　　在items.py文件中定义自己要抓取的数据，我们要爬取英语词汇网站的中英单词这二者的数据，所以此时创建item的两个类。

```python
import scrapy

class BioItem(scrapy.Item):
    # define the fields for your item here like:
    
    # name = scrapy.Field()
    
    title = scrapy.Field()  # //dd/h2
    words = scrapy.Field()  # //dd
```

### 2.2编写spider文件（lesson.py）

```python
import scrapy
from bio.items import BioItem
from scrapy import Request

class Bio2Spider(scrapy.Spider):
    name = 'bio2'
    allowed_domains = ['yingyucihui.scientrans.com']
    start_urls = ['http://yingyucihui.scientrans.com/shengwucihui/238_65_1.html']

    def parse(self, response):
        print(response.url)
        title = response.xpath('//dd/h2/text()').extract_first()
        words = response.xpath('//dd/text()').extract()

        for item in zip(title, words):
            yield {
                # 'title':item[:],
                'words':item[1],
            }

        next1 = 'http://yingyucihui.scientrans.com/shengwucihui/'
        next2 = response.xpath("//span/a[@class='content2']/@href").extract()
        next = next1 + next2[-1]
        # print(next2[-1])
        yield Request(next)
```

## 3. 将内容保存到json文件中：

```python
scrapy crawl bio2 -o 生物化学词汇.json
```

>   [  {
>       **"words"**:**"脱落酸  ABA"**
>     },
>     {
>       **"words"**:**"脱碱基位点，无碱基位点  abasic site"**
>     },
>     {
>       **"words"**:**"远轴的  abaxial"**
>     },
>     {
>       **"words"**:**"阿比可糖，beta脱氧岩藻糖  abequose"**
>     },
>     {
>       **"words"**:**"异常剪接  aberrant splicing"**
>     },]