---
layout:     post           # 使用的布局（不需要改）
title:      OpenNMT新能源机器翻译实验           # 标题 
subtitle:   OpenNMT新能源机器翻译实验 #副标题
date:       2020-06-23             # 时间
author:     甜果果                    # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200715211802.png   #背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - nlp
    - paper
    - 机器翻译
    - 专利
    - 新能源

---

# OpenNMT新能源机器翻译实验

# 一、环境和数据准备

1.  在服务器上新开一个叫OpenNMT的环境，然后开screen
2.  准备四个文件src-train.txt、tgt-train.txt、src-val.txt、tgt-val.txt，其中数据是经过分词之后的，用onmt_preprocess脚本处理数据，它会把train和trg，val的文件处理成三个叫demo.pt的文件里

```python
onmt_preprocess -train_src data/src-train-fenci.txt -train_tgt data/tgt-train-fenci.txt -valid_src data/src-val-fenci.txt -valid_tgt data/tgt-val-fenci.txt -save_data data/demo
```

PS：也可以在数据处理之前用subword-nmt进行子词处理，但是我没有进行实验

​		bpe，subword-nmt子词处理](https://blog.csdn.net/jmh1996/article/details/89286898)

​		生成一个字典

```python
subword-nmt learn-joint-bpe-and-vocab -i src-train.txt -o src-train-code.file --write-vocabulary src-train-voc.txt
subword-nmt apply-bpe -i src-train.txt -c src-train-code.file -o src-train-bpe.txt
```

# 二、训练

训练，使用的是Transformer模型，使用CUDA_VISIBLE_DEVICES=1指定特定的GPU，使用nvidia-smi监视GPU的使用率，减小batch_size的大小，从4096到512才行，还加了log日志，还加了tensorboard可视化，美滋滋~

```python
onmt_train -data data/demo -save_model data/OpenNMT_News_zh_en_transformer_model -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  -encoder_type transformer -decoder_type transformer -position_encoding         -train_steps 200000  -max_generator_batches 2 -dropout 0.1         -batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2         -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2         -max_grad_norm 0 -param_init 0  -param_init_glorot         -label_smoothing 0.1 -valid_steps 2000 -save_checkpoint_steps 2000 -gpu_ranks 0
```

# 三、翻译

```python
onmt_translate -model data/OpenNMT_News_zh_en_transformer_model_step_2000.pt -src data/src-test-fenci.txt -output data/pred-fenci-2000.txt -replace_unk -verbose -log_file data/logoutput-2000.log -log_file_level
```

# 四、评估

```python
perl multi-bleu.perl tgt-test.txt < pred-fenci-12500.txt > res.txt

第一个参数是参考文件，只需要给出前缀，程序会自动加上数字来寻找，即程序会查找 test.ref.0, test.ref.1, …  
第二个参数是要评估的文件， < 要进行评估的结果文件
第三个参数是存放评估结果的文件 > 存放评估结果的文件

BLEU = 4.66, 35.2/10.6/4.0/1.7 (BP=0.655, ratio=0.702 , hyp_len=37765, ref_len=53761)
```





