---
layout:     post           # 使用的布局（不需要改）
title:      SooPAT爬虫           # 标题 
subtitle:   SooPAT爬虫 #副标题
date:       2020-07-11             # 时间
author:     甜果果                    # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/post-bg-ios9-web.jpg    #背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 技术
    - 爬虫
    - python

---

# SooPAT爬虫

# SooPAT_crawl

## 一、获取单页上的patent_num

以新能源为关键字，搜索的url是

[http://www1.soopat.com/Home/Result?Sort=&View=6&Columns=&Valid=&Embed=&Db=&Ids=&FolderIds=&FolderId=&ImportPatentIndex=&Filter=&SearchWord=%E6%96%B0%E8%83%BD%E6%BA%90&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y](http://www1.soopat.com/Home/Result?Sort=&View=6&Columns=&Valid=&Embed=&Db=&Ids=&FolderIds=&FolderId=&ImportPatentIndex=&Filter=&SearchWord=新能源&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y)

![image-20200706104153779](/Users/suntian/Library/Application Support/typora-user-images/image-20200706104153779.png)



```python
import requests 
import bs4

url = 'http://www1.soopat.com/Home/Result?Sort=&View=6&Columns=&Valid=&Embed=&Db=&Ids=&FolderIds=&FolderId=&ImportPatentIndex=&Filter=&SearchWord=%E6%96%B0%E8%83%BD%E6%BA%90&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y'
html = requests.get(url)
soup = bs4.BeautifulSoup(html.text, 'lxml')
# res = soup.find_all(class_='td_line_bottom')
# print(res)
patent_num = []
res2 = soup.select('td ul li')
num = 0
for i in res2:
    if num % 2 == 0:
        patent_num.append(i.text)
    num += 1
    
# patent_num =  ['201910280329.0', '201010507258.2', '201010507302.X', '201020561759.4', '201010507258.2', '201020561654.9', '201911188828.3', '201020561663.8', '201911188791.4', '201020561687.3', '201020561733.X', '201721222011.X', '201010507329.9', '201010507276.0', '201310512089.5', '201010569880.6', '201821226864.5', '201810862347.5', '201821227551.1', '201821233466.6', '201720850640.0', '201911188822.6', '201911189201.X', '201020561672.7', '201010576119.5', '201810860450.6', '201010598204.1', '201010592160.1', '201920387592.5', '201010599880.0']
```

## 二、观察网址标题规律、获得所有的patent_nums

[http://www1.soopat.com/Home/Result?SearchWord=%E6%96%B0%E8%83%BD%E6%BA%90&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y&PatentIndex=30&View=6](http://www1.soopat.com/Home/Result?SearchWord=新能源&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y&PatentIndex=30&View=6)

[http://www1.soopat.com/Home/Result?SearchWord=%E6%96%B0%E8%83%BD%E6%BA%90&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y&PatentIndex=60&View=6](http://www1.soopat.com/Home/Result?SearchWord=新能源&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y&PatentIndex=60&View=6)

http://www1.soopat.com/Home/Result?SearchWord=%E9%A3%8E%E8%83%BD&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y&PatentIndex=60&View=6

构造网址

```
index = 0
full_url = 'http://www1.soopat.com/Home/Result?SearchWord=%E6%96%B0%E8%83%BD%E6%BA%90&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y&PatentIndex=' + str(30*i) + '&View=6'
搜索结果是149071，一个页面上是30个结果，所以是4970个页面
```

```python
import requests 
import bs4
import time

patent_nums = []
def get_one_page_patent_nums(url):
    html = requests.get(url)
    soup = bs4.BeautifulSoup(html.text, 'lxml')
    res = soup.select('td ul li')
    num = 0
    for i in res:
        if num % 2 == 0:
            patent_nums.append(i.text)
        num += 1

def write_patent_nums_into_file(patent_nums_list):
    patent_nums = open('patent_nums2.txt', 'a')
    for num in patent_nums_list:
        patent_nums.write(num.strip())
        patent_nums.write('\n')

def main():
    index = 0
    for i in range(1, 4970):
        full_url = 'http://www1.soopat.com/Home/Result?SearchWord=%E6%96%B0%E8%83%BD%E6%BA%90&FMZL=Y&SYXX=Y&WGZL=Y&FMSQ=Y&PatentIndex=' + str(30*i) + '&View=6'
        get_one_page_patent_nums(full_url)
        print('正在获取第%d页pantent_nums...'%(i+1))
        print(len(patent_nums))
        write_patent_nums_into_file(patent_nums)
        time.sleep(1)

if __name__ == "__main__":
    main()
```

## 三、获取单页上的中英句对

```python
import bs4
import requests
import time
import os

def get_title_abs_pairs(url):
	htmlfile = open(url, 'r', encoding='utf-8')
	htmlhandle = htmlfile.read()
	soup = bs4.BeautifulSoup(htmlhandle, features='lxml')
	# html = requests.get(url)
	# soup = bs4.BeautifulSoup(html.text, 'lxml')
	titles = soup.select('.detailtitle h1')
	try:
		zh_title = titles[0].text.strip().split('\n')[0]
		# print(zh_title)
		en_title = titles[1].text.strip()
		abss = soup.select('td')[:2]
		zh_abs = abss[0].text.strip()
		en_abs = abss[1].text.strip()
		write_2_file(zh_title, en_title, zh_abs, en_abs)
	except Exception as e:
		print(e)

def write_2_file(zh_title, en_title, zh_abs, en_abs):
	soo_pat_ne_title_abs_pairs = open('soo_pat_ne_title_abs_pairs_wind.txt', 'a')
	soo_pat_ne_title_abs_pairs.write(zh_title.strip())
	soo_pat_ne_title_abs_pairs.write('\n')
	soo_pat_ne_title_abs_pairs.write(en_title.strip())
	soo_pat_ne_title_abs_pairs.write('\n')
	soo_pat_ne_title_abs_pairs.write(zh_abs.strip())
	soo_pat_ne_title_abs_pairs.write('\n')
	soo_pat_ne_title_abs_pairs.write(en_abs.strip())
	soo_pat_ne_title_abs_pairs.write('\n')

def main():
	# patent_nums = open('patent_nums5.txt').readlines()
	# base_url = 'http://www1.soopat.com/Patent/'
	# for num in patent_nums:
	# 	url = base_url + num.strip()
	# 	print(url)
	# 	get_title_abs_pairs(url)
	# 	time.sleep(5)
	# urls = ['200810194494.6 - 一种风能设备铸件的熔炼工艺 - SooPAT专利搜索.html',
	#      '201010507258.2 - 风氢新能源应用在水稻直播机上的动力装置 - SooPAT专利搜索.html',
	#      '201020561654.9 - 风氢新能源应用在水稻直播机上的动力装置 - SooPAT专利搜索.html',
	#      '201520723745.0 - 一种高空风能风力发电装置及风能动力系统 - SooPAT专利搜索.html',
	#      '201711296084.8 - 一种实现中高空风能发电装置自动收回的方法 - SooPAT专利搜索.html',
	#      '201711299009.7 - 一种中高空风能发电系统的控制方法 - SooPAT专利搜索.html',
	#      '201910280329.0 - 一种基于区块链的新能源结算系统 - SooPAT专利搜索.html',
	#      '201911188828.3 - 一种新能源电池组 - SooPAT专利搜索.html']
	files = [i for i in os.listdir('./soopat_wind/') if i.endswith('html')]
	base_url = '/Users/.../codes/MT/ne_paired/20200706SooPAT/soopat_wind/'
	urls = [base_url+i for i in files]
	for url in urls:
		get_title_abs_pairs(url)
		# time.sleep(5)

if __name__ == '__main__':
	main()
```

## 四、点触验证码的识别

