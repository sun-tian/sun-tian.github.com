---
layout:     post           # 使用的布局（不需要改）
title:      NLPCC 2019 papers爬虫           # 标题 
subtitle:   NLPCC 2019 papers爬虫 #副标题
date:       2019-10-16             # 时间
author:     甜果果                    # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/post-bg-ios9-web.jpg    #背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 技术
    - 爬虫
    - python

---

# NLPCC 2019 papers爬虫

NLPCC 2019会议计划表上列出了paper列表，可以使用爬虫技术批量获取，省去一个一个下载的麻烦。

![image-20200725155534814](https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200725155641.png)

```python
import requests
from lxml import etree

# 1. 获取NLP论文GitHub地址网页源码

result = requests.get('http://tcci.ccf.org.cn/conference/2019/pro_session.php').content
# 2. 解析页面并获取所有论文标题名称

html = etree.HTML(result)
names = html.xpath("//div[@class='programtitle']/a/span")
print(len(names))
# 4. 获取所有论文链接地址，并存储在resu列表里 579条记录

resu = []
links = html.xpath("//div[@class='programtitle']/a/@href")
for link in links:
    if link.startswith('/login'):
        continue
    if link.startswith('https://scholar.google.com'):
        continue
    if link.endswith('en'):
        continue
    resu.append(link)
# 5. 下载单个论文函数

def download2(name, url):
    try:
        print('Downlodaing '+str(name)+' '+str(url))
        r = requests.get(url,stream=True)
        with open(name,'wb') as pdf:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:
                    pdf.write(chunk)
    except:
        pass
# 6. 下载论文

for i in range(len(names)):
    download2(names[i], resu[i])
```

