---
layout:     post                    # 使用的布局（不需要改）
title:      2019-08-15-seq2seq代码学习               # 标题 
subtitle:   库函数的学习 #副标题
date:       2019-08-15              # 时间
author:     甜果果                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - pytorch 
    - nlp
---

# 1 - Sequence to Sequence Learning with Neural Networks
### 1. spacy
[spaCy](https://spacy.io/) 是一个 Python 和 CPython 的 NLP 自然语言文本处理库。
所有需要了解和学习的内容都在网站上可以找到，写的非常清楚，不用再搬运过来了。
[使用 spacy 进行自然语言处理（一）](https://blog.csdn.net/u012436149/article/details/79321112)，这篇文章用中文讲解了spacy的用法，回头用的到的时候就可以好好看看了。

### 2. TORCH.NN.FUNCTION
[torch.nn.function](https://pytorch.org/docs/stable/nn.functional.html)

### 3. random.seed()的使用
[random.seed\(\)的使用](https://blog.csdn.net/linzch3/article/details/58220569)
seed( ) 用于指定随机数生成时所用算法开始的整数值。 
1.如果使用相同的seed( )值，则每次生成的随即数都相同； 
2.如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。 
3.设置的seed()值仅一次有效

### 4. torch.manual_seed()
[torch.manual_seed\(\)](https://blog.csdn.net/qq_34690929/article/details/79923602) 且每次生成的数据都是一样的。
```
torch.manual_seed(args.seed) #为CPU设置种子用于生成随机数，以使得结果是确定的 
if args.cuda: 
    torch.cuda.manual_seed(args.seed)#为当前GPU设置随机种子；如果使用多个GPU，应该使用torch.cuda.manual_seed_all()为所有的GPU设置种子。
```

### 5. 分词的练习
```
# 分词函数的练习
strings = 'You are the best!'
def tokenize(strings):
    words = strings.split(' ')
    return [word for word in words]

def tokenize2(strings):
    words = strings.split(' ')
    return [word for word in words][::-1]
tokenize(strings), tokenize2(strings)

> (['You', 'are', 'the', 'best!'], ['best!', 'the', 'are', 'You'])
```

### 6. torchtext.data以及torchtext.datasets
[torchtext](https://blog.csdn.net/u012436149/article/details/79310176) 这篇博客写得好哇，是torchtext的入门级文章。
**torchtext.data**
torchtext.data.Example : 用来表示一个样本，数据+标签
torchtext.vocab.Vocab: 词汇表相关
torchtext.data.Datasets: 数据集类，__getitem__ 返回 Example实例
torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）
创建 Example时的 预处理
batch 时的一些处理操作。
torchtext.data.Iterator: 迭代器，用来生成 batch
**torchtext.datasets**: 包含了常见的数据集.

TorchText 的数据预处理流程为：
1. 定义样本的处理流程，field。
2. 加载corpus，都是string
3. 创建词汇表
4. 将处理后的数据进行batch操作

一个简单的小例子帮助我理解Torchtext的处理流程，还是感觉好难哇T_T，等回头用到了再好好学吧T_T.

### 7. torch.nn.Embedding
[torch.nn.Embedding](https://www.cnblogs.com/lindaxin/p/7991436.html)
在pytorch里面实现word embedding是通过一个函数来实现的:nn.Embedding
embeds = nn.Embedding(2, 5) # 这里的2表示有2个词，5表示5维度，其实也就是一个2x5的矩阵，所以如果你有1000个词，每个词希望是100维，你就可以这样建立一个word embedding，nn.Embedding(1000, 100)
```
# -*- coding: utf-8 -*-
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
 
word_to_ix = {'hello': 0, 'world': 1}
embeds = nn.Embedding(2, 5)
hello_idx = torch.LongTensor([word_to_ix['hello']])
hello_idx = Variable(hello_idx)
hello_embed = embeds(hello_idx)
print(hello_embed)
```
```
Variable containing:
 0.4606  0.6847 -1.9592  0.9434  0.2316
[torch.FloatTensor of size 1x5]
```

### 8. torch.nn.LSTM()函数维度详解
[torch.nn.LSTM\(\)函数维度详解](https://blog.csdn.net/m0_37586991/article/details/88561746)
```
import torch
import torch.nn as nn
lstm = nn.LSTM(10, 20, 2)
x = torch.randn(5, 3, 10)
h0 = torch.randn(2, 3, 20)
c0 = torch.randn(2, 3, 20)
output, (hn, cn)=lstm(x, (h0, c0))

>>
output.shape  torch.Size([5, 3, 20])
hn.shape  torch.Size([2, 3, 20])
cn.shape  torch.Size([2, 3, 20])
```
![9bedc4baa24de97bf2deb327f35281fc.png](evernotecid://A48E35C2-11B7-4FAB-B1CB-CE27DEC23A2D/appyinxiangcom/7989011/ENResource/p551)
举个例子：
对句子进行LSTM操作

假设有100个句子（sequence）,每个句子里有7个词，batch_size=64，embedding_size=300

此时，各个参数为：
input_size=embedding_size=300
batch=batch_size=64
seq_len=7

另外设置hidden_size=100, num_layers=1
```
import torch
import torch.nn as nn
lstm = nn.LSTM(300, 100, 1)
x = torch.randn(7, 64, 300)
h0 = torch.randn(1, 64, 100)
c0 = torch.randn(1, 64, 100)
output, (hn, cn)=lstm(x, (h0, c0))

>>
output.shape  torch.Size([7, 64, 100])
hn.shape  torch.Size([1, 64, 100])
cn.shape  torch.Size([1, 64, 100])
```

### 9. torch.nn.dropout
[torch.nn.dropout](https://blog.csdn.net/u014532743/article/details/78453990)
- 注1：设置 Dropout 时，torch.nn.Dropout(0.5), 这里的 0.5 是指该层（layer）的神经元在每次迭代训练时会随机有 50% 的可能性被丢弃（失活），不参与训练，一般多神经元的 layer 设置随机失活的可能性比神经元少的高
- 注2：由于要 Dropout，以及凸显过拟合现象，所以 layer 的神经元设置多些，这里为 300
