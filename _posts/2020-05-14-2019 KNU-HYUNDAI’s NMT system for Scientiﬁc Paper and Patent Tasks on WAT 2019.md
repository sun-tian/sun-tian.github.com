---
layout:     post           # 使用的布局（不需要改）
title:      2019 KNU-HYUNDAI s NMT system for Scientiﬁc Paper and Patent Tasks on WAT 2019           # 标题 
subtitle:   KNU-HYUNDAI s NMT system for Scientiﬁc Paper and Patent Tasks on WAT 2019 #副标题
date:       2020-05-14             # 时间
author:     甜果果                    # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/post-bg-coffee.jpeg    #背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - nlp
    - paper
    - 机器翻译
    - 专利

---

# 2019 KNU-HYUNDAI’s NMT system for Scientiﬁc Paper and Patent Tasks on WAT 2019

![image-20200720163150698](https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200720163222.png)

# 摘要

​		在本文中,我们描述了由江原道国立大学和现代(KNU- HYUNDA)团队提交给第六届亚洲翻译研讨会WAT2019)的翻译任务的神经机器翻译(NMT)系统。我们参与了 ASPEC和JPC2的所有任务,包括中日语、英日语韩语→日语。**我们提交了基于变压器的NMT系统,使用以下方法构建a)相对定位方法对于输入元素两两之间的关系,b)反向翻译和多源转换为数据增加。c)从右到左(r2l)reranking模型健壮与自回归误差传播架构如解码器。和d)检査点整体模型选择最好的三大模型验证双语评价替补(BLEU)**。我们已经报告了上述两项任务的翻译结果。我们在这两项任务中都表现得很好,并且在我们参与的所有JPC2子任务的BLEU得分中排名第

# 1 介绍

​		由于对神经网络的研究,机器翻译领域得到了长足的发展。机器翻译已经尝试了许多方法从简单的方法,如两个递归神经网络(RNN)的编码-解码器( Bahdanau et a.,2014),到包含多个层次和多头部注意的变压器模型 aswani etal,2017)。此外,随着 OpenNMT1等开源软件的发展( Klein et al,2017),任何拥有并行语料库的人都可以轻松地挑战神经机器翻译NMT)。
​		我们在这里描述了KNU- HYUNDA|的NMT系统,它使用了基于 OpenNMT的变压器模型。我们参与了WAT2019的 ASPEC( Nakazawa et al,2016)和JPc2任务( Nakazawa et al,2019)。ASPEC任务由英、日、中、日平行语料库组成,JPC2任务由英、日、中、日、韩、日平行语料库组成。
​		**为了解决开放词汇表问题,我们将所有数据预处理为子单词单元,称为字节对编码(bytepair-encoding,BPE)( Sennrich et al,2015b)。我们将源语言和目标语言编码为共享字典。编码的子字随后被转换为嵌入相对位置表示( shaw etal,2018),并传输到变压器。**
​		**我们尝试了三种使用其他资源的方法。1)通过混合不同的相同语言对的平行语料库进行训练2)将单语语料库的反向翻译( Sennrich et al,2015a)加入到当前的训练集。3)根据多源翻译(zoph和 Knight,2016)训练两对不同的源,日标相同的模型来扩充数据集(Zoph和Knight,2016)**。在翻译时,我们对( Liu et a,2016)使用后向(从右到左)技术解码的训练模型生成的文本和使用前向(从左到右)技术解码的模型重新排序。

# 2系统概述

## 2.1变压器

​		我们的基础系统基于在 OpenNMT中实现的transformer R F-j(Vaswani et al., 2017)( Klein etal,2017)。该变压器由多磁头注意和前馈神经网络(FFNN)组成。 Head(Q,K,V)等多头注意计算每个正面的Q、K和V矩阵的点积注意系数得分,并将所有正面的注意系数连接起来,其公式如下:
![image-20200514161438924](https://tva1.sinaimg.cn/large/007S8ZIlly1ges1s1ushsj30f207kq3m.jpg)
​		这里使用的多头注意力类似于自我注意,并通过捕捉自身的结构信息来计算注意力得分。变压器的编码器对Q、K、Ⅴ进行相同的编码,隐藏状态的维数被h分割并分别乘以WQi、WK、MV。输入Qi、Ki、Ⅵ的注意分值采用点积注意分值进行计算,因此计算得到的注意分值为headi。所有的 headi与Wo相乘,得到了mulihead attention的隐藏状态。然后,执行max(0,XW1+b1)W2+b2,生成变压器组输出,这是个位置向的FFNN。
​		变压器译码器的性能类似于编码器,但通过屏蔽一次从左到右生成一个字。该译码器由三个子层组成:第一个子层是一个掩蔽的多头自我注意它强制注意前一个单词。第二层是多头注意,其次是编码器-解码器注意。最后的子层是一个位置前馈层。变压器模型在每个子层周围使用残差连接(Heta,2016)和层归一化( a et al,2016)。

## 2.2相对位置表示

​		与递归神经网络和卷积神经网络相比,transformer没有显式地对其输入进行相对或绝对位置建模。转换器将位置编码添加到嵌入中,以考虑单词的位置信息。这种类型的编码通过为输入字添加绝对位置表示来进行序列建模为相对位置编码( Shaw et al,2018),使用一个自我注意扩展模型来考虑输入元素之间的成对关系。通过建模的输入作为一个连通图,相对位置编码代表输入习之间的边缘和向量a刈,ajK。这些向量表示输入元素之间位置相对差的信息。相对位置信息是通过添加嵌入向量ajV合并,aKj可以训练 self-attention层在方程(4-6)。
![image-20200514161550764](https://tva1.sinaimg.cn/large/007S8ZIlly1ges1tb13olj30b608cgm3.jpg)

## 2.3 Data Augmentation

​		在深度学习中,需要大量的数据才能获得优异的学习成绩,但是数据标注的代价很高。数据扩充可以通过自动增加训练数据量来提高模型效率。在自然语言处理中,通过使用外部资源、反向翻译或文本生成来扩充数据。
​		在此,我们使用WAT2019( ASPEC,JPC2)提供的一些数据,通过反向翻译和多源翻译来执行数据扩充,这在NMT中经常使用。

### 2.3.1 反向翻译

​		反向翻译( enrich et al,2015a)在NMT单语数据集成中是一种有效且应用广泛的数据增强技术。针对源语言和目标语言,反向进行训练,然后利用模型将新语料库翻译成目标语言。用于翻译和翻译句子的语料库形成一个自动生成的平行语料库,并在原语料库的基础上对翻译模型进行重新训练。我们使用WAT中提供的并行语料库进行反向翻译。

### 2.3.2 用于增强的多源翻译

​		多源翻译( Zoph and Knight,2016)是一种将多种源语言作为输入输入到同一日标语言的训练方法,以提高NMT的质量。在训练 transformer模型时,我们使用相同的目标语言和不同的源语言。例如,如果转换为zh→Ja,我们添加En→Ja和Ko→Ja数据集一起训练。符号Zh、En、Ko和Ja分别表示汉语、英语、韩语和日语。

## 2.4 从右到左的评估

​		序列到序列模型的解码器是一种自回归体系结构,它使用以前的预测作为上下文信息。如果之前的预测是不正确的,这个错误将作为噪音,降低下一个预测的质量。为了解决这个问题,Liu et al、(2016)提出了右到左(r2)模型,将由左到右(2)模型产生的 n-best假设重新转换为r2模型。「2|重新排序模型的公式如下(Mor- ishitaetal,2018)
![image-20200514161757218](https://tva1.sinaimg.cn/large/007S8ZIlly1ges1vhl78oj30du0140ss.jpg)

# 3 实验

​		WAT2019的子任务( Nakazawa et al,2019)包括使用亚洲科学论文摘录语料库( ASPEC)的科学论文( Nakazawa et al,2016)和使用日本专利局专利语料库20的专利任务(JPC2)。我们参与了这两项任务。 ASPEC由英-日(EnJa)和中日(zho-Ja)组成,JPC2由英日(En-Ja)、中-日(Zh-Ja)、韩-日(K0-Ja)组成。

## 3.1 数据集

​		每个子任务的数据集统计数据如表1所示。由于对ASPEC En-Ja数据集进行了相似度评分排序,因此我们使用多达100000M)个平行句子作为训练数据集,使用多达200000)个句子作为反向翻译。

![image-20200514161904634](https://tva1.sinaimg.cn/large/007S8ZIlly1ges1wnnoxbj30ey05ujs7.jpg)

表1:并列句统计(句)

## 3.2 Tokenization

​		我们使用基于bpe的算法进行子单词分割。使用该算法,可以通过固定大小的词汇表将一个句子表示为子单词序列,并求解生字与生字问题的有效解决。 SentencePiece使用了BPE应用程序。 SentencePiece使用基于nfkc的文本规范化来实现句子规范化。因此,在生成的翻译句中,规范化的句子,如C,被改成了℃。我们为每个语言数据集使用了32,000个共享词汇表。日语句子使用 Juman++2进行切分Tolmachev et al,2018:( Kurohashi,2018),对中文数据集的标记化使用斯坦福词根 wegener3( Chang et al,2008)。英语句子用 Moses4进行标记,韩国语句子用 Mecab5进行形态分析(Sim,2014; Matsumoto et al., 1999)

## 3.3 实验装置

​		我们在实验中使用了 OpenNMT变压器。具体采用了Open-NMT的早期停止方法。当模型没有达到具有验证精度的10个保存点(每5000步保存)的新最大精度时,训练就停止了。我们选择了BLEU得分最高的验证模型。当我们对z0-ja数据集进行训练时,除了 OpenNMT中的 early-stop方法外,我们选择了所有验证模型中BLEU得分最高的验证模型。我们将超参数优化为6层,单词嵌入尺寸为512FFNN维度尺寸为2048,注意头数量为8个,训练步骤数量为20万个,dropout为0.1,批处理尺寸为4096acm为2学习率为2。我们对所有模型使用相同的超参数进行训练,并将En-Ja、z-Ja的解码波束大小设置为12,Ko→Ja设置为2。

## 3.4 评估

​		我们测量了排行榜上的双语评价指标(BLEU( Papineni et al,2002)、基于排名的直观双语评价评分(RBES)( Isozaki et a,2010)、适当流利度指标(AMFM)( Banchs et al.,2015)。BLEU的计算方法是用单字符、双字符、三字符和4字符的精度的几何平均值乘以一个简短值。R|BES它提供的值在[o;[ endnoteref:1]被用来解决BLEU的缺点,特别是在遥远的语言对中,语序的变化会降低BLEU的有效性。我们还提交了份手工评估,如两两配对众包评估和JPO充分性评估,提交超过三份的情况下进行。

## 3.5 实验结果

### 3.5.1 英文日文

​		表2给出了我们在WAT209年 ASPEC和JPC2子任务中提交的系统的BLEU得分和排名。在ASPEC En→Ja和Ja→En任务中,我们分别获得了4408和30.88的BLEU分数,并且在五个小组中排名第四,在提交BLEU分数的四个小组中排名第二(B级)。在这个时候我们是排名第五的5支球队,第二例人评价的四个团队(H排在表2)。在JPC2En→是的a→任务,我们的系统记录了47.38和44.72的分数分别因此我们对双语数据排名第1。

​		用于 ASPEC数据集的英语日语表3显示了ASPEC数据集的En-Ja的累积特征消融。我们只使用包含1M并行语句的训练数据集的上部rain-1)将基线模型训练为 transformer基础模型我们对基线模型应用相对定位,使病例的BLEU评分提高051和023分别为En→Ja和Ja→En。我们使用剩下的包含2M个句子的训练数据集进行反向翻译,将反向翻译生成的合成数据加入到训练-1并行数据中进行训练。我们对 train-1并行数据进行过采样,并以1:1的比例使用并行和复合数据。我们在句子的开头使用hBT标记进行训练,以便进行反向翻译( Caswell et al.,2019)。通过反向翻译,我们的系统在En→Ja、Ja→En中的BLEU分数分别提高了141和0.62。
​		接下来,我们进行了检查点集成和独立集成。在第一轮培训中设置的检查点中,前三名模型的验证BLEU得分最高。同样,后者也被用来训练前三名拥有最佳验证BLEU分数的模特。检查点集成方法的性能提高了0.95和0.30的BLEU评分,与1.52和0.54的BLEU评分无关。最后,r2模型通过从右到左的集成模型(类似于12r集成方法)重新获得了从左到右模型的12个最佳输出(光束大小为12)。我们的模型的最佳性能是在En→Ja数据集的BLEU评分为44.08,在Ja→En数据集的BLEU评分为3088
​		JPc2数据集的英日语:仅使用JPCc2数据集的1M并行语句的训练数据集,基线模型使用来自OpenNET的 transformer基础模型进行训练。在表4中,我们演示了JPC2数据集(En→Ja(a)不使用外部资源和AS- PEC En-Ja数据集作为外部资源(En→Ja(b),Ja→En)时的性能。在已有的JPC2训练集上增加En-Ja的 ASPEC训练集-1,对En→Ja(b)和Ja→En模型进行训练。该方法提高了186分和143分在基线。通过相对定位,En→Ja(a)评分较基线提高0.28分,En→Ja(b)、Ja→En分别比 ASPEC数据添加方法高0.37分、0.46分。
​		在En→Ja(a,b)中,反向翻译过程使用了JPC2jz-Ja和Ko-Ja数据集中的日本数据集的2M个单句。Ja→En使用 ASPEC Ja-En数据集中剩余的2M个句子的训练数据集进行反向翻译。En→Ja(a)和Ja→En对JPc2并行数据进行过样,保证与并行数据相加与反向平移的数据比例为1:1。在反向翻译过程中,我们还在句子的开头插入了一个反向翻译标记hBT。通过添加反向翻译数据,与前一种情况(+相对位置)相比En→Ja(a)的BLEU评分提高了289分,En→a(b)的BLEU评分提高了143分,Ja→En的BLEU评分提高了045分。此外,我们验证了BLEU的性能为4536,比前一步(+相对位置)提高了044,并且对于多源应用,我们使用了z2Ja并行数据,而不是在En→Ja()中进行反向转换。
​		我们执行了检查点集成方法。通过从验证中选择前三名的模型,这使得BLEU的得分超过了0.3。最后,我们使用检査点集成方法将I2r模型的12个最佳输出(光束大小为12)重新分配给一个r2模型,以将各自的BLEU分数提高0.87、0.56和078。

表4:JPC2En-Ja子任务的消融方法352中日

表5:排行榜上中日任务的BLEU得分

表7 ASPEC Ja→zh子任务的消融方法

表8:JPc2zh→Ja子任务的消融方法

表12我们模型的充分性评估

### 3.5.2 Chinese-Japanese

​		在本文中,我们将AS-PEC和JPC2两种数据集方法结合起来,分别用于每个子任务:(1)只使用ASPEC或JPC2数据集,(2)同时使用这两种数据(3)使用11的数据比例。我们使用快速对齐工具来匹配的速度数据集。由于JPC2中的语句数为100万,比ASPEC数据集大,所以在专利任务中,11的数据集实验不进行实验。基线是只有 ASPEC或JPC2数据集的 transformer模型。在专利任务的Ja→zh和zh→Ja子任务中,同时使用两个数据集的方法比只使用一个数据集的方法性能更好表5给出了本文所用方法的每个Zh→Ja子任务的BLEU结果。我们使用的系统在 ASPEC数据集的zh→Ja和Ja→zh子任务上分别得分5192和3677BLEU。JPC2数据集的zh→Ja和Ja→zh子任务得分分别为5133和43.30BLEU,因此我们在三个团队中排名第一。
​		ASPEC数据集中文→日语 aspec Zh→Ja子任务的基线性能为4724BLEU评分,是所有组合实验中最高的。基线比使用两个数据集的实验高01分,比使用1:1数据集的实验高0.18分
​		相对定位法提高了0.49BLEU评分。我们使用ASPEC En-Ja数据集的700K个句子进行反向翻译。再次添加现有数据集,将现有数据集与反向转换数据集的比例调整为21。这导致了0.37F1BLEU分数的增加。在多源实验中使用 ASPECEn-Ja数据集的1M个句子时,该方法增加了095个句子。「2重新排序将导致052的性能改进。9个检査点集成模型对6个不同模型的性能是51.92BLEU评分。

​		该任务的最高执行模型之间的差异是235BLEU评分。用于训练这个任务的最终模型的句子对的数量是3044630。

​		日语→中文,用于 ASPEO数据集AS在仅使用 ASPEC数据集(基线)的实验中,PECJa→zh子任务显示3493F1BLEU评分。将ASPEC与JPC2比例调整为11的实验在所有数据组合实验中得分最高。该实验的 BLEU F1得
分为3503,比使用两个数据集的结果高012。
​		相对定位的得分提高了0.2分。我们使用670 K ASPEC En-Ja数据集进行了反向翻译,但是没有发现性能的提高。「2|重新排序方法的得
分增加了046分。
​		最后的成绩是3677,这是 ensemble mode的BLEU分数。集成方法对四个不同模型的八个检査点进行了集成。此性能与此任务的最高性能模型(BLEU得分为082)有所不同。用于训练最终模型的句子对数量是2,014630
​		JPC2数据集中文→日文:JPC2zh→Ja子任务需要100万条JPC2数据和672,315条AS-PEC数据进行进一步实验。JPC2Zh→Ja子任务123BLEU得分高于基线得分。我们应用了相对定位,后退翻译、多源和r2|重新排名,分别将BLEU得分提高01、0.63、1.14和1.32。在本任务中,反向翻译方法结合JPC2Ja→zh数据1M句子和JPc2Ko-Ja也构成1M语句生成新的zhja数据。在多源方法的应用过程中,使用了JPC2Ja-En数据集的1M语句。在这个任务中,最后一个模型中使用的句子对的总数是4,672,315。
​		由于集成方法为多源系统的应用创建了一个新的共享字典,因此集成系统被应用到5个独立模型的7个检查点,直到多源系统。
​		JPC2数据集的日文→中文结合100万JPC2数据集和672,315 ASPEC数据集训练模型,对JPC2Ja→zh子任务进行进一步实验。使用这两个数据集的方法比仅使用JPC2的基线高15个蓝色单位
​		我们使用了相对定位、反向平移和r2|重新排序,使BLEU得分分别增加了0.26、0.17和0.95。反向转换使用现有的一百万数据集生成Ja→Zh数据集。

​		该任务的集成方法对基线模型、附加相对位置模型、另一个「2|重新排序模型和transformer大模型的9个检查点进行集成实验。最终模型中训练数据的句子对数为2672315分,而BLEU的分数是43.3分。这一得分与提交给WAT2019的第二名和第三名不同,分别为13分和13分分别为213分

### 3.5.3 Korean-Japanese

​		表10显示了JPC2数据集对韩语和日语的翻译性能,如JPC2Ko-Ja。我们应用了本文提出的方法。K0-Ja翻译任务只有一个 paten子任务JPc2,我们只参与了韩语到日语(Ko→Ja)的任务。最终蓝衣军团的递交成绩为73.04分,在三支参赛队伍中排名第一。

​		JPC2数据集的韩语→日语:同样,我们使用OpenNMT transformer作为JPc2Ko→Ja数据集的基线,其BLEU值为70.90。在表11中,我们将基线添加到相对位置,然后方法进行方法消融直到R2L重新排序。在变压器中加入相对位置后,BLEU性能从0.62提高到71.52。我们使用JPc2的zh-Ja和En-Ja的日本数据集(2M个句子)进行了反向翻译,并使用相对位置模型对变压器进行了测量,除JPC2K0-Ja外,还测量了总计3M个句子的变压器。
​		与其他子任务不同的是,JPC2K0-a的BLEU性能为71.23,比之前的相对位置互感器方法低0.3。对于反向翻译,我们将JPC2Ja→Ko数据集作为 transformer模型进行训练,其BLEU值为68.53。与反向翻译不同的是,在JPC2Ko→Ja数据集中加入JPC2Ena和jz-Ja数据集的多源训练方法,其性能比BLEU评分70.62低0.9。当使用R2L重链接时,BLEU评分为70.34,与使用相对定位时变压器的情况相比,降低了1.18。在此基础上,采用性能最佳的变压器模型和相对变压器模型,实现了基于模型的集成方法定位。当我们将学习率设置为2时,我们将生成8个检查点;当我们将学习率设置为3时,我们将生成3个检查点。通过集成实验,获得了最佳的性能,BLEU评分为73.04。

## 3.6 充分性评价总结

​		WAT2019( Nakazawa et al,2019)展示了顶级系统的评估总结。表12显示了我们所参与的子任务的适当性能。在充分性方面, ASPEC Ja→En的充分性最好,为4.51。 ASPEC Zh→Ja评分4.59分,Ja→zh充分性评价4.36分。JPC2的En→Ja、Ja→En、Zh→Ja、Ja→Zh、Ko→Ja均表现良好,分值分别为450、4.78、465、4.55、4.65。

# 4 结论

​		我们参与了WAT2019 ASPEC和JPC2的翻译任务。我们利用了NMT系统的几种方法。基于OpenNET的 transformer模型进行相对定位,加入数据构建对误差和反向平移具有鲁棒性的模型,并采用多源方法解决解码器中的误差传播问题,这是一种自回归架构,采用集成方法进一步提高了系统的性能。因此,我们在 ASPEC En-a和j-Ja任务中名列前茅,在JPc2En-Ja、jz-Ja和Ko→Ja子任务中排名第一。