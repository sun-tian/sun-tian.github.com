---
layout:     post                    # 使用的布局（不需要改）
title:      神经机器翻译综述的参考文献及摘要               # 标题 
subtitle:   附参考文献百度云资源 #副标题
date:       2019-08-20              # 时间
author:     甜果果                      # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习日记
    - paper
---

## 神经机器翻译综述的参考文献及摘要

##### 原文：[李亚超,熊德意,张民.神经机器翻译综述\[J\].计算机学报,2018,41\(12\):2734-2755.](http://kns.cnki.net//KXReader/Detail?autoLogin=0&TIMESTAMP=637018294707186250&DBCODE=CJFQ&TABLEName=CJFDLAST2019&FileName=JSJX201812007&RESULT=1&SIGN=EYaeHZNroGKGlc2ghTcKRyejyW4%3d)

##### 本文参考文献：链接: https://pan.baidu.com/s/1ObvF6NCue3Lr5VMrHqUsNA 提取码: 8dfj 复制这段内容后打开百度网盘手机App，操作更方便哦
#####  [1] Jiao Li-Cheng, Yang Shu-Yuan, Liu Fang, et al.Seventy years beyond neural networks:Retrospect and prospect.Chinese Journal of Computers, 2016, 39 (8) :1697-1716 (in Chinese) (焦李成, 杨淑媛, 刘芳等.神经网络七十年:回顾与展望.计算机学报, 2016, 39 (8) :1697-1716)

##### [2] Hinton G E, Osindero S, Teh Y-W.A fast learning algorithm for deep belief nets.Neural Computation, 2006, 18:1527-1554
We show how to use “complementary priors” to eliminate the explaining-away effects thatmake inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of thewake-sleep algorithm. After fine-tuning, a networkwith three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to displaywhat the associativememory has in mind.
我们展示了如何使用“互补先验”来消除在具有许多隐藏层的紧密连接的信念网中使推理变得困难的解释-离开效应。 利用互补先验知识，我们得到了一个快速的贪婪算法，该算法可以一次学习一层有向深层信念网络，前提是顶层和顶层形成一个无向关联存储器。 快速贪婪算法被用来初始化一个较慢的学习过程，使用唤醒睡眠算法的对比版本来微调权重。 经过微调，一个具有三个隐藏层的网络形成了一个很好的手写数字图像及其标签联合分布的生成模型。 这种生成模型比最好的判别学习算法提供了更好的数字分类。 数字所在的低维流形是由顶层联想记忆的自由能景观中的长沟壑建模的，通过使用有向连接来显示联想记忆所想到的东西，可以很容易地探索这些沟壑。

##### [3] Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks//Proceedings of the Neural Information Processing Systems (NIPS 2012) .Lake Tahoe, USA, 2012:1097-1105
We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.
我们训练了一个大的、深卷积的神经网络，将ImageNet LSVRC-2010竞赛中的120万幅高分辨率图像分为1000个不同的类别。 在测试数据上，我们分别获得了37.5%和17.0%的Top-1和Top-5错误率，这比先前的技术水平要好得多。 该神经网络具有6000万个参数和65万个神经元，由5个卷积层和3个完全连接的层组成，其中5个卷积层之后是最大池层，最后是1000路最大软连接层。 为了加快训练速度，我们使用了非饱和神经元和一个非常有效的GPU实现卷积运算。 为了减少完全连接层中的过拟合，我们采用了最近开发的称为“辍学”的正则化方法，该方法被证明是非常有效的。 我们还在ILSVRC-2012竞赛中输入了该模型的一个变体，并获得了15.3%的前5名测试错误率，而第二名的测试错误率为26.2%。

##### [4] Hinton G, Deng L, Yu D.Deep neural networks for acoustic modeling in speech recognition:The shared views of four research groups.IEEE Signal Processing Magazine, 2012, 29 (6) :82-97
Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.
目前大多数语音识别系统使用隐马尔可夫模型(HMMS)来处理语音的时间变异性，使用高斯混合模型(GMMS)来确定每个HMM的每个状态与表示声学输入的系数帧或系数帧的短窗口的拟合程度。 评估拟合度的另一种方法是使用前馈神经网络，该前馈神经网络以多个系数帧为输入，并在HMM状态上产生后验概率作为输出。 深度神经网络（Deep Neural Networks，DNNs）具有许多隐藏层，并且使用新的方法进行训练，在各种语音识别基准上表现出优于GMM的性能，有时表现出很大的优势。 本文概述了这一进展，并代表了四个研究小组的共同观点，这四个研究小组最近成功地将DNN用于语音识别中的声学建模。

##### [5] Collobert R, Weston J, Bottou L, et al.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 2011, 12 (1) :2493-2537
We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.
我们提出了一种统一的神经网络结构和学习算法，可以应用于各种自然语言处理任务，包括词性标注、分块、命名实体识别和语义角色标注。 这种多功能性是通过试图避免特定于任务的工程来实现的，因此忽略了大量的先验知识。 我们的系统不是为每个任务仔细优化人工输入特性，而是基于大量的大部分未标记的训练数据来学习内部表示。 这一工作随后被用作构建一个具有良好性能和最小计算要求的免费可用标记系统的基础。

##### [6] Junczys-Dowmunt M, Dwojak T, Hoang H.Is neural machine translation ready for deployment?A case study on30translation directions.arXiv preprint/1610.01108v2, 2016
In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efﬁcient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-persecond ratios.
本文对基于短语的SMT和神经机器翻译在30个翻译方向上的翻译质量进行了最大的比较。 对于十个方向，我们还包括层次短语为基础的多目标决策。 对最近发布的联合国并行语料库V1.0及其大型六向句子对齐子语料库进行了实验。 在论文的第二部分，我们研究了翻译速度的一些方面，介绍了我们的高效神经机器翻译译码器AMUNMT。 我们证明，当前的神经机器翻译在比较词/秒比时已经可以用于生产系统。

##### [7] Sennrich R, Haddow B, Birch A.Edinburgh neural machine translation systems for WMT 16//Proceedings of the 1st Conference on Machine Translation.Berlin, Germany, 2016:371-376
We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a ﬁxed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3–11.2 B LEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated. 12
我们参与了2016年WMT共享新闻翻译任务，为四种语言对构建了神经翻译系统，每种语言对都接受了双向培训:英语和捷克语、英语和德语、英语和罗马尼亚语以及英语和俄语。 我们的系统基于一个注意力编码器-解码器，使用BPE子字分段进行开放式词汇翻译，并提供固定词汇。 我们尝试使用单语新闻语料库的自动回译作为额外的训练数据、普遍辍学和目标双向模型。 所有报告的方法都有很大的改进，我们看到在基线系统上有4.3-11.2B低浓铀的改进。 在人类的评价中，我们参与的8个翻译方向中有7个方向，我们的系统是（绑定的）最佳约束系统。 12

##### [8] Zhou Jie, Cao Ying, Wang Xuguang, et al.Deep recurrent models with fast-forward connections for neural machine translation.Transactions of the Association for Computational Linguistics, 2016, 4:371-383
Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT’14 Englishto-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the ﬁrst time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difﬁcult WMT’14 English-to-German task.
神经机器翻译（Neural Machine Translation，NMT）是利用神经网络解决机器翻译（Machine Translation，MT）问题的一种方法，近年来取得了很好的效果。 然而，现有的大多数NMT模型都比较肤浅，单一的NMT模型与传统的最优MT系统相比仍有一定的性能差距。 本文介绍了一种基于深度长短时记忆(LSTM)网络的新型线性连接，称为快进连接，以及一种交错式双向LSTM层堆叠结构。 快速转发连接在传播梯度和构建深度为16的深度拓扑中起着重要作用。 在WMT'14英法任务中，我们使用单一注意模型获得了BLEU=37.7，比相应的单一浅层注意模型高出6.2个BLEU点。 这是单个NMT模型首次实现最先进的性能，比最佳传统模型高出0.7个BLEU点。 即使不使用注意机制，我们仍然可以实现BLEU=36.3。 经过对未知单词的特殊处理和模型的集成，我们获得了迄今为止在该任务中报告的最好的分数，BLEU=40.4。 我们的模型也在更困难的WMT'14英语对德语任务中得到验证。

##### [9] Wu Yonghui, Schuster M, Chen Zhifeng, et al.Google’s neural machine translation system:Bridging the gap between human and machine translation.arXiv preprint/1609.08144v1, 2016
Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference – sometimes prohibitively so in the case of very large data sets and large models. Several authors have also charged that NMT systems lack robustness, particularly when input sentences contain rare words. These issues have hindered NMT’s use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google’s Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the ﬁnal translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (“wordpieces”) for both input and output. This method provides a good balance between the ﬂexibility of “character”-delimited models and the eﬃciency of “word”-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. To directly optimize the translation BLEU scores, we consider reﬁning the models by using reinforcement learning, but we found that the improvement in the BLEU scores did not reﬂect in the human evaluation. On the WMT’14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google’s phrase-based production system.
神经机器翻译（Neural Machine Translation，NMT）是一种实现自动翻译的端到端学习方法，具有克服传统短语翻译系统许多缺点的潜力。 不幸的是，NMT系统在训练和翻译推理方面都是计算昂贵的——有时在非常大的数据集和大的模型的情况下是令人望而却步的。 几位作者还指责NMT系统缺乏鲁棒性，尤其是当输入句子中包含稀有单词时。 这些问题阻碍了NMT在实际部署和服务中的应用，在实际部署和服务中，准确性和速度都至关重要。 在这项工作中，我们介绍了Google的神经机器翻译系统GNMT，它试图解决其中的许多问题。 我们的模型由一个深度LSTM网络组成，该网络具有8个编码器和8个解码器层，使用剩余连接以及从解码器网络到编码器的注意连接。 为了提高并行性，从而减少训练时间，我们的注意机制将解码器的底层连接到编码器的顶层。 为了加快最终的转换速度，我们在推理计算中使用了低精度算法。 为了改进对稀有单词的处理，我们将单词分成一组有限的公共子单词单元（“单词”），用于输入和输出。 该方法很好地平衡了“字符”分界模型的灵活性和“单词”分界模型的易用性，自然地处理了稀有单词的翻译，最终提高了系统的整体准确率。我们的波束搜索技术采用长度归一化过程并使用覆盖惩罚，这鼓励生成最有可能覆盖源句中所有单词的输出句。 为了直接优化翻译BLEU分数，我们考虑使用强化学习来重新定义模型，但我们发现BLEU分数的提高并没有在人类评估中重新产生影响。 在WMT'14英法和英德基准测试中，GNMT取得了最先进的竞争结果。 通过对一组孤立的简单句子进行人工并排评估，与谷歌基于短语的生产系统相比，它平均减少了60%的翻译错误。

##### [10] Crego J, Kim J, Klein G, et al.SYSTRAN’s pure neural machine translation systems.arXiv preprint/1610.05540v1, 2016
Since the ﬁrst online demonstration of Neural Machine Translation (NMT) by LISA (Bahdanau et al., 2014), NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing rollout of NMT engines to replace their existing technologies. NMT systems have a large number of training conﬁgurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to productionready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efﬁcient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our ﬁrst ﬁndings and we ﬁnally outline further work.
Our ultimate goal is to share our expertise to build competitive production systems for ”generic” translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case speciﬁc engines integrated in real production workﬂows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems.
自从LISA（Bahdanau等人，2014年）首次在线演示神经机器翻译(NMT)以来，NMT的开发最近从实验室转向生产系统，几个实体宣布推出NMT引擎以取代其现有技术就证明了这一点。 NMT系统有大量的培训配置，此类系统的培训过程通常很长，通常为几周，因此试验的作用至关重要，需要共享。 在这项工作中，我们提出了我们的方法，在发布涵盖多种语言（12种语言，32种语言对）的在线演示程序的同时，我们还发布了ProductionReady系统。 我们探讨了不同的实践选择:一个有效的、进化的开源框架； 数据准备； 网络体系结构； 其他已实现的功能； 为生产而调整； 等。我们讨论了评估方法，介绍了我们的第一份文件，并最终概述了进一步的工作。 
我们的最终目标是分享我们的专业知识，为“通用”翻译建立有竞争力的生产系统。 我们的目标是帮助建立一个协作框架，以加快技术的采用，促进进一步的研究工作，并使集成在实际生产工作流程中的用例规范引擎能够交付给业界/由业界采用。 掌握该技术将使我们能够构建适合特定需要的翻译引擎，其性能优于当前最简单/统一的系统。 

##### [11] Neco R P, Forcada M L.Asynchronous translations with recurrent neural nets//Proceedings of the International Conference on Neural Networks.Houston, USA, 1997:2535-2540
In recent years, many researchers have explored the relation between discrete-time recurrent neural networks(DTRNN) and finite-state machines (FSMs) either by showing their computational equivalence or by trainingthem to perform as finite-state recognizers from examples. Most of this work has focussed on the simplest class of deterministic state machines, that is deterministic finite automata and Mealy (or Moore) machines. The class of translations these machines can perform is very limited, mainly because these machines output symbols at the same rate as they input symbols, and therefore, the input and the translation have the same length; one may call these translations synchronous. Real-life translations are more complex: word reorderings, deletions, and insertions are common in natural-language translations; or, in speech-to-phoneme conversion, the number of frames corresponding to each phoneme is different and depends on the particular speaker or word. There are, however, simple deterministic, finite-state machines ( extensions of Mealy machines) that may perform these classes of asynchronous or time- warped translations. A simple DTRNN model with input and output control lines inspired on this class of machines is presented and successfully applied to simple asynchronous translation tasks with interesting results regarding generalization. Training of these nets from input-output pairs is complicated by the fact that the time alignment between the target output sequence and the input sequence is unknown and has to be learned: we propose a new error function to tackle this problem. This approach to the induction of asynchronous translators is discussed in connection with other approaches.
近年来，许多研究人员通过证明离散时间递归神经网络(DTRNN)和有限状态机的计算等价性或通过实例训练它们作为有限状态识别器来研究它们之间的关系。 这方面的工作主要集中在最简单的一类确定性状态机上，即确定性有限自动机和Mealy（或Moore）机。 这些机器能够执行的翻译种类非常有限，主要是因为这些机器以与它们输入符号相同的速率输出符号，因此，输入和翻译具有相同的长度； 可以将这些转换称为同步的。 现实生活中的翻译比较复杂:词的重新排序、删除和插入在自然语言翻译中很常见； 或者，在语音到音素的转换中，对应于每个音素的帧的数目是不同的，并且取决于特定的说话者或单词。 然而，有一些简单的确定性有限状态机（Mealy机的扩展）可以执行这些类异步或时间扭曲的转换。 在这类机器上提出了一种具有输入输出控制线的简单DTRNN模型，并成功地应用于简单的异步翻译任务，得到了有趣的推广结果。 由于目标输出序列和输入序列之间的时间对齐是未知的，需要学习，因此从输入-输出对中训练这些网络变得复杂:我们提出了一个新的误差函数来解决这个问题。 本文结合其它方法讨论了异步翻译器的归纳法。

##### [12] Castano M A, Casacuberta F.A connectionist approach to machine translation//Proceedings of the 5th European Conference on Speech Communication and Technology.Rhodes, Greece, 1997:1-4
Connectionist Models can be considered as an encouraging approach to Example-Based Machine Translation. However, the neural translators developed in the literature are quite complex and require great human effort to classify and prepare training data. This paper presents an effective and more simple text-to-text connectionist translator with which translations from the source to the target language can be directly, automatically and successfully approached. The neural system, which is based on an Elman Simple Recurrent Network, was trained to tackle a simple pseudo-natural Machine Translation task.
连接主义模型是一种令人鼓舞的基于实例的机器翻译方法。 然而，文献中开发的神经翻译器相当复杂，需要大量的人力来分类和准备训练数据。 本文提出了一种有效的、更简单的文本到文本连接主义翻译器，它可以直接、自动和成功地处理源语到目的语的翻译。 基于Elman简单递归网络的神经系统被训练来处理简单的伪自然机器翻译任务。

##### [13] Zhang Jiajun, Zong Chengqing.Deep neural networks in machine translation:An overview.Intelligent Systems IEEE, 2015, 30 (5) :16-25
Due to the powerful capacity of feature learning and representation, deep neural networks (DNNs) have made big breakthroughs in speech recognition and image processing. Following recent success in signal variable processing, researchers want to figure out whether DNNs can achieve similar progress in symbol variable processing, such as natural language processing (NLP). As one of the more challenging NLP tasks, machine translation (MT) has become a testing ground for researchers who want to evaluate various kinds of DNNs.
MT aims to find for the source language sentence the most probable target language sentence that shares the most similar meaning. Essentially, MT is a sequence-to-sequence prediction task. This article gives a comprehensive overview of applications of DNNs in MT from two views: indirect application, which attempts to improve standard MT systems, and direct application, which adopts DNNs to design a purely neural MT model. We can elaborate further:
• Indirect application designs new features with DNNs in the framework of standard MT systems, which consist of multiple submodels (such as translation selection and language models). For example, DNNs can be leveraged to represent the source language context’s semantics and better predict translation candidates. 
• Direct application regards MT as a sequence-to-sequence prediction task and, without using any information from standard MT systems, designs two deep neural networks—an encoder, which learns continuous representations of source language sentences, and a decoder, which generates the target language sentence with source sentence representation.
Let’s start by examining DNNs themselves.
深层神经网络（Deep Neural Networks，DNNs）以其强大的特征学习和表示能力，在语音识别和图像处理领域取得了突破性进展。 随着近年来在信号变量处理方面的成功，研究人员希望弄清楚DNN能否在符号变量处理方面取得类似的进展，例如自然语言处理(NLP)。 机器翻译（Machine Translation，MT）作为一项更具挑战性的自然语言处理任务，已经成为研究者评估各种DNN的一个试验场。 
机器翻译的目的是为源语言句子找到最可能的目标语言句子，具有最相似的意义。 本质上，MT是一个序列到序列的预测任务。 本文从间接应用和直接应用两个方面对DNNS在MT中的应用进行了综述，间接应用是对标准MT系统的改进，直接应用是采用DNNS设计一个纯神经MT模型。 我们可以进一步阐述: 
•间接应用程序在标准MT系统的框架内使用DNN设计新功能，该系统由多个子模型（如翻译选择和语言模型）组成。 例如，DNN可以用来表示源语言上下文的语义并更好地预测翻译候选。 
•直接应用程序将MT视为序列对序列的预测任务，并且在不使用来自标准MT系统的任何信息的情况下，设计了两个深层神经网络——编码器和解码器，编码器学习源语言句子的连续表示，解码器生成具有源句子表示的目标语言句子。 
让我们从检查DNN本身开始。 

##### [14] Kalchbrenner N, Blunsom P.Recurrent continuous translation models//Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP 2013) .Seattle, USA, 2013:1700-1709
We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show ﬁrst that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of state-of-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.
我们引入了一类概率连续翻译模型，称为递归连续翻译模型，它完全基于单词、短语和句子的连续表示，而不依赖于对齐或短语翻译单元。 这些模型有生成和条件方面。 译文的生成用目标递归语言模型建模，而对源句的制约用卷积句模型建模。 通过各种实验，我们首先证明了我们的模型对于Gold翻译的困惑度比最先进的基于对齐的翻译模型低了43%以上。 其次，我们发现，尽管缺乏对齐，但它们对源句的语序、句法和意义都非常敏感。 最后，我们证明，当重新评分N-最佳翻译列表时，它们与最先进的系统相匹配。

##### [15] Sutskever I, Vinyals O, Le Q V.Sequence to sequence learning with neural networks//Proceedings of the Neural Information Processing Systems (NIPS 2014) .Montreal, Canada, 2014:3104-3112
Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difﬁcult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a ﬁxed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difﬁculty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.
深度神经网络（DNNs）是一种功能强大的模型，在不同的学习任务中取得了优异的性能。 尽管DNN无论何时只要有大的标记训练集可用都能很好地工作，但它们不能用于将序列映射到序列。 在本文中，我们提出了一种端到端的序列学习方法，该方法对序列结构作了最小的假设。 我们的方法使用多层长短时存储器(LSTM)将输入序列映射到固定维度的向量，然后使用另一个深度LSTM从向量中解码目标序列。 我们的主要结果是，在WMT-14数据集的英法翻译任务中，LSTM生成的翻译在整个测试集上的BLEU分数为34.8，其中LSTM的BLEU分数是对词汇外单词的惩罚。 此外，LSTM对长句没有区别。 为了比较，基于短语的SMT系统在相同的数据集上实现了33.3的BLEU分数。 当我们使用LSTM对上述SMT系统产生的1000个假设进行重新排序时，其BLEU分数增加到36.5，这接近于先前的技术水平。 LSTM还学习对语序敏感且对主动语态和被动语态相对不变的有意义短语和句子表示。 最后，我们发现在所有源句（而不是目标句）中颠倒词序显著提高了LSTM的性能，因为这样做在源句和目标句之间引入了许多短期依赖性，使得优化问题变得容易。

##### [16] Cho K, van Merrienboer B, Gulcehre C, et al.Learning phrase representations using RNN encoder-Decoder for statistical machine translation.arXiv preprint/1406.1078v2, 2014
In this paper, we propose a novel neural network model called RNN Encoder Decoder that consists of two recurrent neural networks (RNN). One RNN en- codes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN EncoderDecoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.
本文提出了一种由两个递归神经网络(RNN)组成的新的神经网络模型，称为RNN编码器译码器。 一个rnn将符号序列编码为固定长度向量表示，另一个rnn将该表示解码为另一符号序列。 该模型的编码器和解码器被联合训练以在给定源序列的情况下最大化目标序列的条件概率。 实验表明，利用RNN编码器计算的短语对条件概率作为现有对数线性模型的附加特征，统计机器翻译系统的性能得到了改善。 定性地，我们证明了所提出的模型学习语言短语的语义和句法意义的表示。

##### [17] Cho K, van Merrienboer B, Bahdanau D, et al.On the properties of neural machine translation:encoder-decoder approaches//Proceedings of the SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.Doha, Qatar, 2014:103-111
Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a ﬁxed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we ﬁnd that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.
神经机器翻译是一种比较新的基于神经网络的统计机器翻译方法。 神经机器翻译模型通常由编码器和解码器组成。 编码器从可变长度输入语句中提取固定长度表示，解码器从该表示中生成正确的转换。 本文利用两个模型分析了神经机器翻译的性质； RNN编解码器和一种新提出的门限递归卷积神经网络。 实验结果表明，神经机器翻译对无生词短句的翻译效果较好，但随着句子长度和生词数目的增加，其翻译效果迅速下降。 此外，我们发现所提出的门限递归卷积网络能够自动学习句子的语法结构。

##### [18] Jean S, Cho K, Bengio Y.On using very large target vocabulary for neural machine translation//Proceedings of the53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL 2015) .Beijing, China, 2015:1-10
Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efﬁciently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT’14.
神经机器翻译是最近提出的一种基于神经网络的机器翻译方法，与现有的基于短语的统计机器翻译方法相比，神经机器翻译已经取得了很好的效果。 尽管近年来神经机器翻译取得了很大的成功，但由于训练复杂度和解码复杂度与目标词的数量成正比地增加，神经机器翻译在处理更大的词汇量方面仍有其局限性。 在本文中，我们提出了一种基于重要性抽样的方法，该方法允许我们使用非常大的目标词汇量，而不增加训练复杂度。 我们表明，即使在目标词汇量非常大的情况下，通过只选择整个目标词汇量的一小部分，解码也能有效地完成。 实验结果表明，该方法训练的模型与基于LSTM的神经机器翻译模型和词汇量较小的基线模型相匹配，在某些情况下甚至优于后者。 此外，当我们使用几个模型的集合，具有非常大的目标词汇量时，我们在WMT'14的英语→德语和英语→法语翻译任务上获得了与现有技术水平（用BLEU衡量）相当的性能。

##### [19] Jean S, Firat O, Cho K, et al.Montreal neural machine translation systems for WMT’15//Proceedings of the 10th Workshop on Statistical Machine Translation.Lisbon, Portugal, 2015:134-140
Neural machine translation (NMT) systems have recently achieved results comparable to the state of the art on a few translation tasks, including English→French and English→German. The main purpose of the Montreal Institute for Learning Algorithms (MILA) submission to WMT’15 is to evaluate this new approach on a greater variety of language pairs. Furthermore, the human evaluation campaign may help us and the research community to better understand the behaviour of our systems. We use the RNNsearch architecture, which adds an attention mechanism to the encoderdecoder. We also leverage some of the recent developments in NMT, including the use of large vocabularies, unknown word replacement and, to a limited degree, the inclusion of monolingual language models.
神经机器翻译(NMT)系统最近在一些翻译任务上取得了与现有技术相当的结果，包括英语→法语和英语→德语。 蒙特利尔学习算法研究所(MILA)提交给WMT'15的主要目的是评估这种新方法在更多语言对上的多样性。 此外，人类评估活动可能有助于我们和研究界更好地了解我们系统的行为。 我们使用RNNSearch体系结构，它为编码器解码器添加了一个关注机制。 我们还利用了NMT的一些最新发展，包括大量词汇的使用、未知词的替换，以及在一定程度上包括单语语言模型。

##### [20] Bentivogli L, Bisazza A, Cettolo M, et al.Neural versus phrase-based machine translation quality//Proceedings of the2016Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:257-267
Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-ofthe-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural vs. phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models – such as the reordering of verbs – while pointing out other aspects that remain to be improved.
在统计机器翻译(SMT)领域，神经网络方法(NMT)最近成为能够挑战基于短语的方法(PBMT)长期主导地位的第一种技术。 特别是，在2015年IWSLT评估活动中，NMT在英德两种语言中的表现超过了公认的最先进的PBMT系统，由于形态和句法差异，这一语言对尤其困难。 为了了解NMT在哪些方面提供了比PBMT更好的翻译质量，我们对基于神经和短语的SMT输出进行了详细分析，利用了专业翻译人员对IWSLT数据进行的高质量后期编辑。 我们的分析首次提供了关于哪些语言现象最好用神经模型来建模的有用见解，比如动词的重新排序，同时指出了有待改进的其他方面。

##### [21] Zong Cheng-Qing.Statistical Machine Translation.2nd Edition.Beijing:Tsinghua University Press, 2013 (in Chinese) (宗成庆.统计自然语言处理.第2版.北京:清华大学出版社, 2013)

##### [22] Bahdanau D, Cho K, Bengio Y.Neural machine translation by jointly learning to align and translate.arXiv preprint/1409.0473v6, 2014
Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.
神经机器翻译是近年来提出的一种机器翻译方法。 与传统的统计机器翻译不同，神经机器翻译的目标是建立一个可以联合调整的单一神经网络，以使翻译性能达到最大。 最近提出的用于神经机器翻译的模型通常属于编码器-解码器家族，并且将源语句编码为限定长度的向量，解码器从该向量生成翻译。 在本文中，我们猜想使用固定长度向量是提高这种基本编码器-解码器架构性能的瓶颈，并建议通过允许模型自动（软）搜索源语句中与预测目标单词相关的部分，而不必显式地将这些部分形成硬段，来扩展这一点。 通过这种新的翻译方法，我们实现了与现有的基于短语的英法翻译系统相当的翻译性能。 此外，定性分析表明，模型发现的（软）排列与我们的直觉很好地吻合。

##### [23] Tu Zhaopeng, Lu Zhengdong, Liu Yang, et al.Modeling coverage for neural machine translation//Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:76-85
Attention mechanism has enhanced stateof-the-art Neural Machine Translation(NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.1
注意机制通过联合学习对齐和翻译来增强最先进的神经机器翻译(NMT)。 然而，它往往忽略过去的对齐信息，这往往导致过度翻译和翻译不足。 针对这一问题，本文提出了基于覆盖的NMT。 我们维护一个覆盖向量来跟踪关注历史。 复盖向量被反馈到注意模型中，以帮助调整未来的注意，这使得NMT系统能够更多地考虑未翻译的源词。 实验表明，与标准的基于注意的NMT.1方法相比，该方法显著提高了翻译质量和对齐质量

##### [24] Druck G, Ganchev K, Graca J.Rich prior knowledge in learning for NLP//Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011) .Portland, USA, 2011:1-57

##### [25] Tu Zhaopeng, Liu Yang, Shang Lifeng, et al.Neural machine translation with reconstruction//Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI 2017) .San Francisco, USA, 2017:3097-3103
Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-theart NMT and statistical MT systems.
尽管端到端神经机器翻译(NMT)在过去的两年中取得了显著的进展，但它也存在一个主要的缺陷:由NMT系统生成的译文往往不够充分。 人们普遍认为，NMT倾向于重复翻译一些源词，而错误地忽略了其他词。 为了解决这一问题，我们提出了一种新的NMT编解码重构框架。 该重构器结合到NMT模型中，设法从输出目标句的隐藏层重构输入源句，以确保尽可能多地将源端的信息转换到目标端。 实验表明，与现有的NMT和统计MT系统相比，该框架显著提高了NMT输出的充分性，获得了更好的翻译结果。

##### [26] Cheng Yong, Xu Wei, He Zhongjun, et al.Semi-supervised learning for neural machine translation//Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:1965-1974
While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semisupervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourceto-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.
端到端神经机器翻译(NMT)近年来取得了显著的进展，但NMT系统仅依靠并行语料库进行参数估计。 由于平行语料库在数量、质量和覆盖面等方面都受到限制，尤其是对于资源较少的语言，利用单一语言语料库来提高国家语言测试的质量已成为一种趋势。 本文提出了一种半监督的NMT模型训练方法，用于标注（平行语料库）和未标注（单语语料库）数据的级联。 其核心思想是使用一个自动编码器来重建单语语料库，其中源到目标和目标到源翻译模型分别充当编码器和解码器。 该方法不仅可以利用目的语的单语语料库，而且可以利用源语的单语语料库。 在中英文数据集上的实验表明，该方法比现有的SMT和NMT系统有显著的改进。

##### [27] Zhang Biao, Xiong Deyi, Su Jinsong.Variational neural machine translation//Proceedings of the 2016Conference on Empirical Methods in Natural Language Processing (EMNLP2016) .Austin, USA, 2016:521-530
Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and EnglishGerman translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.
神经机器翻译的模型通常来自于一个有区别的编解码器家族，它们学习给定源句的目标句的条件分布。 本文提出了一种学习神经机器翻译条件分布的变分模型:一种可进行端到端训练的变分编码器译码器模型。 与传统的仅从源句的隐藏表示生成目标译文的编码器-解码器模型不同，变分模型引入了一个连续的潜在变量，以显式地建模源句的底层语义，并指导目标译文的生成。 为了进行有效的后验推理和大规模训练，我们构造了一个同时满足源边和目标边条件的神经后验逼近器，并用一种重新参数化技术来估计变分下界。 汉英和英德两种语言的翻译实验表明，本文提出的变分神经机器翻译方法比普通的神经机器翻译方法有了显著的改进。

##### [28] Wang Mingxuan, Lu Zhengdong, Li Hang, et al.Memoryenhanced decoder for neural machine translation//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:278-286
We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memoryenhanced RNN decoder is called MEMDEC. At each time during decoding, MEMDEC will read from this memory and write to this memory once, both with content-based addressing.
Unlike the unbounded memory in previous work(Bahdanau et al., 2014) to store the representation of source sentence, the memory in MEMDEC is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step.
Our empirical study on Chinese-English translation shows that it can improve by 4.8 BLEU upon Groundhog and 5.3 BLEU upon on Moses, yielding the best performance achieved with the same training set.
提出了一种基于外部存储器的神经机器翻译器(NMT)中的RNN译码器，作为对译码RNN中状态的自然而有力的扩展。 这种内存增强的RNN解码器称为MEMDEC。 在解码期间，memdec将每次从该存储器读取并写入该存储器一次，两者都使用基于内容的寻址。 
与先前工作（Bahdanau et al.，2014）中存储源语句表示的无界内存不同，MEMDEC中的内存是一个具有预定大小的矩阵，其设计用于在每个时间步骤更好地捕获对解码过程重要的信息。 
我们对汉英翻译的实证研究表明，在相同的训练条件下，土拨鼠和摩西的翻译能力分别提高了4.8和5.3倍，取得了最好的翻译效果。 

##### [29] Elman J L.Finding structure in time.Cognitive Science, 1990, 14 (2) :179-211

##### [30] Goodfellow I, Bengio Y, Courville A.Deep Learning.Cambridge, USA:MIT Press, 2015

##### [31] Bengio Y, Simard P, Frasconi P.Learning long-term dependencies with gradient descent is difficult.IEEETransactions on Neural Networks, 1994, 5 (2) :157-166
Abstract- Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. 
摘要-递归神经网络可用于将输入序列映射到输出序列，例如用于识别、生产或预测问题。 然而，在训练递归神经网络以执行输入/输出序列中存在的时间偶然性跨越较长间隔的任务方面，已经报道了实际困难。 我们展示了基于梯度的学习算法为什么随着要捕获的依赖项的持续时间的增加而面临日益困难的问题。 这些结果揭示了在通过梯度下降进行有效学习和长时间锁定信息之间的折衷。 基于对这一问题的理解，考虑了标准梯度下降的替代方法。 

##### [32] Hochreiter S, Schmidhuber J.Long short-term memory.Neural Computation, 1997, 9 (8) :1735-1780
Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O . 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.
通过递归反向传播学习在延长的时间间隔上存储信息需要很长的时间，这主要是因为错误回流不足和衰减。 我们简要回顾了Hochreiter(1991)对这一问题的分析，然后通过引入一种新的、有效的、基于梯度的方法称为长短时记忆(LSTM)来解决这一问题。 在不造成损害的情况下截断梯度，LSTM可以通过在特定单元内强制恒定误差流通过恒定误差转盘，学习桥接超过1000个离散时间步长的最小时间滞后。 乘法门单元学习打开和关闭对恒定错误流的访问。 LSTM在空间和时间上都是局部的； 该算法的时间步长和权重的计算复杂度为O。 1.我们对人工数据的实验包括局部、分布式、实值和有噪声的模式表示。 与实时递归学习、时间反向传播、递归级联相关、Elman网络和神经序列分块相比，LSTM可以获得更多的成功运行，并且学习速度更快。 LSTM还可以解决复杂的、人工的、长时间滞后的任务，这些任务以前的递归网络算法从未解决过。

##### [33] Chung J, Gulcehre C, Cho K, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling.arXiv preprint/1412.3555v1, 2014
In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.
本文比较了递归神经网络(RNNS)中不同类型的递归单元。 特别地，我们关注实现选通机制的更复杂的单元，例如长短时记忆(LSTM)单元和最近提出的选通递归单元(GRU)。 在复调音乐建模和语音信号建模的任务中，我们对这些递归单元进行评估。 我们的实验表明，这些先进的递归单位确实比更传统的递归单位，如tanh单位更好。 此外，我们发现GRU与LSTM相当。

##### [34] Socher R, Huang E H, Pennington J.Dynamic pooling and unfolding recursive autoencoders for paraphrase detection//Proceedings of the Neural Information Processing Systems (NIPS 2011) .Granada, Spain, 2011:801-809
Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.
释义检测的任务是检查两个句子并确定它们是否具有相同的意义。 为了获得较高的准确率，需要对这两个语句进行深入的句法和语义分析。 介绍了一种基于递归自动编码器(RAE)的释义检测方法。 我们的无监督RAES基于一个新的展开目标，学习句法树中短语的特征向量。 这些特征被用来衡量两个句子在词和短语方面的相似性。 由于句子可以是任意长度的，因此得到的相似性度量矩阵的大小是可变的。 我们引入了一种新的动态池层，它从可变大小的矩阵中计算出固定大小的表示。 然后，将池表示用作分类器的输入。 在具有挑战性的MSRP释义语料库上，我们的方法优于其他现有的方法。

##### [35] Graves A, Wayne G, Reynolds M, et al.Hybrid computing using a neural network with dynamic external memory.Nature, 2016, 538:471-476

##### [36] Graves A, Wayne G, Danihelka I.Neural turing machines.arXiv preprint/1410.5401v2, 2014
We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.
我们通过将神经网络与外部记忆资源耦合来扩展神经网络的能力，它们可以通过注意过程与外部记忆资源交互。 该组合系统类似于图灵机或冯·诺伊曼结构，但是是可区分的端到端的，允许它以梯度下降的方式被有效地训练。 初步结果表明，神经图灵机可以从输入和输出示例中推断出简单的算法，如复制、排序和联想回忆。

##### [37] Weston J, Chopra S, Bordes A.Memory networks.arXiv preprint/1410.3916v11, 2014
We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.
我们描述了一类新的学习模型，称为记忆网络。 推理成分与长时记忆成分相结合的记忆网络推理 他们学习如何联合使用这些。 长期记忆可以读写，目的是用来预测。 在问答系统中，长时记忆作为一个（动态）知识库，输出是一种语篇反应，我们对这些模型进行了研究。 我们在一个大规模的问答任务，以及一个更小，但更复杂的玩具任务，从一个模拟的世界产生评估他们。 在后者中，我们通过链接多个支持句来回答需要理解动词内涵的问题，从而展示了这种模型的推理能力。

##### [38] Xu K, Ba J, Kiros R, et al.Show, attend and tell:neural image caption generation with visual attention.arXiv preprint/1502.03044, 2015
Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.
受机器翻译和目标检测研究的启发，本文提出了一种基于注意力的图像内容自动学习模型。 我们描述了如何使用标准的反向传播技术以确定性的方式训练该模型，以及如何通过最大化变分下界来随机地训练该模型。 我们还通过可视化展示了该模型如何在输出序列中生成相应单词的同时自动学习将视线固定在显著对象上。 我们在三个基准数据集:Flickr8K、Flickr30K和MS Coco上以最先进的性能验证了注意力的使用。

##### [39] Luong M-T, Pham H, Manning C D.Effective approaches to attention-based neural machine translation//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015) .Lisbon, Portugal, 2015:1412-1421
An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a signiﬁcant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1
近年来，注意机制被用来改进神经机器翻译(NMT)，在翻译过程中有选择地关注源句的某些部分。 然而，对于基于注意力的NMT，很少有人探索有用的体系结构。 本文探讨了两类简单而有效的注意机制:一种是始终关注所有源词的整体方法，另一种是一次只关注源词子集的局部方法。 我们从两个方面论证了这两种翻译方法在英语和德语之间的翻译任务中的有效性。 在本地关注的情况下，与已经集成了辍学等已知技术的非关注系统相比，我们实现了5.0个BLEU点的显著增益。 我们的集成模型使用了不同的注意结构，在WMT'15英德翻译任务中产生了一个新的最先进的结果，得到了25.9个BLEU点，比NMT支持的现有最佳系统和一个N-Gram重排序器提高了1.0个BLEU点。 1

##### [40] Liu L, Utiyama M, Finch A, et al.Neural machine translation with supervised attention//Proceedings of the COLING2016, the 26th International Conference on Computational Linguistics.Osaka, Japan, 2016:3093-3102
The attention mechanisim is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.
注意力机制对神经机器翻译很有吸引力，因为它能够通过在目标词和源词之间产生对齐来动态地编码源句。 不幸的是，它在对准精度方面已经被证明比传统的对准模型差。 本文从重新排序的角度对这一问题进行了分析和解释，提出了一种在常规对齐模型指导下学习的监督注意。 在两个汉英翻译任务中的实验表明，监督注意机制比基于标准注意的NMT有更好的对齐效果。

##### [41] Liu Yang, Sun Maosong.Contrastive unsupervised word alignment with non-local features//Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI 2015) .Austin, USA, 2015:2295-2301
Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top-n alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over stateof-the-art unsupervised word alignment methods.
词对齐是一项重要的自然语言处理任务，它反映了自然语言之间的对应关系。 近年来，对数线性模型的无监督学习结合了产生式学习和判别式学习的优点，受到了广泛的关注。 然而，一个主要的挑战仍然存在:计算对捕获自然语言之间的差异至关重要的非本地特征的期望是很难的。 我们提出了一个对比的方法，旨在区分观察到的训练样本和噪声。 它不仅引入先验知识来指导无监督学习，而且消除了划分函数。 基于对数线性词对齐模型的概率质量通常是高度集中的，我们建议使用Top-N对齐来近似对后验分布的期望。 这允许高效和准确地计算非本地特征的期望。 实验表明，与现有的无监督词对齐方法相比，该方法有显著的改进。

##### [42] Cheng Yong, Shen Shiqi, He Zhongjun, et al.Agreementbased joint training for bidirectional attention-based neural machine translation//Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI 2016) .New York, USA, 2016:2761-2767
The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently, our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on ChineseEnglish and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training.
注意机制在提高端到端神经机器翻译中已被证明是有效的。 然而，由于自然语言之间错综复杂的结构差异，基于单向注意的模型可能只捕捉到注意规律的部分方面。 提出了一种基于协议的双向注意端到端神经机器翻译联合训练方法。 我们的方法不是独立地训练源到目标和目标到源翻译模型，而是鼓励这两个互补的模型在相同的训练数据上就单词对齐矩阵达成一致。 在汉英和英法翻译任务中的实验表明，基于协议的联合训练比独立训练显著提高了对齐和翻译质量。

##### [43] Brown P F, Pietra V J D, Pietra S A D, et al.The mathematics of statistical machine translation:Parameter estimation.Computational Linguistics, 1993, 19 (2) :263-311
We describe a series o,ffive statistical models o,f the translation process and give algorithms,for estimating the parameters o,f these models given a set o,f pairs o,f sentences that are translations o,f one another. We define a concept o,f word-by-word alignment between such pairs o,f sentences. For any given pair of such sentences each o,f our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable o,f these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair o,f sentences. We have a great deal o,f data in French and English from the proceedings o,f the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we,feel that because our algorithms have minimal linguistic content they would work well on other pairs o,f languages. We also ,feel, again because of the minimal linguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.
我们描述了一系列的O，5个统计模型O，F的翻译过程，并给出了估计参数O，F的算法，这些模型给出了一组O，F对O，F句子，它们是相互翻译的O，F。 我们定义了一个概念O，F逐字对齐这样的对O，F句子。 对于任何给定的这样的句子对，每个O，F我们的模型为每个可能的逐字对齐分配一个概率。 我们给出了一个算法来寻找这些对齐中最有可能的o，f。 虽然该算法是次优的，但由此得到的对齐很好地解释了O、F对句子中的逐字关系。 我们从加拿大议会议事录中获得了大量的法文和英文数据。 因此，我们的工作仅限于这两种语文； 但是我们觉得，因为我们的算法有最小的语言内容，他们将很好地工作在其他对O，F语言。 我们还认为，同样由于我们的算法的最小语言内容O，我们有理由认为，逐字对齐是任何足够大的双语语料库所固有的。

##### [44] Feng Shi, Liu Shujie, Li Mu, et al.Implicit distortion and fertility models for attention-based encoder-decoder NMTmodel.arXiv preprint/1601.03317v3, 2016
Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder framework. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoderdecoder and compare them with other models on machine translation. Our proposed method achieved an improvement of 2 BLEU points over the original attentionbased encoder-decoder.
近年来，神经机器翻译取得了很好的效果。 大多数NMT模型遵循编码器-解码器框架。 为了使编解码器模型更加灵活，将注意力机制引入机器翻译以及语音识别和图像字幕等任务中。 我们观察到，当对齐不正确时，基于注意的编码器-解码器的翻译质量会受到显著损害。 我们把这些问题归咎于缺乏扭曲和生育模型。 针对这些问题，本文提出了一种新的基于注意力的编码器译码器，并与其他机器翻译模型进行了比较。 我们提出的方法比原来的基于注意的编解码器提高了2个BLEU点。

##### [45] Cohn T, Hoang C D V, Vymolova E, et al.Incorporating structural alignment biases into an attentional neural translation model.arXiv preprint/1601.01085v1, 2016
Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.
近年来，神经机器翻译取得了很好的效果。 大多数NMT模型遵循编码器-解码器框架。 为了使编解码器模型更加灵活，将注意力机制引入机器翻译以及语音识别和图像字幕等任务中。 我们观察到，当对齐不正确时，基于注意的编码器-解码器的翻译质量会受到显著损害。 我们把这些问题归咎于缺乏扭曲和生育模型。 针对这些问题，本文提出了一种新的基于注意力的编码器译码器，并与其他机器翻译模型进行了比较。 我们提出的方法比原来的基于注意的编解码器提高了2个BLEU点。

##### [46] Zhang Jinchao, Wang Mingxuan, Liu Qun, et al.Incorporating word reordering knowledge into attention-based neural machine translation//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1524-1534
This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality.
本文提出了三种失真模型，将词的重新排序知识显式地融入到基于注意的神经机器翻译(NMT)中，以进一步提高翻译性能。 我们提出的模型使注意机制能够兼顾词义要求和词序惩罚。 汉英翻译实验表明，该方法能显著提高词对齐质量，在基于基本注意的NMT基础上实现显著的翻译改进。 与已有的同类语料库相比，本系统在翻译质量方面取得了较好的效果。

##### [47] Tu Zhaopeng, Liu Yang, Lu Zhengdong, et al.Context gates for neural machine translation.Transactions of the Association for Computational Linguistics, 2017, 5:87-99
In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance both the adequacy and fluency of NMT with more careful control of the information flow from contexts. Experiments show that our approach significantly improves upon a standard attentionbased NMT system by +2.3 BLEU points.
在神经机器翻译(NMT)中，目标词的生成既依赖于源语境，也依赖于目标语境。 我们发现，源语语境直接影响译文的充分性，而目的语语境则影响译文的流利性。 从直觉上讲，实词的生成应该更多地依赖于源语境，虚词的生成应该更多地依赖于目标语境。 由于缺乏对源语和目的语语境影响的有效控制，传统的国家语言测试往往产生流利但不充分的翻译。 为了解决这个问题，我们提出了上下文门，它动态地控制源上下文和目标上下文对目标词生成的贡献比。 这样，我们可以通过更仔细地控制来自上下文的信息流来提高NMT的充分性和流畅性。 实验表明，该方法比标准的基于注意的NMT系统提高了+2.3bleu点。

##### [48] Kim Y, Jernite Y, Sontag D, et al.Character-aware neural language models.arXiv preprint/1508.06615v4, 2015
We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.
我们描述了一个仅依赖于字符级输入的简单神经语言模型。 预测仍然是在文字一级作出的。 我们的模型采用卷积神经网络(CNN)和字符上的公路网，其输出被提供给长短时记忆(LSTM)递归神经网络语言模型（RNN-LM）。 在英国宾夕法尼亚州的树丛中，该模型与现有的最先进的模型不相上下，尽管参数少了60%。 对于形态丰富的语言（阿拉伯文、捷克文、法文、德文、西班牙文、俄文），该模型的性能优于词级/语素级LSTM基线，参数也较少。 结果表明，在许多语言中，字符输入对于语言建模是足够的。 对从模型的字符组成部分获得的单词表示的分析表明，该模型能够仅从字符编码语义和正字法信息。

##### [49] Sennrich R, Haddow B, Birch A.Neural machine translation of rare words with subword units.arXiv preprint/1508.07909v3, 2015
Neural machine translation (NMT) models typically operate with a ﬁxed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 B LEU, respectively.
神经机器翻译(NMT)模型通常使用固定的词汇，但翻译是一个开放的词汇问题。 先前的工作是通过倒退到字典中来解决词汇量不足的单词的翻译问题。 本文介绍了一种更简单、更有效的方法，使NMT模型能够通过将稀有和未知词编码为子词单元序列来进行开放词汇翻译。 这是基于这样一种直觉，即各种词类可以通过比词更小的单位来翻译，例如名称（通过字符复制或音译）、复合词（通过构词翻译）以及同源词和外来词（通过语音和形态转换）。 我们讨论了不同的分词技术，包括简单字符NGRAM模型和基于字节对编码压缩算法的分词技术的适用性，并通过实验表明，对于WMT15翻译任务，子词模型比退避词典基线分别提高了1.1和1.3bLEU。

##### [50] Chung J, Cho K, Bengio Y.A character-level decoder without explicit segmentation for neural machine translation//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:1693-1703
The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder–decoder with a subword-level encoder and a character-level decoder on four language pairs–En-Cs, En-De, En-Ru and En-Fi– using the parallel corpora from WMT’15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.
现有的机器翻译系统，无论是基于短语的还是基于神经网络的，几乎完全依赖于具有显式分词的词级建模。 在本文中，我们提出了一个基本问题:神经机器翻译能够在不进行任何显式分割的情况下生成字符序列吗？ 为了回答这个问题，我们使用WMT'15中的并行语料库，在四个语言对en-CS、en-DE、en-RU和en-FI上评估了一个具有子字级编码器和字符级解码器的基于注意的编码器-解码器。 实验表明，在四种语言对上，采用字符级译码器的模型都优于采用子字级译码器的模型。 此外，具有字符级解码器的神经模型集成在EN-CS、EN-DE和EN-FI上的性能优于现有的非神经机器翻译系统，在EN-RU上的性能相当。

##### [51] Costa-Jussa M R, Fonollosa J A R.Character-based neural machine translation.arXiv preprint/1603.00810v2, 2016
We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.
我们提出了一个神经机器翻译模型，它把输入和输出的句子看成是字符序列而不是单词。 由于词级信息提供了一个重要的偏倚源，我们的输入模型将字符序列的表示组成词的表示（由空格边界决定），然后使用联合注意/翻译模型对这些表示进行翻译。 在目标语言中，翻译被建模为单词向量序列，但是每个单词一次生成一个字符，条件是每个单词中的前几个字符生成。 由于单词的表示和生成是在字符级执行的，因此我们的模型能够解释和生成看不见的单词形式。 这种方法的第二个好处是，它减轻了与源语言和目标语言的预处理/标记化相关的许多挑战。 实验结果表明，该模型能够获得与传统的基于词的模型相当的翻译效果。

##### [52] Su Jinsong, Tan Zhixing, Xiong Deyi, et al.Latticebased recurrent neural network encoders for neural machine translation//Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI 2017) .San Francisco, USA, 2017:3302-3308
Neural machine translation (NMT) heavily relies on wordlevel modelling to learn semantic representations of input sentences. However, for languages without natural word delimiters (e.g., Chinese) where input sentences have to be tokenized first, conventional NMT is confronted with two issues: 1) it is difficult to find an optimal tokenization granularity for source sentence modelling, and 2) errors in 1-best tokenizations may propagate to the encoder of NMT. To handle these issues, we propose word-lattice based Recurrent Neural Network (RNN) encoders for NMT, which generalize the standard RNN to word lattice topology. The proposed encoders take as input a word lattice that compactly encodes multiple tokenizations, and learn to generate new hidden states from arbitrarily many inputs and hidden states in preceding time steps. As such, the word-lattice based encoders not only alleviate the negative impact of tokenization errors but also are more expressive and flexible to embed input sentences. Experiment results on Chinese-English translation demonstrate the superiorities of the proposed encoders over the conventional encoder.
神经机器翻译（Neural Machine Translation，NMT）在很大程度上依赖于词级建模来学习输入句子的语义表示。 然而，对于没有自然词分隔符的语言（如汉语），输入句子首先需要标记，传统的NMT面临两个问题:1）难以找到源句子建模的最佳标记粒度；2）1-最佳标记中的错误可能传播到NMT的编码器。 为了解决这些问题，我们提出了一种基于字格的递归神经网络(RNN)编码器，它将标准RNN推广到字格拓扑。 该编码器以一个字格作为输入，该字格紧凑地编码多个令牌化，并学习从任意多个输入和在前时间步中的隐藏状态生成新的隐藏状态。 因此，基于词格的编码器不仅减轻了标记错误的负面影响，而且更具有表现力和嵌入输入句子的灵活性。 汉英翻译实验结果表明，该编码器优于传统编码器。

##### [53] Yang Zhen, Chen Wei, Wang Feng, et al.A characteraware encoder for neural machine translation//Proceedings of the COLING 2016, the 26th International Conference on Computational Linguistics.Osaka, Japan, 2016:3063-3070
This article proposes a novel character-aware neural machine translation (NMT) model that views the input sequences as sequences of characters rather than words. On the use of row convolution (Amodei et al., 2015), the encoder of the proposed model composes word-level information from the input sequences of characters automatically. Since our model doesn’t rely on the boundaries between each word (as the whitespace boundaries in English), it is also applied to languages without explicit word segmentations (like Chinese). Experimental results on Chinese-English translation tasks show that the proposed character-aware NMT model can achieve comparable translation performance with the traditional word based NMT models. Despite the target side is still word based, the proposed model is able to generate much less unknown words.
本文提出了一种新的特征感知神经机器翻译(NMT)模型，该模型将输入序列看成是字符序列而不是单词序列。 在使用行卷积（Amodei等人，2015）时，所提出的模型的编码器从字符的输入序列自动地合成字级信息。 由于我们的模型不依赖于每个单词之间的边界（如英语中的空格边界），它也适用于没有显式分词的语言（如汉语）。 在汉英翻译任务中的实验结果表明，本文提出的基于字符感知的NMT模型能够获得与传统的基于单词的NMT模型相当的翻译性能。 尽管目标端仍然是基于单词的，但所提出的模型能够生成更少的未知单词。

##### [54] Ling W, Trancoso I, Dyer C, et al.Character-based neural machine translation.arXiv preprint/1511.04586v1, 2015
We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.
我们提出了一个神经机器翻译模型，它把输入和输出的句子看成是字符序列而不是单词。 由于词级信息提供了一个重要的偏倚源，我们的输入模型将字符序列的表示组成词的表示（由空格边界决定），然后使用联合注意/翻译模型对这些表示进行翻译。 在目标语言中，翻译被建模为单词向量序列，但是每个单词一次生成一个字符，条件是每个单词中的前几个字符生成。 由于单词的表示和生成是在字符级执行的，因此我们的模型能够解释和生成看不见的单词形式。 这种方法的第二个好处是，它减轻了与源语言和目标语言的预处理/标记化相关的许多挑战。 实验结果表明，该模型能够获得与传统的基于词的模型相当的翻译效果。

##### [55] Lee J, Cho K, Hofmann T.Fully character-level neural machine translation without explicit segmentation.arXiv preprint/1610.03017v1, 2016
Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subwordlevel encoder on WMT’15 DE-EN and CSEN, and gives comparable performance on FIEN and RU-EN. We then demonstrate that it is possible to share a single characterlevel encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.
现有的机器翻译系统大多是在词的层次上运行的，依赖于显式的分词来提取令牌。 介绍了一种神经机器翻译(NMT)模型，该模型将源字符序列映射到目标字符序列而不进行任何分割。 我们使用字符级卷积网络，在编码器处使用最大池来缩短源表示的长度，从而在捕获局部规律性的同时，能够以与子字级模型相当的速度对模型进行训练。 我们的字符对字符模型在WMT'15DE-EN和CSEN上的性能优于最近提出的使用子字级编码器的基线，并且在FIEN和RU-EN上的性能相当。 然后，我们证明了通过训练一个多对一翻译任务的模型，跨多个语言共享单个字符级编码器是可能的。 在这种多语言设置中，字符级编码器在所有语言对上的性能显著优于子字级编码器。 我们观察到，在CS-EN、FI-EN和RU-EN三种语言中，多语言文字级翻译的质量甚至超过了专门针对该语言对训练的模型，无论是在BLEU评分还是在人类判断方面都是如此。

##### [56] Luong M-T, Le Q V, Sutskever I, et al.Multi-task sequence to sequence learning.arXiv preprint/1511.06114v4, 2015
Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting – where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting – useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting – where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.
序列到序列学习是近年来出现的一种新的监督学习模式。 到目前为止，它的大多数应用程序只集中于一个任务，而没有多少工作为多个任务探索这个框架。 本文研究了序列到序列模型的三种多任务学习(MTL)设置:(a)一对多设置——编码器在机器翻译和句法分析等多个任务之间共享；(b)多对一设置——当只有解码器可以共享时有用，例如在翻译和图像字幕生成的情况下；以及(c)多对多设置——多个编码器和解码器共享，即多个编码器和解码器无监督的目标和翻译。 实验结果表明，在WMT标准上，少量解析和图像字幕数据的训练可以使英德翻译质量提高1.5个BLEU点。 此外，我们已经建立了一个新的最先进的结果，即使用93.0F1进行成分解析。 最后，我们揭示了两个无监督学习目标:自动编码器和跳过思想在MTL上下文中的有趣特性:与跳过思想相比，自动编码器在困惑方面的帮助较小，但在BLEU分数方面的帮助较大。

##### [57] Dong Daxiang, Wu Hua, He Wei, et al.Multi-task learning for multiple language translation//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL 2015) .Beijing, China, 2015:1723-1732
In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.
本文研究了一种能够同时将句子从一种源语言翻译成多种目标语言的机器翻译模型的学习问题。 我们的解决方案受到了最近提出的神经机器翻译模型的启发，该模型将机器翻译概括为一个序列学习问题。 我们将神经机器翻译扩展到一个多任务学习框架，该框架共享源语言表示，并分离不同目标语言翻译的建模。 我们的框架可以应用于大量并行数据或有限并行数据可用的情况。 实验表明，在两种情况下，我们的多任务学习模型都能在公开数据集上获得比单独学习模型更高的翻译质量。

##### [58] Zoph B, Knight K.Multi-source neural translation.arXiv preprint/1601.00710, 2016
We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.
我们建立了一个多源机器翻译模型，并对其进行训练，以使给定法语和德语源的目标英语字符串的概率达到最大。 使用神经编码器-解码器框架，我们探索了几种组合方法，并在一个非常强的基于注意的神经翻译模型上报告了高达+4.8bleu的增加。

##### [59] Firat O, Cho K, Bengio Y.Multi-way, multilingual neural machine translation with a shared attention mechanism//Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016) .San Diego, USA, 2016:866-875
We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT’15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.
我们提出了多途径、多语言的神经机器翻译。 该方法使得单个神经翻译模型能够在多个语言之间进行翻译，并且具有仅随语言数量线性增长的多个参数。 通过在所有语言对之间共享一个注意机制，这是可能的。 我们同时在WMT'15的10个语言对上训练了所提出的多路、多语言模型，并观察到与仅在一个语言对上训练的模型相比，性能有明显的提高。 特别是，我们发现该模型显著提高了低资源语言对的翻译质量。

##### [60] Johnson M, Schuster M, Le Q V, et al.Google’s multilingual neural machine translation system:Enabling zero-shot translation.Transactions of the Association for Computational Linguistics, 2017, 5:339-351
We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes an encoder, decoder and attention module, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. On the WMT’14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-the-art results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT’14 and WMT’15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.
我们提出了一个简单的解决方案，使用单一的神经机器翻译(NMT)模型在多种语言之间进行翻译。 我们的解决方案不需要对标准NMT系统的模型体系结构进行更改，而是在输入语句的开头引入一个人工标记来指定所需的目标语言。 模型的其余部分，包括编码器、解码器和注意模块，保持不变，并在所有语言中共享。 使用共享的词条词汇表，我们的方法能够使用单个模型实现多语言NMT，而不增加任何参数，这比以前的多语言NMT提案要简单得多。 在WMT'14基准测试中，一个单一的多语言模型在英语→法语的测试中取得了可比的性能，并超过了英语→德语的最新测试结果。 同样，在WMT'14和WMT'15基准上，一个单一的多语种模型超过了法语→英语和德语→英语的最新结果。 在生产语料库中，多语种多语种多语种多语种多语种多语种多语种多语种多语种多语种多语种多语种多语种多语种多语种多 除了提高模型训练语言对的翻译质量外，我们的模型还可以学习在训练过程中从未见过的语言对之间执行隐式桥接，这表明迁移学习和零镜头翻译对于神经翻译是可能的。 最后，我们给出了模型中一个通用的中间语言表示的分析，并给出了混合语言时一些有趣的例子。

##### [61] Gulcehre C, Ahn S, Nallapati R, et al.Pointing the unknown words//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:140-149
The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including both the traditional countbased and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each time-step, the decision of which softmax layer to use choose adaptively made by an MLP which is conditioned on the context. We motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. We observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset using our proposed model.
稀有词和未知词问题是影响许多自然语言处理系统性能的一个重要问题，包括传统的基于计数的自然语言处理系统和深度学习系统。 本文提出了一种新的基于注意力的神经网络模型中稀有词和隐形词的处理方法。 我们的模型使用两个SoftMax层来预测条件语言模型中的下一个单词:一个预测单词在源句中的位置，另一个预测单词在入围词汇表中的位置。 在每个时间步骤，由MLP根据上下文自适应地选择要使用哪个SoftMax层。 我们工作的动机来自一个心理学证据，即当一个物体的名字不为人所知时，人类自然倾向于指向上下文或环境中的物体。 我们观察到两个任务的改进，一个是Europarl英法平行语料库的神经机器翻译，另一个是千兆字数据集的文本摘要。

##### [62] Luong M-T, Sutskever I, Le Q V, et al.Addressing the rare word problem in neural machine translation//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL 2015) .Beijing, China, 2015:11-19
Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT’14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT’14 contest task.
神经机器翻译（Neural Machine Translation，NMT）是一种新的机器翻译方法，其结果与传统的机器翻译方法相当。 传统的NMT系统的一个显着缺点是不能正确翻译非常罕见的单词:端到端NMT的词汇量相对较少，只有一个UNK符号来表示每一个可能的词汇量不足(OOV)的单词。 在本文中，我们提出并实现了一种有效的技术来解决这个问题。 我们对一个NMT系统进行数据训练，该系统通过一个词对齐算法的输出来增强，从而允许NMT系统针对目标句子中的每个OOV词发出其对应词在源句子中的位置。 该信息随后在使用字典翻译每个OOV单词的后处理步骤中使用。 我们在WMT'14英法翻译任务上的实验表明，与不使用该技术的等效NMT系统相比，该方法提供了高达2.8个BLEU点的显著改进。 我们的NMT系统有37.5个BLEU分，是第一个超过WMT'14竞赛任务最佳成绩的系统。

##### [63] Li Xiaoqing, Zhang Jiajun, Zong Chengqing.Towards zero unknown word in neural machine translation//Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI 2016) .New York, USA, 2016:2852-2858
Neural Machine translation has shown promising results in recent years. In order to control the computational complexity, NMT has to employ a small vocabulary, and massive rare words outside the vocabulary are all replaced with a single unk symbol. Besides the inability to translate rare words, this kind of simple approach leads to much increased ambiguity of the sentences since meaningless unks break the structure of sentences, and thus hurts the translation and reordering of the in-vocabulary words. To tackle this problem, we propose a novel substitution-translation-restoration method. In substitution step, the rare words in a testing sentence are replaced with similar in-vocabulary words based on a similarity model learnt from monolingual data. In translation and restoration steps, the sentence will be translated with a model trained on new bilingual data with rare words replaced, and finally the translations of the replaced words will be substituted by that of original ones. Experiments on Chinese-to-English translation demonstrate that our proposed method can achieve more than 4 BLEU points over the attention-based NMT. When compared to the recently proposed method handling rare words in NMT, our method can also obtain an improvement by nearly 3 BLEU points.
近年来，神经机器翻译取得了令人鼓舞的成果。 为了控制计算复杂度，NMT必须使用一个小词汇表，词汇表之外的大量稀有词汇都被一个UNK符号替换。 这种简单的方法除了不能翻译稀有词汇外，还会导致句子的歧义性大大增加，因为无意义的unks破坏了句子的结构，从而损害了词汇中词汇的翻译和重新排序。 针对这一问题，我们提出了一种新的替换-平移-恢复方法。 在替换步骤中，基于从单语数据学习的相似性模型，将测试句中的稀有词替换为词汇中的相似词。 在翻译和恢复步骤中，用一个新的双语数据训练模型对句子进行翻译，将稀有词替换，最后将替换词的翻译替换为原始词的翻译。 汉英翻译实验表明，本文提出的方法比基于注意力的NMT能获得4个以上的BLEU点。 与新近提出的处理NMT中稀有词的方法相比，我们的方法还可以获得近3个BLEU点的改进。

##### [64] Hirschmann F, Nam J, Furnkranz J.What makes word-level neural machine translation hard:A case study on EnglishGerman translation//Proceedings of the COLING 2016, the26th International Conference on Computational Linguistics.Osaka, Japan, 2016:3199-3208
Traditional machine translation systems often require heavy feature engineering and the combination of multiple techniques for solving different subproblems. In recent years, several endto-end learning architectures based on recurrent neural networks have been proposed. Unlike traditional systems, Neural Machine Translation (NMT) systems learn the parameters of the model and require only minimal preprocessing. Memory and time constraints allow to take only a fixed number of words into account, which leads to the out-of-vocabulary (OOV) problem. In this work, we analyze why the OOV problem arises and why it is considered a serious problem in German. We study the effectiveness of compound word splitters for alleviating the OOV problem, resulting in a 2.5+ BLEU points improvement over a baseline on the WMT’14 German-to-English translation task. For English-to-German translation, we use target-side compound splitting through a special syntax during training that allows the model to merge compound words and gain 0.2 BLEU points.
传统的机器翻译系统往往需要大量的特征工程和多种技术的结合来解决不同的子问题。 近年来，提出了几种基于递归神经网络的端到端学习结构。 与传统系统不同，神经机器翻译(NMT)系统学习模型的参数，只需最小的预处理。 内存和时间限制只允许考虑固定的字数，这导致了词汇量不足(OOV)问题。 在本文中，我们分析了OOV问题产生的原因，以及为什么它在德语中被认为是一个严重的问题。 我们研究了复合分词器在缓解OOV问题方面的有效性，在WMT'14德语到英语的翻译任务中，复合分词器比基线提高了2.5+BLEU点。 对于英德翻译，我们在训练过程中使用了目标端复合句法拆分的方法，使得模型能够合并复合词并获得0.2个BLEU点。

##### [65] Mi Haitao, Wang Zhiguo, Ittycheriah A.Vocabulary manipulation for neural machine translation//Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:124-129
In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentencelevel or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a wordto-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-toFrench task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).
为了捕捉丰富的语言现象，神经机器翻译模型必须使用大量的词汇量，这就需要较高的计算时间和较大的内存使用量。 在本文中，我们通过引入句子级或批处理级词汇来缓解这一问题，这只是整个输出词汇的一个非常小的子集。 对于每个句子或批次，我们只预测其句子级或批次级词汇中的目标词。 因此，我们减少了计算时间和内存使用。 该方法简单地考虑了源句中每个单词或短语的翻译选项，并基于单词到单词的翻译模型或从传统机器翻译模型中学习的双语短语库为每个句子选择非常小的目标词汇。 实验结果表明，与Jean等人的大词汇量神经机器翻译系统相比，该方法的翻译性能提高了1个bleu点。 （2015年）。

##### [66] Luong M-T, Manning C D.Achieving open vocabulary neural machine translation with hybrid word-character models//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:1054-1063
Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel wordcharacter solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT’15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1−11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.
几乎所有关于神经机器翻译(NMT)的前期工作都使用了非常有限的词汇，可能还有一种后续的方法来修补未知的单词。 本文提出了一种新的实现开放式词汇NMT的WordCharacter解决方案。 我们构建了混合系统，主要在单词级翻译，并参考稀有单词的字符成分。 我们的字符级递归神经网络计算源词表示，并在需要时恢复未知的目标词。 这种混合方法的双重优点是，它比基于字符的方法训练得快得多，也容易得多； 同时，它从不像基于单词的模型那样生成未知单词。 在WMT'15英语到捷克的翻译任务中，这种混合方法比已经处理未知单词的模型增加了+2.1-11.4个BLEU点。 我们最好的系统达到了一个新的最先进的结果，20.7 BLEU分数。 我们证明，我们的字符模型不仅可以成功地为捷克语生成结构良好的单词，而且可以为英语源词建立正确的表示。捷克语是一种高度屈折变化的语言，词汇非常复杂。

##### [67] Chitnis R, DeNero J.Variable-length word encodings for neural translation models//Proceedings of the 2015Conference on Empirical Methods in Natural Language Processing (EMNLP 2015) .Lisbon, Portugal, 2015:2088-2093
Recent work in neural machine translation has shown promising performance, but the most effective architectures do not scale naturally to large vocabulary sizes. We propose and compare three variable-length encoding schemes that represent a large vocabulary corpus using a much smaller vocabulary with no loss in information. Common words are unaffected by our encoding, but rare words are encoded using a sequence of two pseudo-words. Our method is simple and effective: it requires no complete dictionaries, learning procedures, increased training time, changes to the model, or new parameters. Compared to a baseline that replaces all rare words with an unknown word symbol, our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU.
最近在神经机器翻译方面的工作显示了很好的性能，但是最有效的体系结构并不能自然地扩展到大的词汇量。 我们提出并比较了三种变长编码方案，它们使用更小的词汇量来表示大词汇量的语料库，并且没有信息损失。 普通单词不受我们编码的影响，但稀有单词使用两个伪单词的序列进行编码。 我们的方法简单有效:它不需要完整的字典、学习过程、增加的训练时间、对模型的更改或新的参数。 与用未知单词符号替换所有稀有单词的基线相比，我们的最佳可变长度编码策略将WMT英法翻译性能提高了1.7bleu。

##### [68] Pouget-Abadie J, Bahdanau D, van Merrienboer B, et al.Overcoming the curse of sentence length for neural machine translation using automatic segmentation//Proceedings of the SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8) .Doha, Qatar, 2014:78-85
The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.
（Cho等人，2014a)的作者已经表明，与现有的基于短语的翻译系统不同，最近引入的神经网络翻译系统在翻译长句子时的翻译质量显著下降。 本文提出了一种将输入句子自动分割成易于神经网络翻译模型翻译的短语的方法来解决这一问题。 一旦神经机器翻译模型独立地翻译了每个片段，翻译的子句就被连接起来形成最终的翻译。 实证结果表明，长句的翻译质量有了显著的提高。

##### [69] He Wei, He Zhongjun, Wu Hua, et al.Improved neural machine translation with SMT features//Proceedings of the30th AAAI Conference on Artificial Intelligence (AAAI2016) .Phoenix, USA, 2016:151-157
Neural machine translation (NMT) conducts end-to-end translation with a source language encoder and a target language decoder, making promising translation performance. However, as a newly emerged approach, the method has some limitations. An NMT system usually has to apply a vocabulary of certain size to avoid the time-consuming training and decoding, thus it causes a serious out-of-vocabulary problem. Furthermore, the decoder lacks a mechanism to guarantee all the source words to be translated and usually favors short translations, resulting in fluent but inadequate translations. In order to solve the above problems, we incorporate statistical machine translation (SMT) features, such as a translation model and an n-gram language model, with the NMT model under the log-linear framework. Our experiments show that the proposed method significantly improves the translation quality of the state-of-the-art NMT system on Chinese-toEnglish translation tasks. Our method produces a gain of up to 2.33 BLEU score on NIST open test sets.
神经机器翻译(NMT)利用源语言编码器和目标语言解码器进行端到端翻译，具有良好的翻译性能。 然而，作为一种新兴的方法，该方法存在一定的局限性。 NMT系统通常需要使用一定数量的词汇来避免耗时的训练和解码，从而造成严重的词汇量不足问题。 此外，解码器缺乏保证所有源词被翻译的机制，并且通常偏爱短翻译，从而导致流畅但不充分的翻译。 为了解决上述问题，我们在对数线性框架下，将统计机器翻译(SMT)特征（如翻译模型和N-gram语言模型）与NMT模型相结合。 实验结果表明，该方法显著提高了目前最先进的NMT系统对汉英翻译任务的翻译质量。 我们的方法在NIST开放测试集上产生高达2.33BLEU分数的增益。

##### [70] Wang Xing, Lu Zhengdong, Tu Zhaopeng, et al.Neural machine translation advised by statistical machine translation//Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI 2017) .San Francisco, USA, 2017:3330-3336
Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016b; Tu et al. 2016a; He et al. 2016; Tu et al. 2017). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of-the-art NMT and SMT systems on multiple NIST test sets.
神经机器翻译（Neural Machine Translation，NMT）是近年来发展起来的一种新的机器翻译方法。 然而，最近的研究表明，NMT通常产生流利但不充分的翻译（Tu等人，2016b；Tu等人，2016a；He等人，2016；Tu等人，2017）。 这与传统的统计机器翻译(SMT)形成对比，后者通常产生足够但不流利的翻译。 因此，利用这两种模型的优点进行更好的翻译是很自然的，在这项工作中，我们建议将SMT模型合并到NMT框架中。 更具体地说，在每个解码步骤，SMT基于来自NMT的解码信息（例如，生成的部分翻译和注意历史）提供生成字的附加推荐。 然后，我们使用辅助分类器对SMT推荐进行评分，并使用选通函数将SMT推荐与NMT生成相结合，两者都以端到端的方式在NMT体系结构中联合训练。 汉英翻译实验结果表明，该方法在多个NIST测试集上比现有的NMT和SMT系统取得了显著和一致的改进。

##### [71] Zhou Long, Hu Wenpeng, Zhang Jiajun, et al.Neural system combination for machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:378-384
Neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chineseto-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.
神经机器翻译（Neural Machine Translation，NMT）是一种新的机器翻译方法，与统计机器翻译（Statistical Machine Translation，SMT）相比，它能产生更流畅的翻译结果。 然而，SMT在翻译充分性方面通常优于NMT。 因此，将NMT和SMT的优点结合起来是一个很有前途的方向。 本文提出了一种基于多源NMT的神经系统组合框架，该框架以NMT和SMT系统的输出作为输入，产生最终的转换结果。 大量的汉英翻译实验表明，我们的模型比最好的单个系统输出提高了5.3个BLEU点，比最先进的传统系统组合方法提高了3.4个BLEU点。

##### [72] Stahlberg F, de Gispert A, Hasler E, et al.Neural machine translation by minimising the Bayes-risk with respect to syntactic translation lattices//Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017) .Valencia, Spain, 2017:362-368
We present a novel scheme to combine neural machine translation (NMT) with traditional statistical machine translation (SMT). Our approach borrows ideas from linearised lattice minimum Bayes-risk decoding for SMT. The NMT score is combined with the Bayes-risk of the translation according the SMT lattice. This makes our approach much more flexible than n-best list or lattice rescoring as the neural decoder is not restricted to the SMT search space. We show an efficient and simple way to integrate risk estimation into the NMT decoder which is suitable for word-level as well as subword-unit-level NMT. We test our method on EnglishGerman and Japanese-English and report significant gains over lattice rescoring on several data sets for both single and ensembled NMT. The MBR decoder produces entirely new hypotheses far beyond simply rescoring the SMT search space or fixing UNKs in the NMT output.
提出了一种将神经机器翻译(NMT)与传统的统计机器翻译(SMT)相结合的新方案。 我们的方法借鉴了线性格子最小贝叶斯风险译码的思想。 根据SMT格将NMT得分与转换的Bayes风险相结合。 这使得我们的方法比N-最佳列表或格子重评分灵活得多，因为神经解码器不限于SMT搜索空间。 我们给出了一种将风险估计集成到适合于字级和子字单元级NMT的NMT解码器中的简单有效的方法。 我们在英语、德语和日语-英语上测试了我们的方法，并报告了在单个和集成NMT的几个数据集上，与点阵得分相比，我们的方法有了显著的提高。 MBR解码器产生了全新的假设，远远超出了简单地重新评分SMT搜索空间或在NMT输出中固定UNK的范围。

##### [73] Tang Yaohua, Meng Fandong, Lu Zhengdong, et al.Neural machine translation with external phrase memory.arXiv preprint/1606.01792v1, 2016
In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU improvement over the generic neural machine translator.
本文提出了一种具有短语记忆功能的神经机器翻译器PhraseNet，它以符号形式存储从语料库中挖掘出来的短语对或由人类专家指定的短语对。 对于任何给定的源句，短语网扫描短语存储器以确定候选短语对，并相应地将标记信息集成到源句的表示中。 该解码器利用单词生成组件和短语生成组件的混合，使用特定设计的策略来一次生成多个单词的序列。 该短语网不仅为将外部知识融入到神经机器翻译中迈出了一步，而且还努力扩展了递归神经网络的逐字生成机制。 我们对汉英翻译的实证研究表明，经过精心选择的短语表，短语网比一般的神经机器翻译器提高了3.45bleu。

##### [74] Feng Yang, Zhang Shiyue, Zhang Andi, et al.Memoryaugmented neural machine translation//Proceedings of the2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1401-1410
Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memoryaugmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by 9.0 and 2.7 BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods
近年来，神经机器翻译(NMT)取得了显著的成功，但人们也普遍认识到，这种方法在处理非频繁词和词对时存在一定的局限性。 本文提出了一种新的记忆增强NMT（MemoryAugmented NMT，M-NMT）结构，该结构将关于单词（通常很少遇到的单词）应该如何翻译的知识存储在存储器中，然后利用它们来辅助神经模型。 利用该存储机制，将传统统计机器翻译系统的知识与NMT系统的规则相结合，提出了一种基于该框架的词汇表外(OOV)词解决方案。 我们在两个汉英翻译任务上的实验表明，M-NMT架构在这两个任务上分别比NMT基线高9.0和2.7个BLEU点。 此外，我们发现与竞争方法相比，这种体系结构导致了更有效的OOV处理

##### [75] Wang Xing, Tu Zhaopeng, Xiong Deyi, et al.Translating phrases in neural machine translation//Proceedings of the2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1432-1442

##### [76] Arthur P, Neubig G, Nakamura S.Incorporating discrete translation lexicons into neural machine translation//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:1557-1567
Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.1
神经机器翻译（neural machine translation，NMT）在翻译对理解句子意义至关重要的低频实词时经常出错。 我们提出了一种方法来缓解这一问题，通过使用离散翻译词典来增强NMT系统，这些词典可以有效地对这些低频词的翻译进行编码。 我们描述了一种计算候选翻译中下一个单词的词典概率的方法，该方法使用NMT模型的注意向量来选择模型应该关注的源单词的词汇概率。 我们测试了两种将该概率与标准NMT概率相结合的方法:（1）将其用作偏置；（2）线性插值。 在两个语料库上的实验表明，BLEU和NIST评分分别提高了2.0-2.3和0.13-0.44，且收敛速度更快。

##### [77] Sennrich R, Haddow B.Linguistic input features improve neural machine translation//Proceedings of the 1st Conference on Machine Translation.Berlin, Germany, 2016:83-91
Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder–decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-ofspeech tags, and syntactic dependency labels as input features to English↔German and English→Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An opensource implementation of our neural MT system is available1, as are sample files and configurations2.
近年来，神经机器翻译取得了令人瞩目的成果，但对外部语言信息的利用却很少。 本文证明了神经MT模型具有较强的学习能力，不会造成语言特征的冗馀； 它们可以很容易地结合在一起，以提供性能上的进一步改进。 我们将编码器的嵌入层推广到注意编码器-解码器体系结构中，以支持除基线字特征之外的任意特征的包含。 我们将词形特征、词性标记和句法依存标记作为输入特征添加到英语→德语和英语→罗马尼亚语的神经机器翻译系统中。 在对WMT16训练集和测试集的实验中，我们发现语言输入特征可以通过三个指标来提高模型质量:困惑度、BLEU和CHRF3。 我们的Neural MT系统的一个开放源码实现是可用的1，示例文件和配置也是可用的2。

##### [78] Chen Kehai, Wang Rui, Utiyama M, et al.Neural machine translation with source dependency representation//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:2836-3842
Source dependency information has been successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel attentional NMT with source dependency representation to improve translation performance of NMT, especially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system.
源依赖信息已被成功地引入到统计机器翻译中。 然而，对于神经机器翻译(NMT)的初步尝试还很少，如将源词及其依存关系标签的表示级联在一起。 本文提出了一种新的具有源依存关系表示的注意NMT，以提高NMT的翻译性能，尤其是对长句的翻译性能。 在NIST汉译英任务中的实验结果表明，与一个强大的NMT系统相比，该方法平均提高了1.6个BLEU。

##### [79] Li Junhui, Xiong Deyi, Tu Zhaopeng, et al.Modeling source syntax for neural machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:688-697
Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.
尽管神经机器翻译(NMT)中的非语言学序列到序列模型具有一定的隐式学习源句句法信息的能力，但本文表明，源句法可以有效地显式地融入NMT中，以提供进一步的改进。 具体地说，我们线性化源语句的解析树以获得结构标记序列。 在此基础上，我们提出了三种不同的编码器将源句法并入NMT:1）并行RNN编码器，并行学习单词和标注向量； 2）分层RNN编码器，学习两级层次结构中的单词和标签注释向量；3）混合RNN编码器，在单词和标签混合的序列上拼接地学习单词和标签注释向量。汉英翻译实验表明，所有三种建议的句法编码器都能够提高翻译精度。有趣的是，最简单的RNN编码器产生1.4bleu点的显著改进的性能。

##### [80] Bastings J, Titov I, Aziz W, et al.Graph convolutional encoders for syntax-aware neural machine translation//Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1947-1957
We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoderdecoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.
本文提出了一种将句法结构与基于神经注意的机器翻译编解码器模型相结合的简单而有效的方法。 我们依赖于图卷积网络（GCNs），这是一种最近发展起来的用于对图结构数据建模的神经网络。 我们的GCN使用源句的预测句法依赖树来产生对它们的句法邻域敏感的单词（即编码器的隐藏状态）的表示。 GCN以字表示为输入，以字表示为输出，因此它们可以很容易地作为层合并到标准编码器中（例如，在双向RNN或卷积神经网络之上）。 我们通过不同类型编码器的英德和英捷克翻译实验来评估它们的有效性，并观察到在所有考虑的设置中，它们的语法不可知版本都有显著的改进。

##### [81] Wu Shuangzhi, Zhou Ming, Zhang Dongdong.Improved neural machine translation with source syntax//Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017) .Melbourne, Australia, 2017:4179-4185
Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently achieved the state-of-the-art performance. Researchers have proven that extending word level attention to phrase level attention by incorporating source-side phrase structure can enhance the attention model and achieve promising improvement. However, word dependencies that can be crucial to correctly understand a source sentence are not always in a consecutive fashion (i.e. phrase structure), sometimes they can be in long distance. Phrase structures are not the best way to explicitly model long distance dependencies. In this paper we propose a simple but effective method to incorporate source-side long distance dependencies into NMT. Our method based on dependency trees enriches each source state with global dependency structures, which can better capture the inherent syntactic structure of source sentences. Experiments on Chinese-English and English-Japanese translation tasks show that our proposed method outperforms state-of-the-art SMT and NMT baselines.
基于编码器-解码器结构的神经机器翻译(NMT)最近取得了最先进的性能。 研究人员已经证明，通过结合词源侧词组结构，将词级注意扩展到词组级注意，可以增强注意模型，并取得很好的效果。 然而，对正确理解源句至关重要的词的依存关系并不总是以连续的方式（即短语结构）存在，有时它们可能相隔很远。 短语结构不是显式建模远程依赖关系的最佳方法。 本文提出了一种简单而有效的方法，将信源端的长距离相关性引入到NMT中。 我们的方法基于依赖树，用全局依赖结构丰富了每一个源状态，能够更好地捕捉源句的固有句法结构。 在汉英和英日翻译任务中的实验表明，本文提出的方法优于目前最先进的SMT和NMT基线。

##### [82] Chen Huadong, Huang Shujian, Chiang D, et al.Improved neural machine translation with a syntax-aware encoder and decoder//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1936-1945
##### [83] Niehues J, Cho E.Exploiting linguistic resources for neural machine translation using multi-task learning//Proceedings of the Conference on Machine Translation (WMT 2017) .Copenhagen, Denmark, 2017:80-89
Linguistic resources such as part-ofspeech (POS) tags have been extensively used in statistical machine translation (SMT) frameworks and have yielded better performances. However, usage of such linguistic annotations in neural machine translation (NMT) systems has been left under-explored.
In this work, we show that multi-task learning is a successful and a easy approach to introduce an additional knowledge into an end-to-end neural attentional model. By jointly training several natural language processing (NLP) tasks in one system, we are able to leverage common
information and improve the performance of the individual task.
We analyze the impact of three design decisions in multi-task learning: the tasks used in training, the training schedule, and the degree of parameter sharing across the tasks, which is defined by the network architecture. The experiments are conducted for an German to English translation task. As additional linguistic resources, we exploit POS information and named-entities (NE). Experiments show that the translation quality can be improved by up to 1.5 BLEU points under the low-resource condition. The performance of the POS tagger is also improved using the multi-task learning scheme.
词性标注(POS)等语言资源在统计机器翻译(SMT)框架中得到了广泛的应用，并取得了较好的性能。 然而，这类语言注释在神经机器翻译系统中的应用一直没有得到充分的研究。 
在这项工作中，我们证明了多任务学习是一个成功的和容易的方法来引入额外的知识到端到端的神经注意模型。 通过在一个系统中联合训练多个自然语言处理(NLP)任务，我们能够利用公共 
信息和提高单个任务的性能。 
我们分析了三个设计决策对多任务学习的影响:训练中使用的任务、训练计划和跨任务参数共享的程度，这三个设计决策是由网络体系结构定义的。 实验是为一个德语到英语的翻译任务而进行的。 作为额外的语言资源，我们利用了POS信息和命名实体(NE)。 实验表明，在低资源条件下，该算法的翻译质量可提高1.5个bleu点。 使用多任务学习方案，也提高了词性标注器的性能。 

##### [84] Zhang Jiacheng, Liu Yang, Luan Huanbo, et al.Prior knowledge integration for neural machine translation using posterior regularization//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1514-1523
Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation. We represent prior knowledge sources as features in a log-linear model, which guides the learning process of the neural translation model. Experiments on ChineseEnglish translation show that our approach leads to significant improvements.
尽管近年来神经机器翻译取得了很大的进展，但如何集成多个重叠的、任意的先验知识源仍然是一个难题。 在本文中，我们提出使用后验正则化来提供一个将先验知识集成到神经机器翻译中的一般框架。 将先验知识表示为对数线性模型的特征，指导神经翻译模型的学习过程。 在汉英翻译中的实验表明，我们的方法有了很大的改进。 

##### [85] Eriguchi A, Hashimoto K, Tsuruoka Y.Tree-to-Sequence attentional neural machine translation//Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:823-833
Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequenceto-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT’15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system
现有的神经机器翻译(NMT)模型大多集中于序列数据的转换，没有直接利用句法信息。 我们提出了一种新的端到端句法NMT模型，扩展了源端短语结构的序列到序列模型。 我们的模型有一个注意机制，使解码器能够生成一个已翻译的单词，同时将其与源句子的短语和单词轻柔地对齐。 在WAT'15英日数据集上的实验结果表明，我们提出的模型比序列对序列的注意NMT模型具有更好的性能，并与目前最先进的树对串SMT系统进行了比较

##### [86] Aharoni R, Goldberg Y.Towards string-to-tree neural machine translation//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:132-140
We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. Experiments on the WMT16 German-English news translation task shown improved BLEU scores when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A smallscale human evaluation also showed an advantage to the syntax-aware system.
我们提出了一种简单的方法，通过翻译成线性化、词汇化的成分树，将目标语言的句法信息合并到神经机器翻译系统中。 在WMT16德英新闻翻译任务中的实验表明，与在相同数据集上训练的语法不可知的NMT基线相比，BLEU得分有所提高。 对句法感知系统翻译的分析表明，与基线相比，它在翻译过程中执行了更多的重新排序。 小规模的人工评估也显示了语法感知系统的优势。 

##### [87] Wu Shuangzhi, Zhang Dongdong, Yang Nan, et al.Sequenceto-Dependency neural machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:698-707
Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks.
目前，典型的神经机器翻译(NMT)模型以线性序列的形式产生从左到右的翻译，在此过程中，目标句子的潜在句法结构并没有得到明确的关注。 受利用目标语言句法知识改进统计机器翻译的启发，本文提出了一种新的序列依赖神经机器翻译方法（SD-NMT），该方法将目标词序列及其对应的依赖结构联合构建和建模，并将该结构作为上下文来促进词的生成。 实验结果表明，该方法在汉英和日语翻译任务中的性能明显优于现有的基线。

##### [88] Gulcehre C, Firat O, Xu K, et al.On using monolingual corpora in neural machine translation.arXiv preprint/1503.03535v2, 2015
Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to 1.96 BLEU improvement on the lowresource language pair Turkish-English, and 1.59 BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of 0.39 and 0.47 BLEU scores over the neural machine translation baselines, respectively.
近年来，基于端到端神经网络的机器翻译体系结构在EN-FR和EN-DE翻译中得到了很好的应用。 可以说，这一成功背后的一个主要因素是高质量的平行语料库的可用性。 本文研究了如何利用丰富的单语语料库进行神经机器翻译。 与基于短语和分层的基线相比，我们在低资源语言对土耳其语-英语上获得了1.96bleu的改进，在中英文聊天消息的焦点域任务上获得了1.59bleu的改进。 虽然我们的方法最初是针对并行数据较少的任务，但我们发现它也扩展到高资源语言，如CS-EN和DE-EN，在这两种语言中，我们分别比神经机器翻译基线提高了0.39和0.47个BLEU分数。

##### [89] Domhan T, Hieber F.Using target-side monolingual data for neural machine translation through multi-task learning//Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1501-1506
The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of parallel data, and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable multi-task learning for two strongly related tasks: target-side language modeling and translation. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve NMT performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only.
神经机器翻译（Neural Machine Translation，NMT）模型的性能在很大程度上依赖于足够多的并行数据的可用性，而如何有效利用大量的单语数据还没有找到。 我们建议在一个神经序列到序列的模型中修改解码器，以便能够对两个强相关的任务进行多任务学习:目标端语言建模和翻译。 解码器通过两个通道预测下一个目标单词，一个是最低层的目标端语言模型，另一个是以源表示为条件的注意递归模型。 这种体系结构允许对大量的单语和中等数量的双语数据进行联合培训，以提高NMT性能。 三种语言对的新闻领域的初步结果显示，与仅对双语数据进行培训的基线相比，有适度但一致的改进。

##### [90] Sennrich R, Haddow B, Birch A.Improving neural machine translation models with monolingual data//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:86-96
Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic backtranslation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English↔German (+2.8–3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish→English (+2.1–3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English→German.
神经机器翻译（Neural Machine Translation，NMT）在仅使用并行数据进行训练的情况下，对多个语言对获得了最先进的性能。 Targetside单语数据在提高基于短语的统计机器翻译流利性方面起着重要作用，我们研究了单语数据在NMT中的应用。 与以往将NMT模型与单独训练的语言模型相结合的工作相反，我们注意到编码器-解码器NMT体系结构已经具有学习与语言模型相同的信息的能力，并且我们探索了在不改变神经网络体系结构的情况下使用单语言数据进行训练的策略。 通过将单语训练数据与自动回译数据配对，我们可以将其作为额外的并行训练数据处理，并且我们对WMT15任务英语×德语(+2.8-3.7bleu)和资源不足的IWSLT14任务土耳其语→英语(+2.1-3.4bleu)获得了显著的改进，获得了新的最新结果。 我们还表明，对域内单语和并行数据的微调对IWSLT15任务英语→德语有很大的改进。

##### [91] Zhang Jiajun, Zong Chengqing.Exploiting source-side monolingual data in neural machine translation//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:1535-1545
Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently become a new paradigm. Researchers have proven that the target-side monolingual data can greatly enhance the decoder model of NMT. However, the source-side monolingual data is not fully explored although it should be useful to strengthen the encoder model of
NMT, especially when the parallel corpus is far from sufficient. In this paper, we propose two approaches to make full use of the sourceside monolingual data in NMT. The first approach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach applies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT.
基于编解码器结构的神经机器翻译(NMT)是近年来出现的一个新的研究范式。 研究人员已经证明，目标端的单语数据可以大大增强NMT的解码器模型。 然而，源端单语言数据并没有得到充分的探讨，尽管加强编码器模型应该是有用的。 
NMT，特别是当平行语料库远远不够时。 本文提出了两种充分利用NMT源端单语数据的方法。 第一种方法采用自学习算法生成用于NMT训练的合成大规模并行数据。 第二种方法采用多任务学习框架，使用两个NMT同时预测源端单语句子的翻译和重排。 大量的实验表明，与基于强注意的NMT方法相比，本文提出的方法得到了显著的改进。 

##### [92] Ramachandran P, Liu P J, Le Q V.Unsupervised pretraining for sequence to sequence learning//Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:383-391
This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-theart results on the WMT English→German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from the previous best models on both WMT’14 and WMT’15 English→German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.
为了提高序列对序列(SEQ2SEQ)模型的精度，本文提出了一种通用的无监督学习方法。 在我们的方法中，SEQ2SEQ模型的编码器和解码器的权重被初始化为两个语言模型的预先训练的权重，然后用标记的数据进行微调。 我们将该方法应用于机器翻译和抽象摘要中具有挑战性的基准测试，发现它显著地改进了后续的有监督模型。 我们的主要结果是，预训练提高了SEQ2SEQ模型的泛化能力。 我们在WMT英语→德语任务上取得了最新的成果，超过了基于短语的机器翻译和神经机器翻译的一系列方法。 我们的方法在WMT'14和WMT'15英语→德语两种语言上都比以前的最佳模型有了1.3BLEU的显著改进。 我们还对抽象摘要进行了人类评价，发现我们的方法在统计上显著优于纯监督学习基线。 

##### [93] Currey A, Barone A, Heafield K.Copied monolingual data improves low-resource neural machine translation//Proceedings of the Conference on Machine Translation (WMT 2017) .Copenhagen, Denmark, 2017:148-156
We train a neural machine translation (NMT) system to both translate sourcelanguage text and copy target-language text, thereby exploiting monolingual corpora in the target language. Specifically, we create a bitext from the monolingual text in the target language so that each source sentence is identical to the target sentence. This copied data is then mixed with the parallel corpus and the NMT system is trained like normal, with no metadata to distinguish the two input languages.
Our proposed method proves to be an effective way of incorporating monolingual data into low-resource NMT. On Turkish↔English and Romanian↔English translation tasks, we see gains of up to 1.2 BLEU over a strong baseline with back-translation. Further analysis shows that the linguistic phenomena behind these gains are different from and largely orthogonal to back-translation, with our copied corpus method improving accuracy on named entities and other words that should remain identical between the source and target languages.
我们训练了一个神经机器翻译(NMT)系统来翻译源语言文本和复制目标语言文本，从而利用目标语言中的单语语料库。 具体地说，我们从目标语言中的单语文本创建一个位文本，以便每个源句与目标句相同。 然后将复制的数据与并行语料库混合，NMT系统像正常情况一样进行训练，没有元数据来区分两种输入语言。 
我们提出的方法被证明是一种将单语数据合并到低资源NMT中的有效方法。 在土耳其语英语和罗马尼亚语英语翻译任务中，我们看到在有回译的强大基线上增加了1.2个bleu。 进一步的分析表明，这些收获背后的语言现象不同于回译，并且在很大程度上正交于回译，我们的复制语料库方法提高了命名实体和其他应该在源语言和目标语言之间保持相同的单词的准确性。 

##### [94] Fadaee M, Bisazza A, Monz C.Data augmentation for low-resource neural machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:567-573
The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.
神经机器翻译系统的质量在很大程度上取决于大规模并行语料库的可用性。 对于资源较少的语言对，情况并非如此，导致翻译质量较差。 受计算机视觉研究的启发，我们提出了一种新的数据增强方法，该方法通过在新的、综合创建的上下文中生成包含稀有词的新句子对来针对低频词。 在模拟的低资源设置上的实验结果表明，该方法在基线上提高了2.9个BLEU点，在回译上提高了3.2个BLEU点。

##### [95] Zoph B, Yuret D, May J, et al.Transfer learning for low-resource neural machine translation//Proceedings of the2016Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:1568-1575
The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.
用于神经机器翻译(NMT)的编码器-解码器框架已被证明在大数据场景中是有效的，但对于低资源语言的效果要差得多。 我们提出了一种迁移学习方法，它可以显著提高一系列低资源语言的BLEU分数。 我们的核心思想是首先训练一个高资源语言对（父模型），然后将一些学习到的参数传递给低资源语言对（子模型）来初始化和约束训练。 使用我们的迁移学习方法，我们在四个低资源语言对上平均改进了基线NMT模型5.6BLEU。 集成和未知词替换增加了2个BLEU，使得低资源机器翻译的NMT性能接近于一个强的基于语法的机器翻译系统(SBMT)，超过了它在一个语言对上的性能。 另外，利用迁移学习模型对SBMT系统进行重新评分，平均提高了1.3BLEU，改善了低资源机器翻译的现状。

##### [96] Chen Yun, Liu Yang, Cheng Yong, et al.A teacherstudent framework for zero-resource neural machine translation//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1925-1935
While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on this assumption, our method is able to train a source-to-target NMT model (“student”) without parallel corpora available, guided by an existing pivot-to-target NMT model (“teacher”) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.
端到端神经机器翻译(NMT)近年来取得了长足的进步，但仍然面临着低资源语言对和领域的数据稀缺问题。 本文提出了一种零资源NMT方法，该方法假设并行句子在第三语言中生成句子的概率很小。 基于这一假设，我们的方法能够在现有的枢轴到目标NMT模型（“教师”）的指导下，在没有平行语料库的情况下，在一个源枢轴平行语料库上训练源到目标NMT模型（“学生”）。 实验结果表明，该方法比基于基线枢轴的模型在不同的语言对上提高了+3.0个BLEU点。


##### [97] Zheng Hao, Cheng Yong, Liu Yang.Maximum expected likelihood estimation for zero-resource neural machine translation//Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017) .Melbourne, Australia, 2017:4251-4257
While neural machine translation (NMT) has made remarkable progress in translating a handful of resource-rich language pairs recently, parallel corpora are not always readily available for most language pairs. To deal with this problem, we propose an approach to zero-resource NMT via maximum expected likelihood estimation. The basic idea is to maximize the expectation with respect to a pivot-to-source translation model for the intended source-to-target model on a pivot-target parallel corpus. To approximate the expectation, we propose two methods to connect the pivot-to-source and source-to-target models. Experiments on two zero-resource language pairs show that the proposed approach yields substantial gains over baseline methods. We also observe that when trained jointly with the source-to-target model, the pivotto-source translation model also obtains improvements over independent training.
近年来，神经机器翻译(NMT)在翻译少数资源丰富的语言对方面取得了显著的进展，但并行语料库并不总是对大多数语言对有效。 针对这一问题，本文提出了一种基于最大期望似然估计的零资源NMT算法。 其基本思想是在枢轴-目标并行语料库上，最大限度地提高目标源-目标模型对枢轴-源翻译模型的期望。 为了逼近期望，我们提出了两种方法来连接枢轴到源模型和源到目标模型。 在两个零资源语言对上的实验表明，所提出的方法比基线方法产生了显著的增益。 我们还观察到，当与源语到目标语模型联合训练时，枢轴到源语翻译模型也比独立训练得到了改进。

##### [98] Cheng Yong, Yang Qian, Liu Yang, et al.Joint training for pivot-based neural machine translation//Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017) .Melbourne, Australia, 2017:3974
While recent neural machine translation approaches have delivered state-of-the-art performance for resource-rich language pairs, they suffer from the data scarcity problem for resource-scarce language pairs. Although this problem can be alleviated by exploiting a pivot language to bridge the source and target languages, the source-to-pivot and pivot-to-target translation models are usually independently trained. In this work, we introduce a joint training algorithm for pivot-based neural machine translation. We propose three methods to connect the two models and enable them to interact with each other during training. Experiments on Europarl and WMT corpora show that joint training of source-to-pivot and pivot-to-target models leads to significant improvements over independent training across various languages.
虽然最近的神经机器翻译方法已经为资源丰富的语言对提供了最先进的性能，但是它们受到资源稀缺的语言对的数据稀缺问题的困扰。 虽然可以通过使用透视语言来桥接源语言和目标语言来缓解这个问题，但是源到透视和透视到目标翻译模型通常是独立训练的。 本文提出了一种基于枢轴的神经机器翻译联合训练算法。 我们提出了三种方法来连接这两个模型，使它们能够在训练过程中相互交互。 在Europarl和WMT语料库上的实验表明，源到枢和枢到目标模型的联合训练比跨语言的独立训练有显著的改进。 

##### [99] Ranzato M A, Chopra S, Auli M, et al.Sequence level training with recurrent neural networks.arXiv preprint/1511.06732v7, 2015
Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.
许多自然语言处理应用程序使用语言模型来生成文本。 这些模型通常被训练来预测序列中的下一个单词，给出前一个单词和一些上下文，例如图像。 但是，在测试时，预期模型将从头开始生成整个序列。 这种差异使生成变得脆弱，因为错误可能会在生成过程中累积。 我们通过提出一种新的序列级训练算法来解决这个问题，该算法直接优化测试时使用的度量，例如BLEU或Rouge。 在三个不同的任务上，我们的方法优于贪婪生成的几个强基线。 当这些基线采用波束搜索时，该方法也是有竞争力的，同时速度是波束搜索的几倍。

##### [100] Bengio S, Vinyals O, Jaitly N, et al.Scheduled sampling for sequence prediction with recurrent neural networks//Proceedings of the Neural Information Processing Systems (NIPS 2015) .Montreal, Canada, 2015:1-9
Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.
递归神经网络可以被训练以在给定一些输入的情况下产生令牌序列，例如最近在机器翻译和图像字幕方面的结果。 当前训练它们的方法包括在给定当前（重复）状态和前一个令牌的情况下，最大化序列中每个令牌的可能性。 在推理时，未知的前一个令牌然后被模型本身生成的令牌替换。 训练和推理之间的这种差异可以产生错误，这些错误可以沿着生成的序列快速累积。 我们提出了一个课程学习策略，将训练过程从使用真实的前一个令牌的完全指导方案轻轻地改变为使用生成的令牌的较少指导方案。 在多个序列预测任务上的实验表明，该方法取得了显著的改进。 此外，它还成功地用于我们赢得2015年MSCOCO图像字幕挑战赛的参赛作品。

##### [101] Shen Shiqi, Cheng Yong, He Zhongjun, et al.Minimum risk training for neural machine translation//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:1683-1692
We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.
提出了端到端神经机器翻译的最小风险训练方法。 与传统的最大似然估计不同，最小风险训练能够直接针对不一定可区分的任意评估度量来优化模型参数。 实验表明，我们的方法在不同语言对的最大似然估计上取得了显着的改进。 对于体系结构来说，我们的方法是透明的，可以应用于更多的神经网络，并且潜在地受益于更多的NLP任务。

##### [102] Bahdanau D, Brakel P, Lowe R, et al.An actor-critic algorithm for sequence prediction.arXiv preprint/1607.07086v2, 2016
We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.
本文提出了一种基于强化学习(RL)的神经网络训练方法。 当前的对数似然训练方法由于训练和测试模式之间的差异而受到限制，因为模型必须生成基于先前猜测的令牌，而不是基于实际情况的令牌。 我们通过引入一个批评性网络来解决这个问题，该网络被训练成在给定参与者网络策略的情况下预测输出令牌的值。 这导致了一个更接近测试阶段的训练过程，并允许我们直接优化特定于任务的得分，例如BLEU。 至关重要的是，由于我们在监督学习环境中而不是在传统的RL环境中利用这些技术，因此我们将批评性网络设置为实际的真理输出。 实验结果表明，该方法在合成任务和德英机器翻译中都能提高性能。 我们的分析为这些方法在自然语言生成任务中的应用铺平了道路，例如机器翻译、字幕生成和对话建模。

##### [103] Wiseman S, Rush A M.Sequence-to-Sequence learning as beam-search optimization//Proceedings of the 2016Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:1296-1302
Sequence-to-Sequence (seq2seq) modeling has rapidly become an important generalpurpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beamsearch training scheme, based on the work of Daume III and Marcu (2005), that extends ´ seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highlyoptimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.
序列对序列(SEQ2SEQ)建模已迅速成为一种重要的通用自然语言处理工具，在文本生成和序列标注等领域得到了广泛的应用。 SEQ2SEQ建立在深度神经语言模型的基础上，继承了它在估计局部、下一个单词分布方面的显著精确度。 在本文中，我们在Daume III和Marcu(2005)的工作基础上引入了一种模型和波束搜索训练方案，将SEQ2SEQ扩展到学习全局序列分数。 该结构化方法避免了与局部训练相关的经典偏差，并将训练损失与测试时间的使用统一起来，同时保留了SEQ2SEQ经过验证的模型体系结构及其有效的训练方法。 我们证明，我们的系统在排序任务的三个不同序列（词序、解析和机器翻译）上优于高度优化的基于注意的SEQ2SEQ系统和其他基线。

##### [104] Norouzi M, Bengio S, Chen Zhifeng, et al.Reward augmented maximum likelihood for neural structured prediction//Proceedings of the Neural Information Processing Systems (NIPS 2016) .Barcelona, Spain, 2016:1723-1731
A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. By establishing a link between the log-likelihood and expected reward objectives, we show that an optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated scaled rewards. Accordingly, we present a framework to smooth the predictive probability of the outputs using their corresponding rewards. We optimize the conditional log-probability of augmented outputs that are sampled proportionally to their exponentiated scaled rewards. Experiments on neural sequence to sequence models for speech recognition and machine translation show notable improvements over a maximum likelihood baseline by using reward augmented maximum likelihood (RML), where the rewards are defined as the negative edit distance between the outputs and the ground truth labels.
结构化输出预测中的一个关键问题是直接优化测试评估所需的任务奖励函数。 本文提出了一种将任务报酬纳入最大似然框架的简单且计算效率高的方法。 通过建立对数似然与期望报酬目标之间的联系，我们证明了当给定输入的输出的条件分布与其指数比例报酬成正比时，获得最优正则化期望报酬。 因此，我们提出了一个框架，以平滑的预测概率的输出使用其相应的奖励。 我们优化了增加输出的条件对数概率，这些输出是按指数比例采样的。 在语音识别和机器翻译的神经序列到序列模型的实验中，使用奖励增强的最大似然(RML)对最大似然基线进行了显著改进，其中奖励被定义为输出和地面真实标签之间的负编辑距离。 

##### [105] Calixto I, Liu Q, Campbell N.Doubly-attentive decoder for multi-modal neural machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1913-1924
We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.
本文提出了一种多模态神经机器翻译模型，该模型中的双注意译码器自然地融合了利用预训练的卷积神经网络获得的空间视觉特征，弥补了图像描述与翻译之间的差距。 我们的解码器在目标语言中生成单词时，通过两个独立的注意机制独立地注意源语言单词和图像的部分。 我们发现，该模型不仅能有效地利用领域内的多模态数据，而且能有效地利用大型通用领域的纯文本多模态语料库。 我们还报告了多30K数据集的最新结果。

##### [106] Delbrouck J-B, Dupont S.An empirical study on the effectiveness of images in multimodal neural machine translation//Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:921-930
In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multimodal translation task (English, image → German) and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.
在最先进的神经机器翻译(NMT)中，在解码期间使用注意机制来增强翻译。 在每个步骤中，解码器使用这种机制来关注源句的不同部分，以在输出其目标字之前收集最有用的信息。 近年来，注意机制在多模态任务中的有效性也得到了研究，在多模态任务中，人们可以同时关注句子部分和图像区域。 本文比较了多模态翻译任务（英语、图像→德语）中的几种注意机制，并评价了该模型利用图像提高翻译质量的能力。 我们在多个30K数据集上的得分超过了最先进的得分，但是我们在翻译时识别并报告了机器的不同错误行为。

##### [107] Calixto I, Liu Qun.Incorporating global visual features into attention-based neural machine translation//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1003-1014
We introduce multi-modal, attentionbased Neural Machine Translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. Global image features are extracted using a pre-trained convolutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate translations into English and German, how different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models. We report new state-of-the-art results and our best models also significantly improve on a comparable Phrase-Based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set.
我们引入了多模态的、基于注意的神经机器翻译(NMT)模型，该模型将视觉特征结合到编码器和解码器的不同部分中。 使用预先训练的卷积神经网络提取全局图像特征，并且将全局图像特征并入(i)作为源句子中的单词，(ii)用于初始化编码器隐藏状态，以及(iii)作为用于初始化解码器隐藏状态的附加数据。 在我们的实验中，我们评估了英文和德文的翻译，不同的融入全球图像特征的策略如何比较，以及哪些策略表现最好。 我们还研究了添加合成多模态、多语种数据对多模态模型的影响，发现添加数据对多模态模型有正向影响。 我们报告了新的最先进的结果，我们最好的模型也显著改进了基于短语的统计MT(PBSMT)模型，该模型根据评估的所有度量在多30K数据集上进行了训练。 据我们所知，这是第一次纯粹的神经模型比PBSMT模型在该数据集上评估的所有度量上都有显著的改进。

##### [108] Caglayan O, Aransa W, Wang Y, et al.Does multimodality help human and machine for translation and image captioning?//Proceedings of the 1st Conference on Machine Translation (WMT 2016) .Berlin, Germany, 2016:627-633
This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.
本文介绍了LIUM公司和CVC公司为应对WMT16多模态机器翻译挑战而开发的系统。 我们探索了各种比较方法，即基于短语的系统和使用单模态或多模态数据训练的注意递归神经网络模型。 为了评估多模态数据对机器翻译和图像描述生成的有用性，我们还进行了人的评估。 根据自动评估指标BLEU和Meteor，我们的系统在这两个任务中都获得了最好的结果。

##### [109] Gehring J, Auli M, Grangier D, et al.Convolutional sequence to sequence learning.arXiv preprint/1705.03122v3, 2017
The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT’14 English-German and WMT’14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.?
序列学习的普遍方法是通过递归神经网络将输入序列映射到可变长度的输出序列。 介绍了一种完全基于卷积神经网络的结构。 与递归模型相比，在训练期间可以完全并行化所有元素的计算，以更好地利用GPU硬件，并且优化更容易，因为非线性的数目是固定的并且与输入长度无关。 门控线性单元的使用简化了梯度传播，并且我们为每个解码器层配备了单独的关注模块。 我们的性能优于Wu等人的深度LSTM设置的精度。 （2016）在WMT'14英德翻译和WMT'14英法翻译上，GPU和CPU上的速度都快了一个数量级。 

##### [110] Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need.arXiv preprint/1706.03762v4, 2017
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.
显性序列转导模型基于包括编码器和解码器的复杂的递归或卷积神经网络。 性能最好的型号还通过一个注意机制连接编码器和解码器。 我们提出了一种新的简单的网络结构&变压器，它完全基于注意机制，完全避免了重复和卷积。 在两个机器翻译任务上的实验表明，这些模型具有更高的质量，同时具有更强的并行性，并且训练时间明显减少。 我们的模型在2014年WMT英德翻译任务中实现了28.4BLEU，比现有的最佳结果（包括集成）提高了2BLEU以上。 在WMT2014英法翻译任务中，我们的模型在8个GPU上进行3.5天的培训后，建立了一个新的单模型最新BLEU评分为41.0，这只是文献中最佳模型培训成本的一小部分。

##### [111] He Di, Xia Yingce, Qin Tao, et al.Dual learning for machine translation//Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS 2016) .Barcelona, Spain, 2016:1-9
While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the languagemodel likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English↔French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.
近两年来，神经机器翻译(NMT)取得了长足的发展，但其训练需要数千万对双语句子。 然而，人的标签是非常昂贵的。 为了解决这一训练数据瓶颈，我们开发了一种双学习机制，使NMT系统能够通过双学习游戏从未标记的数据中自动学习。 这一机制受到以下观察的启发:任何机器翻译任务都有一个双重任务，例如，英语-法语翻译（原语）和法语-英语翻译（双语）； 原始和双重任务可以形成一个闭环，并产生信息反馈信号来训练翻译模型，即使没有人类标记器的参与。 在双学习机制中，我们使用一个Agent来表示原始任务的模型，另一个Agent来表示双任务的模型，然后通过一个强化学习过程要求它们相互学习。 基于在此过程中产生的反馈信号（例如，模型输出的语言模型似然性，以及原始和对偶翻译后原始句子的重构误差），我们可以迭代地更新这两个模型直到收敛（例如，使用策略梯度方法）。 我们把相应的神经机器翻译方法称为双NMT。 实验表明，双NMT在英法翻译中有很好的效果； 特别是，通过学习单语数据（10%的双语数据作为WARM START），它达到了与全双语数据训练的NMT相当的准确率，用于法语到英语的翻译任务。

##### [112] Nguyen K, Daume III H, Boyd-Graber J.Reinforcement learning for bandit neural machine translation with simulated human feedback//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP2017) .Copenhagen, Denmark, 2017:1465-1475
Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoderdecoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.
机器翻译是从人类反馈中进行强化学习的一个自然的候选问题:用户对候选翻译提供快速、肮脏的评级，以指导系统改进。 然而，目前的神经机器翻译训练主要集中在昂贵的人工生成的参考译文上。 我们描述了一种强化学习算法，该算法从模拟人的反馈中改进神经机器翻译系统。 我们的算法结合了Advantage Actor-Critic算法（MNIH等人，2016）和基于注意力的神经编码器解码器架构（Luong等人，2015）。 该算法能有效地优化传统的语料库级机器翻译度量，对模拟人类实际行为的偏斜、高方差、粒度反馈具有较强的鲁棒性。 

##### [113] Yang Zhen, Chen Wei, Wang Feng, et al.Improving neural machine translation with conditional sequence generative adversarial nets.arXiv preprint/1703.04887v2, 2017
This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences ( i.e., the golden target sentences); And the discriminator makes efforts to discriminate the machine-generated sentences from humantranslated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.
本文提出了一种将GANS应用于NMT的方法。 我们建立了一个条件序列生成对抗性网络，它包括两个子模型，一个生成器和一个鉴别器。 生成器的目的是生成难以区别于人工翻译的句子（即金色目标句）； 鉴别者努力区分机器生成的句子和人工翻译的句子。 这两个子模型在达到纳什均衡时进行最小-最大博弈，达到双赢。 此外，静态句子级BLEU被用作生成器的增强目标，这将生成器偏向于高BLEU点。 在训练过程中，采用动态鉴别器和静态BLEU目标对生成的句子进行评价，并对评价结果进行反馈，以指导生成器的学习。 实验结果表明，该模型在英德和汉英翻译任务中的性能均优于传统的RNN搜索和最新出现的最先进的转换算法。

##### [114] Wu Lijun, Xia Yingce, Zhao Li, et al.Adversarial neural machine translation.arXiv preprint/1704.06933v3, 2017
In this paper, we study a new learning paradigm for neural machine translation (NMT). Instead of maximizing the likelihood of the human translation as in previous works, we minimize the distinction between human translation and the translation given by an NMT model. To achieve this goal, inspired by the recent success of generative adversarial networks (GANs), we employ an adversarial training architecture and name it as AdversarialNMT. In Adversarial-NMT, the training of the NMT model is assisted by an adversary, which is an elaborately designed 2D convolutional neural network (CNN). The goal of the adversary is to differentiate the translation result generated by the NMT model from that by human. The goal of the NMT model is to produce high quality translations so as to cheat the adversary. A policy gradient method is leveraged to co-train the NMT model and the adversary. Experimental results on English→French and German→English translation tasks show that Adversarial-NMT can achieve significantly better translation quality than several strong baselines.
本文研究了一种新的神经机器翻译(NMT)学习范式。 我们不是像前人那样最大化人工翻译的可能性，而是最小化人工翻译和NMT模型给出的翻译之间的区别。 为了实现这一目标，在最近生成性对抗网络(GANS)成功的启发下，我们采用了一种对抗训练体系结构，并将其命名为对抗NMT。 在对抗-NMT中，NMT模型的训练由一个对手辅助，该对手是一个精心设计的2D卷积神经网络(CNN)。 对手的目标是将NMT模型产生的翻译结果与人类产生的翻译结果区分开来。 NMT模式的目标是产生高质量的翻译，从而欺骗对手。 利用策略梯度法对NMT模型和敌手进行联合训练。 在英语→法语和德语→英语翻译任务中的实验结果表明，反义NMT比几种强基线能获得更好的翻译质量。

##### [115] Shah K, Logacheva V, Paetzold G H.SHEF-NN:Translation quality estimation with neural networks//Proceedings of the10th Workshop on Statistical Machine Translation.Lisbon, Portugal, 2015:342-347
We describe our systems for Tasks 1 and 2 of the WMT15 Shared Task on Quality Estimation. Our submissions use (i) a continuous space language model to extract additional features for Task 1 (SHEFGP, SHEF-SVM), (ii) a continuous bagof-words model to produce word embeddings as features for Task 2 (SHEF-W2V) and (iii) a combination of features produced by QuEst++ and a feature produced with word embedding models (SHEFQuEst++). Our systems outperform the baseline as well as many other submissions. The results are especially encouraging for Task 2, where our best performing system (SHEF-W2V) only uses features learned in an unsupervised fashion.
我们描述了WMT15质量评估共享任务的任务1和任务2的系统。 我们提交的材料使用(i)连续空间语言模型来提取任务1的附加特征(SHEFGP，SHEF-SVM），(ii)连续Bagof-Words模型来生成作为任务2特征的单词嵌入(SHEF-W2V)，以及(iii)由Quest++生成的特征和由单词嵌入模型生成的特征的组合(SHEFQuest+++)。 我们的系统性能优于基线和许多其他提交的数据。 任务2的结果尤其令人鼓舞，在任务2中，我们的最佳性能系统(SHEF-W2V)只使用以无人监督的方式学习的特性。

##### [116] Guzman F, Joty S, Marquez L, et al.Pairwise neural machine translation evaluation//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL 2015) .Beijing, China, 2015:805-814
We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.
本文提出了一种新的基于神经网络的机器翻译评价框架，其目标是在给定参考译文的情况下，从一对假设中选择更好的译文。 在该框架中，来自指称和两个假设的词汇、句法和语义信息被压缩成相对较小的分布式向量表示，并被馈送到多层神经网络中，该多层神经网络对每个假设和指称之间以及两个假设之间的交互进行建模。 这些紧凑的表示依次基于单词和句子的嵌入，这些嵌入是使用神经网络学习的。 该框架是灵活的，允许有效的学习和分类，并产生与人类的相关性，与现有技术的竞争。 

##### [117] Gupta R, Orasan C, van Genabith J.ReVal:A simple and effective machine translation evaluation metric based on recurrent neural networks//Proceedings of the 2015Conference on Empirical Methods in Natural Language Processing (EMNLP 2015) .Lisbon, Portugal, 2015:1066-1072
Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve best results. We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data.
许多最先进的机器翻译(MT)评估度量是复杂的，涉及大量的外部资源（例如用于释义），并且需要进行调优以获得最佳结果。 提出了一种基于稠密向量空间和递归神经网络（RNNs），特别是长短时记忆(LSTM)网络的简单替代方法。 对于WMT-14，我们的新指标在五个语言对中的两个语言对中得分最高，在所有语言对中的总体最佳和次优，分别使用Spearman和Pearson相关。 我们还展示了如何从WMT排名数据中自动计算训练数据。

##### [118] Shi X, Padhi I, Knight K.Does string-based neural MTlearn source syntax//Proceedings of the 2016Conference on Empirical Methods in Natural Language Processing (EMNLP2016) .Austin, USA, 2016:1526-1534
We investigate whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training. We propose two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing.
我们研究了一个神经编码器译码器翻译系统是否作为训练的副产品来学习源端的句法信息。 我们提出了两种方法来检测编码器是否已经学习了本地和全局源语法。 对编码器学习到的语法结构进行细粒度分析，可以发现学习到的语法类型和缺少的语法类型。

##### [119] Ding Yanzhuo, Liu Yang, Luan Huanbo, et al.Visualizing and understanding neural machine translation//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1150-1159
While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoderdecoder framework. We show that visualization with LRP helps to interpret the internal workings of NMT and analyze translation errors.
近年来，神经机器翻译(NMT)取得了显著的进展，但由于神经网络的连续表示和非线性特性，很难解释其内在的工作原理。 在本文中，我们提出在基于注意力的编码器解码器框架中，使用层相关传播(LRP)来计算每个上下文单词对任意隐藏状态的贡献。 实验结果表明，LRP可视化有助于解释NMT的内部工作机制和分析翻译错误。

##### [120] Reed S, Akata Z, Yan X, et al.Generative adversarial text to image synthesis//Proceedings of the 33rd International Conference on Machine Learning (ICML 2016) .New York, USA, 2016:1060-1069
Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.
从文本中自动合成逼真的图像将是有趣和有用的，但目前的人工智能系统仍远未达到这一目标。 然而，近年来发展了通用的、功能强大的递归神经网络结构来学习区分文本特征表示。 同时，深度卷积生成对抗性网络（GANS）已经开始生成特定类别的非常引人注目的图像，例如人脸、专辑封面和房间内部。 在这项工作中，我们开发了一种新颖的深层结构和GaN公式，以有效地连接文本和图像建模的这些进展，将视觉概念从字符转换为像素。 我们展示了我们的模型从详细的文本描述中生成鸟和花的可信图像的能力。