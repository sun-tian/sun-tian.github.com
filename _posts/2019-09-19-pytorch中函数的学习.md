---
layout:     post                    # 使用的布局（不需要改）
title:      pytorch中函数的学习             # 标题 
subtitle:   encoder、decoder、attention中函数的学习 #副标题
date:       2019-09-19              # 时间
author:     甜果果                      # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/post-bg-coffee.jpeg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习日记
    - paper
    - pytorch
    - python
    - nlp
    - 机器翻译
---

### 1. [pytorch中的embedding词向量的使用](https://blog.csdn.net/david0611/article/details/81090371)
##### Embedding
词嵌入在 pytorch 中非常简单，只需要调用 torch.nn.Embedding(m, n) 就可以了，m 表示单词的总数目，n 表示词嵌入的维度，其实词嵌入就相当于是一个大矩阵，矩阵的每一行表示一个单词。

##### emdedding初始化
默认是随机初始化的
```python
import torch
from torch import nn
from torch.autograd import Variable
# 定义词嵌入
embeds = nn.Embedding(2, 5) # 2 个单词，维度 5
# 得到词嵌入矩阵,开始是随机初始化的
torch.manual_seed(1)
embeds.weight
# 输出结果：
Parameter containing:
-0.8923 -0.0583 -0.1955 -0.9656  0.4224
 0.2673 -0.4212 -0.5107 -1.5727 -0.1232
[torch.FloatTensor of size 2x5]
```
如果从使用已经训练好的词向量，则采用
```python
pretrained_weight = np.array(args.pretrained_weight)  # 已有词向量的numpy
self.embed.weight.data.copy_(torch.from_numpy(pretrained_weight))
```

##### embed的读取
读取一个向量。 
注意参数只能是LongTensor型的

```python
# 访问第 50 个词的词向量
embeds = nn.Embedding(100, 10)
embeds(Variable(torch.LongTensor([50])))
# 输出：
Variable containing:
 0.6353  1.0526  1.2452 -1.8745 -0.1069  0.1979  0.4298 -0.3652 -0.7078  0.2642
[torch.FloatTensor of size 1x10]
```
读取多个向量。 
输入为两个维度(batch的大小，每个batch的单词个数)，输出则在两个维度上加上词向量的大小。

- Input: LongTensor (N, W), N = mini-batch, W = number of indices to extract per mini-batch
- Output: (N, W, embedding_dim)
见代码
```python
# an Embedding module containing 10 tensors of size 3
embedding = nn.Embedding(10, 3)
# 每批取两组，每组四个单词
input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))
a = embedding(input) # 输出2*4*3
a[0],a[1]
```
输出为：
```python
(Variable containing:
 -1.2603  0.4337  0.4181
  0.4458 -0.1987  0.4971
 -0.5783  1.3640  0.7588
  0.4956 -0.2379 -0.7678
 [torch.FloatTensor of size 4x3], Variable containing:
 -0.5783  1.3640  0.7588
 -0.5313 -0.3886 -0.6110
  0.4458 -0.1987  0.4971
 -1.3768  1.7323  0.4816
 [torch.FloatTensor of size 4x3])
```

### 2. [关于pytorch中的GRU](https://www.cnblogs.com/duye/p/10590146.html)
取词向量，放进GRU。
```python
gru = torch.nn.GRU(input_size,hidden_size,n_layers)
# 这里的input_size就是词向量的维度，hidden_size就是RNN隐藏层的维度，这两个一般相同就可以
# n_layers是GRU的层数
```
可见，并不需要指定时间步数，也即seq_len，这是因为，GRU和LSTM都实现了自身的迭代。

### 3. [embedded = self.embedding(input).view(1, 1, -1)](https://blog.csdn.net/jiangpeng59/article/details/84859640)
```python
def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1) # RNN的输入大小都是(1,1,hidden_size)，即batch=1,seq_len=1,hidden_size=embed_size
```
RNN的输入大小都是(1,1,hidden_size)，即batch=1,seq_len=1,hidden_size=embed_size

### 4. [nn.Softmax()与nn.LogSoftmax()](https://blog.csdn.net/geter_CS/article/details/82878083)
nn.Softmax()计算出来的值，其和为1，也就是输出的是概率分布  
而logsofmax输出的是小于0的数
```python
import torch.nn as nn
import torch
import numpy as np

layer1=nn.Softmax()
layer2=nn.LogSoftmax()
 
input=np.asarray([2,3])
print(input)
input=Variable(torch.Tensor(input))
 
output1=layer1(input)
output2=layer2(input)
print('output1:',output1)
print('output2:',output2)
```

```python
[2 3]
output1: tensor([0.2689, 0.7311])
output2: tensor([-1.3133, -0.3133])
```

### 5. [PyTorch之nn.ReLU与F.ReLU的区别](https://blog.csdn.net/u011501388/article/details/86602275)
nn.ReLU作为一个层结构，必须添加到nn.Module容器中才能使用，而F.ReLU则作为一个函数调用，看上去作为一个函数调用更方便更简洁。
```python
import torch.nn as nn
import torch.nn.functional as F
import torch.nn as nn
 
class AlexNet_1(nn.Module):
 
    def __init__(self, num_classes=n):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
         )
 
    def forward(self, x):
        x = self.features(x)
 
class AlexNet_2(nn.Module):
 
    def __init__(self, num_classes=n):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
         )
 
    def forward(self, x):
        x = self.features(x)
        x = F.ReLU(x)
```

### 6. [Relu函数作用](https://www.zhihu.com/question/29021768)
1.为什么引入非线性激励函数？

如果不适用激励函数，那么在这种情况下每一层的输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（perceptron）了

正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了，不再是输入的线性组合，可以逼近任意函数，最早的想法是用sigmoid函数或者tanh函数，输出有界，很容易充当下一层的输入

2.为什么引入Relu?

第一，采用sigmoid等函数，算激活函数时候（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相当大，而采用Relu激活函数，整个过程的计算量节省很多

第二，对于深层网络，sigmoid函数反向传播时，很容易就出现梯度消失的情况（在sigmoid函数接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练

第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生

当然，现在也有一些对relu的改进，比如，prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进

多加一句，现在主流的做法，会多做一步batch normalization，尽可能保证每一层网络的输入具有相同的分布

一言以蔽之，其实，relu函数的作用就是增加了神经网络各层之间的非线性关系，否则，如果没有激活函数，层与层之间是简单的线性关系，每层都相当于矩阵相乘，这样怎么能够完成我们需要神经网络完成的复杂任务，

我们利用神经网络去解决图像分割，边界探测，超分辨等问题时候，我们的输入（假设为x），与期望的输出（假设为y）之间的关系究竟是什么？也就是y=f(x)中，f是什么，我们也不清楚，但是我们对一点很确信，那就是f不是一个简单的线性函数，应该是一个抽象的复杂的关系，那么利用神经网络就是去学习这个关系，存放在model中，利用得到的model去推测训练集之外的数据，得到期望的结果

### 7. [PyTorch的torch.cat](https://blog.csdn.net/qq_39709535/article/details/80803003)
torch.cat是将两个张量（tensor）拼接在一起，cat是concatnate的意思，即拼接，联系在一起。
使用torch.cat((A,B),dim)时，除拼接维数dim数值可不同外其余维数数值需相同，方能对齐。
```python
>>> import torch
>>> A=torch.ones(2,3) #2x3的张量（矩阵）                                     
>>> A
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])
>>> B=2*torch.ones(4,3)#4x3的张量（矩阵）                                    
>>> B
tensor([[ 2.,  2.,  2.],
        [ 2.,  2.,  2.],
        [ 2.,  2.,  2.],
        [ 2.,  2.,  2.]])
>>> C=torch.cat((A,B),0)#按维数0（行）拼接
>>> C
tensor([[ 1.,  1.,  1.],
         [ 1.,  1.,  1.],
         [ 2.,  2.,  2.],
         [ 2.,  2.,  2.],
         [ 2.,  2.,  2.],
         [ 2.,  2.,  2.]])
>>> C.size()
torch.Size([6, 3])
>>> D=2*torch.ones(2,4) #2x4的张量（矩阵）
>>> C=torch.cat((A,D),1)#按维数1（列）拼接
>>> C
tensor([[ 1.,  1.,  1.,  2.,  2.,  2.,  2.],
        [ 1.,  1.,  1.,  2.,  2.,  2.,  2.]])
>>> C.size()
torch.Size([2, 7])
```

### 8. [pytorch学习 中 torch.squeeze() 和torch.unsqueeze()的用法](https://blog.csdn.net/xiexu911/article/details/80820028)
squeeze的用法主要就是对数据的维度进行压缩或者解压。

先看**torch.squeeze()** 这个函数主要对数据的维度进行压缩，去掉维数为1的的维度，比如是一行或者一列这种，一个一行三列（1,3）的数去掉第一个维数为一的维度之后就变成（3）行。squeeze(a)就是将a中所有为1的维度删掉。不为1的维度没有影响。a.squeeze(N) 就是去掉a中指定的维数为一的维度。还有一种形式就是b=torch.squeeze(a，N) a中去掉指定的定的维数为一的维度。

再看**torch.unsqueeze()** 这个函数主要是对数据维度进行扩充。给指定位置加上维数为一的维度，比如原本有个三行的数据（3），在0的位置加了一维就变成一行三列（1,3）。a.squeeze(N) 就是在a中指定位置N加上一个维数为1的维度。还有一种形式就是b=torch.squeeze(a，N) a就是在a中指定位置N加上一个维数为1的维度
```python
a = torch.randn(1,3)
a, a.shape

(tensor([[-0.0910, -0.1256, -0.5418]]), torch.Size([1, 3]))
```

```python
b = torch.unsqueeze(a, 1)
c = a.unsqueeze(0)
print(b)
print(b.shape)
print(c.shape)

tensor([[[-0.0910, -0.1256, -0.5418]]])
torch.Size([1, 1, 3])
torch.Size([1, 1, 3])
```

```python
f = torch.randn(3)
print(f)
print(f.shape)
g = f.unsqueeze(0)
print(g.shape)

tensor([-0.7645,  0.7322,  0.9287])
torch.Size([3])
torch.Size([1, 3])
```

```python
d = torch.squeeze(c)
print(d)
print(d.shape)

tensor([-0.0910, -0.1256, -0.5418])
torch.Size([3])
```

### 9. [一篇文章搞懂Python中的面向对象编程](http://yangcongchufang.com/%E9%AB%98%E7%BA%A7python%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/python-object-class.html)

### 10. 服务器训练神经网络容易中断，中断后需要重新训练的解决办法
一、开启一个新窗口：screen -S name  
二、可以关闭运行窗口  
三、screen -ls：查看当前打开的窗口，输入screen -r ID，就能进入到相应的screen    
四、当结束程序以后，可以输入exit，或者screen -S screenID -X quit关闭窗口  