---
layout:     post           # 使用的布局（不需要改）
title:      2019 中文字粒度切分在蒙汉机器翻译的应用           # 标题 
subtitle:   2019 中文字粒度切分在蒙汉机器翻译的应用          #副标题
date:       2020-05-20             # 时间
author:     甜果果                    # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/post-bg-coffee.jpeg    #背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - nlp
    - paper
    - 机器翻译
    - 低资源机器翻译

---

# 2019 中文字粒度切分在蒙汉机器翻译的应用

![image-20200516121637581](https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200720164258.jpg)

# 摘 　 要：

​		在机器翻译任务中，主流的深度学习算法大多使用词或子词作为基础的语义单元，在词或子词层面学习嵌 入表征。 然而，词粒度层面存在一系列缺点。 该文基于 ＬＳＴＭ 和 Ｔｒａｎｓｆｏｒｍｅｒ 蒙汉翻译模型，对蒙文进行子词粒 度切分，对中文分别进行子词和字粒度切分对比实验。 实验结果显示，相比于子词粒度切分，基于 Ｔｒａｎｓｆｏｒｍｅｒ 的 蒙汉翻译模型和基于 ＬＳＴＭ 的蒙汉翻译模型的字粒度切分有极大的 ＢＬＥＵ 值提升，字级别的蒙汉翻译模型在验 证集和测试集上都显著优于混合字和词的子词级别的蒙汉翻译模型。 其表明，字级别的蒙汉翻译模型更能捕捉单 元之间的语义联系，提高蒙汉翻译性能。 ； 

# 关键词：

​		字粒度切分：Transformer；LSTM

# ０引言

​		自 ２０１３ 年以来，得益于深度学习方面取得的进 展以及一些资源丰富的大规模平行语料库的可用性，神经机器翻译（ ｎｅｕｒａｌｍａｃｈｉｎｅｔｒａｎｓｌａｔｅ ， ＮＭＴ ） 　 　 １ 模型获得了迅速发展 ［ ］ ， ｓｔａｔｉｓｔｉ 与统计机器翻译（ｃａｌｍａｃｈｉｎｅｔｒａｎｓｌａｔｉｏｎ ， ＳＭＴ ） 模型相比，其翻译质量明显提升。 机器翻译是解决不同民族和国家之间 信息交流所面临的“语言屏障”问题的关键技术，在 加强文化交流、促进民族团结以及推动对外贸易等 ２ 方面都意义重大 ［ ］ 。 蒙汉机器翻译对于蒙汉两种文 化的价值观相互渗透，凝聚民族的核心文化，促进良 好民族关系的建立具有重要意义。 然而由于内蒙古地区经济发展相对缓慢，蒙汉平行语料收集困难， 利用现有的神经网络方法会放大数据稀疏以及训 练过拟合 等 问 题，导 致 翻 译 质 量 不 高。 并 且 在 翻 译模型中，编码器和解码器的计算复杂度比较高，由 于计算量和 ＧＰＵ 内存的限制，神经机器翻译模型 需要事先确定一个规模受到限制的常用词词表，神 经机器翻译系统往往将词汇表限制为高频词， 并将 ， 其他所有低频词视为未登录词（ ｏｕｔ － ｏｆ － ｖｏｃａｂｕｌａｒ ｙ ＯＯＶ ）。 中文的字是最小的音义结合体，但是在现 代汉语中，词才是中文的基本表达单位，并且中文以 双字词或者多字词为主， 而大部分的词都是由多个 字组合而成。 中文分词的统计机器学习方法优于传 统的规则方法，尤其是在未登录词上具有无可比拟 的优势。 主流的深度学习算法也大多使用词或子词作为基础 的 语 义 单 元，在 词 或 子 词 层 面 学 习 嵌 入 表征。

​		虽然词粒度级别的模型在很多任务上得到普遍 应用，但是词粒度级别的输入单元有大量显著不足： （ ） 词粒度数据会放大稀疏问题，容易导致过拟合， １ 并且过多的未登录词会阻碍模型的学习能力。 （ ） ２ 分词会引入噪声，并且词的多义性会影响分词的方 法，同时人对词的颗粒度的认识都会有差别，故分词 方法的不同导致分词效果不同，错误的分词将会影 响后续翻译模型效果； （ ） 直观上看，词携带的语义 ３ 信息比字更丰富，但是对于神经网络机器翻译而言， 词粒度模型是否优于字粒度模型还不清楚。

​		相比于词粒度，字粒度具有以下优点：（ 可以 相比于词粒度， １ ） 缩小词汇表，以词为单位时，词汇表太大，而以字为 单位时，词汇表（即字表）的大小适中。 词汇表越大， 深度学习所需要的参数就越多，训练起来就越困难。 （ ） 以字粒度为单元，不依赖于分词工具的正确性。 ２ （ ） 可以让模型集中在不同字的交互方面，例如，以 ３ “ 可能” ” 这个词为例， “ 可”可能有“可爱” “ 可能” “ 可 以”等多种意思。 “ 能”也有可能有“能力” “ 能够”等 多种意思。 但是因为深度学习模型善于处理远距离 依赖关系，模型在后面的层是会去学习“可”和“能” 组合起来的意思。

​		文献［ ３ － ４ ］ 证明，中文输入粒度是混合字与词粒 度的子词粒度的效果比单纯的词粒度的效果更好。 中文分词效果差的原因是在较小语料库的情况下， 大粒度的切分会放大数据稀疏问题，使得翻译效果 不太好，子词粒度切分后，由分词后的低频词切换成 高频词，从一定程度上缓解数据稀疏问题，提高了翻 译质量。 故 本 文 基 于 ＬＳＴＭ 和 Ｔｒａｎｓｆｏｒｍｅｒ 为 基 本的蒙汉翻译模型，以内蒙古工业大学构建的 １２６ 万蒙汉平行语料库和 ３ 万蒙汉专有名词为实验数 据，对中文输入粒度进行子词粒度和字粒度切分的 对比实验。

​		本文的内容安排如下：第 １ 节为神经机器翻译 模型概述，主要介绍了目前比较热门的神经机器翻译 技术和方法。 第 ２ 节为相关技术介绍，主要介绍了中 文的词、子词、字粒度。 第 ３ 节为实验部分，主要包 括语料库的划分、实验设置和实验结果。 第 ４ 节为 结论部分，对文中所做的工作进行了总结和展望。

# 1 神经机器翻译模型

​		NMT相比于传统的SMT而言，是一个能够通过训练把一个序列映射到另一个序列的神经网络， 输出的可以是长度变化了的序列，这对实现自动翻 译、人机 对 话 以 及 文 字 概 括 等 有 很 大 好 处。 ＮＭＴ 其实就是一个编码器—解码器结构，编码器把源语 言序列进行编码，并提取源语言中的有用信息，再通 过解码器将这些信息转换至目标语言中来， 以实现 对语言的翻译。

## １．１　基于ＬＳＴＭ的神经网络翻译模型

​		长短期记忆网络 ［ ］ （ ｌｏｎ ｇ ｓｈｏｒｔｔｅｒｍ　ｍｅｍｏｒ ｙ 　 　 ， ＬＳＴＭ ） 是一种特殊的 ＲＮＮ 。 ＬＳＴＭ 能够避免长期 依赖性问 题，其 实 现 建 模 的 公 式 如 式 （ ） 式 （ ） １ ～ ４ 所示。

![image-20200516122027363](https://tva1.sinaimg.cn/large/007S8ZIlly1geu68znj3kj30g805ogm7.jpg)

​		其中， 是元素的 Ｓｉ ｇ ｍｏｉｄ 函数。 对于给定的 σ 一个包含 ｎ 个单词的句子 （ ｘ １ ， ｘ ２ ， ．．． ， ｘ ）， 每个单 ｎ 词表示为 ｄ 维的词向量。 

​		本文的实验使用了哈佛大学开源的神经机器翻 译系统 Ｏ ｐ ｅｎＮＭＴ ， 这是基于注意力机制的 ＬＳＴＭ 翻 ６ 译模型。 注意力机制 ［ ］ 的结构原理如图 １ 所示。

![image-20200516122059937](https://tva1.sinaimg.cn/large/007S8ZIlly1geu69k9juqj30d60gg0tq.jpg)

## １．２　基于的神经网络翻译模型

​		Ｔｒａｎｓｆｏｒｍｅｒ是完全基于注意力机制的Ｓｅ ｑ ２Ｓｅ ｑ ８模型，采用多头自注意力 ［ ］ 来构造编码器和解码器。 它的设计思想是把序列中的所有单词并行处理，同 时借助自注意力机制将上下文与较远的单词结合起来。 在每个步骤中，句子中的每一个词的信息都可 以借助自注意力机制与句子中的所有其他的词进行 沟通。 图 ２ 为 Ｔｒａｎｓｆｏｒｍｅｒ 结构图。

![image-20200516122325342](https://tva1.sinaimg.cn/large/007S8ZIlly1geu6c2tidrj30ki0rkwho.jpg)

# ２ 　相关技术

## ２．１蒙古文预处理

　　　 传统蒙古文虽然属于阿尔泰语系蒙古语族，但 却与印欧语系中的大部分语言一样，属于一种拼音 文字，只是它的构成基础是回鹘字母 ［ １０ ］ 。 所以和英 语一样，蒙古文句子由字、空格以及标点符号构成， 句子本身就已经算是具有词级粒度的形式。 蒙古语 属于黏着语， 黏着语的一个特点是通过在词根的前、 中、后位置缀接其他构词成分作为派生新词的手段， 因此蒙古文构词及其形态变换非常丰富，导致集外 词和未登录词现象频发。 文献［ ３ － ４ ］ 证明，蒙古文进 行子词粒度的切分能够减少集 外词和未登录词现象，故本文蒙古文的输入粒度是子词。 蒙古文的具 体构词规则如图 ３ 所示。

![image-20200516122406813](https://tva1.sinaimg.cn/large/007S8ZIlly1geu6csu2guj30fu06qq3g.jpg)

图３　蒙古文构词规则

​		对蒙古文进行子词处理，是指把蒙古文切分为 比词级粒度更小的粒度。 Ｓｅｎｎｒｉｃｈ 和 Ｈａｄｄｏｗ 提出一种子词级粒度处理文本的方式，称为字节对编 码 （ ｂ ｙ ｔｅ ｐ ａｉｒｅｎｃｏｄｅｒ ， ＢＰＥ ） ［ １１ ］ 。 ＢＰＥ 处 理 使 用Ｇｉｔｈｕｂ 上 的 开 源 系 统 ｆａｓｔＢＰＥ ， 实 验 中 将 ＢＰＥ 中 ｃｏｄｅｓ 数设置为 ３２０００ 。 例如，蒙古文“ ”（ 　 文意思是衷心祝贺！ ）， 经过 ＢＰＥ 处理后，蒙古文变 成“ ”。 将 原 蒙 古 文 单 词 切 分 为 两 部 分，前半部分属于蒙古语词干，后半部分是词缀，其 实并不是语言学上真正的词缀，而是剩余蒙古语词 缀连接在一起的结果，词缀并没有实际意思，这样切 分的好处是增加了低频词子词的共现次数， 从而一 定程度上减少未登录词的出现。

## ２．２　

### ２．２．１　 词级粒度 

​		汉语属于汉藏语系，每句话只由单个的字和标 点符号构成。 汉语分词是将连续的中文字符序列按 ［ １２ ］ 照某种规则分割成词的序列过程 。 当然，语料的 切分也会带来相应的一些问题， 比如会出现歧义词 现象和破坏语料整体的语义关系等。

​		本文中文分词使用的分词工具是双 向 ＬＳＴＭ ［ ］ ［ ］ 和 ＣＲＦ （ Ｂｉ － ＬＳＴＭ－ＣＦ ）模 型 １３ 。 ＣＲＦ １４ （ ｃｏｎｄｉ ｔｉｏｎａｌｒａｎｄｏｍｆｉｅｌｄｓ ） 中文名称是条件随机场，其主 　 　 要作用是弥补最大熵马尔科夫模型（ ｍａｘｉｍｕｍ 　 ｅｎ ｔｒｏ ｐｙ ｍａｒｋｏｖ ｍｏｄｅｌ ， ＭＥＭＭ ） 分词方法的不足，也 　 　 就是为了 缓 解 ＭＥＭＭ 中 的 标 记 偏 置 问 题。 ＣＲＦ 是一种判别式模型， ＣＲＦ 通过定义条件概率 Ｐ （ ｜ Ｙ Ｘ ）来描述模型。

​		融合双向 ＬＳＴＭ 和 ＣＲＦ （ Ｂｉ － ＬＳＴＭ－ＣＲＦ ） 分词算法结构如图 ４ 所示。 图 ４ 中，输入层为词嵌 入表示层，经过一个双向的 ＬＳＴＭ 网络编码，输出 层是一个 ＣＲＦ 层，经过 ＬＳＴＭ 网络输出的实际上 是当前位置对于各词性的得分， ＣＲＦ 是对词性得分加上前一位置的词性概率转移的约束，其优点在于 引入了一些语法规则的先验信息。 

### ２．２．２　 子词级粒度

​		子词级粒度切分指的是将语料切分为比词级粒 度更小的单元。 把罕见词拆分为子词单元的组合， 子词单元的拆分策略，是借鉴了一种数据压缩算法 其基本原理是：首先将语料以最细粒度为单 ＢＰＥ ， 元进行分切，构造一个初始化子词表，接着统计出语 料中所有单元之间组合出现的次数， 最后将统计到 的次数按从大到小的顺序排列， 选择出现次数最多 的组合替换初始子词表中的单个单元， 在这个过程 中最重要的是系统需要自动学习一个词典， 从而才 能依据学习到的词典对语料进行切分。 经过子词切 分后，即便是训练语料库里没有见过的新词，也可以 通过拼接子词来生成翻译。 子词切分可以有效地缓 解神经机器翻译中的 ＯＯＶ 和罕见词翻译。 

### ２．２．３　 字级粒度

​		将中文进行字粒度切分，分字使用自己编写的 脚本文件，通过将原语料读入之后，为每个字之间加 入空格，然后输出到新的文本保存即可。

# ３ 　实验

## ３．１　语料库划分

　　 本文实验数据来源于内蒙古工业大学构建的 126万蒙汉平行语料库和 3万蒙汉专有名词。 3万 的蒙汉名词库包含地名、人名、农业名词、医学名词 和物理名 词 等。 16万 蒙 汉 平 行 语 料 包 括 政 府 新 闻、法律公文、日常对话、日常用语、网络对话与聊天 等。 蒙汉平行语料库数据集的划分如表 １ 所示。

![image-20200516122640547](https://tva1.sinaimg.cn/large/007S8ZIlly1geu6fgtt5gj30g203qglz.jpg)

​		对蒙汉平行语料库中的蒙文进行子词粒度切分 后，统计出词表，如表 ２ 所示。

![image-20200516122659651](https://tva1.sinaimg.cn/large/007S8ZIlly1geu6fslhbuj30g404w3yx.jpg)

​		从表 ２ 可以看出，语料库里蒙古文总共有 14946860个，只出现过一次的词有323508个。 经过子词粒度切分操作后，蒙古文总共有17128867个词，只出现过一次的词有19865个。 只出现一次的比率从2.2％ 下降到0.1％ ， 有效缓解了数据稀疏。 对 126万蒙汉平行语料库中的中文分别进行词 粒度、子词粒度和字粒度切分后，统计出词表如表 ３ 所示。

![image-20200516122746924](https://tva1.sinaimg.cn/large/007S8ZIlly1geu6gm3j3aj30fs06mt97.jpg)

​		Ｚ ｉ ｐ ｆ 定律表明，大部分中文词的出现频率都很 小，并且在数据集中的占比十分有限，这导致模型不 能充分学得数据中的语法、语义知识。 如表 ３ 所示， 经过双向 ＬＳＴＭ 和 ＣＲＦ （ Ｂｉ － ＬＳＴＭ－ＣＲ ） 模型分词 后，数据集中的中文词总共有 １１８９１５３０ 个，其中有 　 　 ３２０８６０ 个 词 仅 出 现 了 一 次，占 数 据 集 总 词 的 　 ２．７％ 。 这表明，词粒度级别的数据 放 大 了 数 据 稀 疏，比较容易导致过拟合问题。 子词切分粒度下，中 文词总共有 １４１８００９５ 个，并且有 ２４２４１ 个词只出 　 　 　 现了 １ 次，比 率为 ０．２％ ， 说 明 经 过 子 词 粒 度 切 分 后，减少了数据稀疏。 字粒度模型下，中文词（即字） 总共 有 ２１ ８０８ １８７ 个，只 出 现 了 一 次 的 词 仅 有 　 　 ７１４２ 个，占比率 ０．０３％ 。 经过字粒度切分后，只出 　 现过 一 次 词 的 比 率 从 词 粒 度 的 ２．７％ 缩 小 到 ０．０３％ ， 极大减少了低频词。

## ３．２　实验设置

​		基于Transformer的蒙汉翻译模型本文选用清华大学自然语言处理小组开发的机器翻译库THUMT。 使 用 一 台 搭 载NVIDIA Tesla P100 GPU，RAM16GB； 基于LSTM的蒙汉翻译模型本 　 文选用开源库OpenNMT，NVIDIA使用一台搭载1070TiGPU，RAM 8GB。 实 验 环 境 为Ubuntu 16.04，Linux系统，语言为Python2.7.0， Tensorflow版本为1.6.0，Anaconda 3.5.2.0用multibleu.perl脚本评测翻译性能BLUE值。LSTM神经网络的编码器和解码器的 隐藏层数设置为4层，词向量维度设置为1000， 解码器中全局注意力机制中输入特征设置为500， 输 出特征设置 为500， 激活函数选择tanh()。Torch0.4.0，TorchText 0.2.3。dropout设 置 为0.3，train_steps设置为200000， 学习率初始值设置为   学习率衰减速率设置为0.1。Transformer的神经网络层数设置为6层，多头注意力机制设置为8头，激活函数使 用GELU， 优化函数使用Adam优化算法，学习率初始值设置为 一阶矩估计的指数衰减率设置0.1， 为二阶矩估计的指数衰减率设置为0.98，train_steps设置为200000，batch_size设置为4096。

## ３．３　实验结果

​		如图 ５ 、 图 ６ 所示，本文分别统计出了基于 ＬＳＴＭ 神经网络蒙汉机器翻译模型和基于 Ｔｒａｎｓｆｏｒｍｅｒ 神 经网络蒙汉机器翻译模型在 ２０００００ｔｒａｉｎ ＿ ｓｔｅ ｐ ｓ 上 　 对蒙文进行子词粒度切分， 中文分别进行子词和字 粒度切分的测试集和验证集的 ＢＬＥＵ 值以及其变 化趋势。

![image-20200516122915597](https://tva1.sinaimg.cn/large/007S8ZIlly1geu6i5v4oaj30w80e6gov.jpg)

![image-20200516122927982](https://tva1.sinaimg.cn/large/007S8ZIlly1geu6id7f14j30p00cwdis.jpg)

​		表 ４ 和 表 ５ 分 别 是 基 于 ＬＳＴＭ 翻 译 系 统 和 Ｔｒａｎｓｆｏｒｍｅｒ 翻译模型，对蒙文进行子词粒度切分，

![image-20200516122951800](https://tva1.sinaimg.cn/large/007S8ZIlly1geu6is7t5zj30gk0dy3zx.jpg)

​		从表 ４ 可以看出，基于 ＬＳＴＭ 翻译模型，对蒙 文进行子词粒度切分，对中文进行子词粒度切分后， ＢＬＥＵ 值达到了 ２５．２５ ， 准确率 ＡＣＣ 达到了 ５８．５ ； 准 对中文进行字粒度切分后， ＢＬＥＵ 值达到 ４３．５８ ， 确率 ＡＣＣ 达到了 ７２．７ 。 基于 Ｔｒａｎｓｆｏｒｍｅｒ 翻译模 型中，中文子词粒度翻译模型的 ＢＬＥＵ 值为 ４４．１４ ， 相对于 中文字粒度翻译模型的 ＢＬＥＵ 值为 ５７．９０ ， 子词粒度提高了 １３．７６ 个 ＢＬＥＵ 值，准确率大约提 高了 １０ 个值。

​		 图 ５ 、 图 ６ 、 表 ４ 和表 ５ 说明基于字粒度的模型 优于混合字和词的子词模型，这表明了基于字粒度 的模型已经编码了语言建模任 务所必要的语言信 息，另外加入词反而会损害其翻译表现。 在蒙汉翻 译中，解码端的 ＵＮＫ 对于词影响更大。 字级别的 模型在验证集和测试集上都显著优于混合词级别和 字级别的子词模型。 字粒度的模型能集中在不同字 的交互方面，更能捕捉单元之间的语义联系，提高翻 译质量。

# 4 总结与未来工作

​		在蒙汉机器翻译中，翻译单元的大小直接影响 着翻译的性能， 而翻译单元的大小又是通过语料的 切分粒度而体现的，所以本文主要介绍了对中文语 料切分的两种方式：字切分粒度和混合字与词的子 词切分粒度。 对于每一种方法，都通过原理概述和 实验来进行说明， 最后对各方法的实验结果进行了 对比分析。 结果表明在蒙汉机器翻译中，对中文进 行字级别粒度切分要优于混合词和字的子词粒度切 分。 本文的结论与 Ｍｅｎ ｇ Ｙ ［ １５ ］ 等人的结论也一致。 　 但是字粒度切分也存在问题，一个很关键的问题就 是一字多义，而词在一定程度上减轻了这个问题，这 也是在统计时代分词存在的必要性。 而最新超火的 预训练语言模型 ｂｅｒｔ 就完全舍弃了分词的过程，而 是采用字粒度划分。 如果一字多义的问题能够通过 预训练语言模型来解决， 下一步拟将最新的预训练 模型应用于蒙汉翻译。 本文对中文划分粒度的研究 为后续的蒙汉机器翻译的研究做了很好的理论铺 垫，也为后续的实验打下了坚实基础，是非常重要的 一个环节。