---
layout:     post           # 使用的布局（不需要改）
title:      SOCKEYE新能源机器翻译实验           # 标题 
subtitle:   SOCKEYE新能源机器翻译实验 #副标题
date:       2020-05-01             # 时间
author:     甜果果                    # 作者
header-img: https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200715211802.png   #背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - nlp
    - paper
    - 机器翻译
    - 专利
    - 新能源

---

# SOCKEYE新能源机器翻译实验

1.  准备训练集，测试集，验证集，6个文件

    train.en   train.zh   test.en   test.zh   dev.en   dev.zh

2.  preprocessing 建立词汇表

    为了能使用learn_apply_bpe，需要把subword-nmt和apply_bpe粘过来，并且将位置导出（是在data文件夹的外面）

    ```python
    export PYTHONPATH=$(pwd)/subword-nmt:$PYTHONPATH
    ```

    创建共同的源端和目标端BPE词汇表，联合训练不用分词，但是会特别特别慢。

    ```python
    python -m learn_joint_bpe_and_vocab --input train.zh train.en -s 30000 -o bpe.codes --write-vocabulary bpe.vocab.zh bpe.vocab.en
    ```

    将字节对编码应用于我们的训练、开发和测试数据

    ```python
    python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.zh --vocabulary-threshold 50 < train.zh > train.BPE.zh
    python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.en --vocabulary-threshold 50 < train.en > train.BPE.en
    python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.zh --vocabulary-threshold 50 < dev.zh > dev.BPE.zh
    python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.en --vocabulary-threshold 50 < dev.en > dev.BPE.en
    python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.zh --vocabulary-threshold 50 < test.zh > test.BPE.zh
    python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.en --vocabulary-threshold 50 < test.en > test.BPE.en
    ```

3.  训练

    碎片并以矩阵格式序列化

    ```python
    python -m sockeye.prepare_data   -s train.BPE.zh  -t train.BPE.en  -o train_data
    ```

    ```python
    python -m sockeye.train -d train_data -vs dev.BPE.zh -vt dev.BPE.en --encoder transformer --decoder transformer --transformer-attention-heads 8 --transformer-feed-forward-num-hidden 2048 --max-updates 200000 --num-embed 512 --initial-learning-rate 2 --label-smoothing 0.1 --batch-size 4096 --decode-and-evaluate 500 --optimizer adam -o UM_News_zh_en_model --loglevel DEBUG --device-ids 1 --checkpoint-interval 500
    ```

    以上两行代码不行，报错`Shared vocabulary settings need to match these of the prepared data (e.g. for weight tying). Specify or omit --shared-vocab consistently when training and preparing the data.`，下面的命令可以

    ```python
    python -m sockeye.train --source train.BPE.zh --target train.BPE.en -vs dev.BPE.zh -vt dev.BPE.en --encoder transformer --decoder transformer --transformer-attention-heads 8 --transformer-feed-forward-num-hidden 2048 --max-updates 200000 --num-embed 512 --initial-learning-rate 2 --label-smoothing 0.1 --batch-size 2048 --decode-and-evaluate 500 --optimizer adam -o UM_News_zh_en_model --loglevel DEBUG --device-ids 1 --checkpoint-interval 500
    ```

4.  评分

    ```
    python3 -m sockeye.score -m MODEL --source SOURCE --target TARGET
    ```

    


